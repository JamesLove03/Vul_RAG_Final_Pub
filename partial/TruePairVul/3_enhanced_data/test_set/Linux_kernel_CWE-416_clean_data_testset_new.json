[
    {
        "cve_id": "CVE-2020-14381",
        "code_before_change": "int inode_init_always(struct super_block *sb, struct inode *inode)\n{\n\tstatic const struct inode_operations empty_iops;\n\tstatic const struct file_operations no_open_fops = {.open = no_open};\n\tstruct address_space *const mapping = &inode->i_data;\n\n\tinode->i_sb = sb;\n\tinode->i_blkbits = sb->s_blocksize_bits;\n\tinode->i_flags = 0;\n\tatomic_set(&inode->i_count, 1);\n\tinode->i_op = &empty_iops;\n\tinode->i_fop = &no_open_fops;\n\tinode->__i_nlink = 1;\n\tinode->i_opflags = 0;\n\tif (sb->s_xattr)\n\t\tinode->i_opflags |= IOP_XATTR;\n\ti_uid_write(inode, 0);\n\ti_gid_write(inode, 0);\n\tatomic_set(&inode->i_writecount, 0);\n\tinode->i_size = 0;\n\tinode->i_write_hint = WRITE_LIFE_NOT_SET;\n\tinode->i_blocks = 0;\n\tinode->i_bytes = 0;\n\tinode->i_generation = 0;\n\tinode->i_pipe = NULL;\n\tinode->i_bdev = NULL;\n\tinode->i_cdev = NULL;\n\tinode->i_link = NULL;\n\tinode->i_dir_seq = 0;\n\tinode->i_rdev = 0;\n\tinode->dirtied_when = 0;\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tinode->i_wb_frn_winner = 0;\n\tinode->i_wb_frn_avg_time = 0;\n\tinode->i_wb_frn_history = 0;\n#endif\n\n\tif (security_inode_alloc(inode))\n\t\tgoto out;\n\tspin_lock_init(&inode->i_lock);\n\tlockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);\n\n\tinit_rwsem(&inode->i_rwsem);\n\tlockdep_set_class(&inode->i_rwsem, &sb->s_type->i_mutex_key);\n\n\tatomic_set(&inode->i_dio_count, 0);\n\n\tmapping->a_ops = &empty_aops;\n\tmapping->host = inode;\n\tmapping->flags = 0;\n\tmapping->wb_err = 0;\n\tatomic_set(&mapping->i_mmap_writable, 0);\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tatomic_set(&mapping->nr_thps, 0);\n#endif\n\tmapping_set_gfp_mask(mapping, GFP_HIGHUSER_MOVABLE);\n\tmapping->private_data = NULL;\n\tmapping->writeback_index = 0;\n\tinode->i_private = NULL;\n\tinode->i_mapping = mapping;\n\tINIT_HLIST_HEAD(&inode->i_dentry);\t/* buggered by rcu freeing */\n#ifdef CONFIG_FS_POSIX_ACL\n\tinode->i_acl = inode->i_default_acl = ACL_NOT_CACHED;\n#endif\n\n#ifdef CONFIG_FSNOTIFY\n\tinode->i_fsnotify_mask = 0;\n#endif\n\tinode->i_flctx = NULL;\n\tthis_cpu_inc(nr_inodes);\n\n\treturn 0;\nout:\n\treturn -ENOMEM;\n}",
        "code_after_change": "int inode_init_always(struct super_block *sb, struct inode *inode)\n{\n\tstatic const struct inode_operations empty_iops;\n\tstatic const struct file_operations no_open_fops = {.open = no_open};\n\tstruct address_space *const mapping = &inode->i_data;\n\n\tinode->i_sb = sb;\n\tinode->i_blkbits = sb->s_blocksize_bits;\n\tinode->i_flags = 0;\n\tatomic64_set(&inode->i_sequence, 0);\n\tatomic_set(&inode->i_count, 1);\n\tinode->i_op = &empty_iops;\n\tinode->i_fop = &no_open_fops;\n\tinode->__i_nlink = 1;\n\tinode->i_opflags = 0;\n\tif (sb->s_xattr)\n\t\tinode->i_opflags |= IOP_XATTR;\n\ti_uid_write(inode, 0);\n\ti_gid_write(inode, 0);\n\tatomic_set(&inode->i_writecount, 0);\n\tinode->i_size = 0;\n\tinode->i_write_hint = WRITE_LIFE_NOT_SET;\n\tinode->i_blocks = 0;\n\tinode->i_bytes = 0;\n\tinode->i_generation = 0;\n\tinode->i_pipe = NULL;\n\tinode->i_bdev = NULL;\n\tinode->i_cdev = NULL;\n\tinode->i_link = NULL;\n\tinode->i_dir_seq = 0;\n\tinode->i_rdev = 0;\n\tinode->dirtied_when = 0;\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tinode->i_wb_frn_winner = 0;\n\tinode->i_wb_frn_avg_time = 0;\n\tinode->i_wb_frn_history = 0;\n#endif\n\n\tif (security_inode_alloc(inode))\n\t\tgoto out;\n\tspin_lock_init(&inode->i_lock);\n\tlockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);\n\n\tinit_rwsem(&inode->i_rwsem);\n\tlockdep_set_class(&inode->i_rwsem, &sb->s_type->i_mutex_key);\n\n\tatomic_set(&inode->i_dio_count, 0);\n\n\tmapping->a_ops = &empty_aops;\n\tmapping->host = inode;\n\tmapping->flags = 0;\n\tmapping->wb_err = 0;\n\tatomic_set(&mapping->i_mmap_writable, 0);\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tatomic_set(&mapping->nr_thps, 0);\n#endif\n\tmapping_set_gfp_mask(mapping, GFP_HIGHUSER_MOVABLE);\n\tmapping->private_data = NULL;\n\tmapping->writeback_index = 0;\n\tinode->i_private = NULL;\n\tinode->i_mapping = mapping;\n\tINIT_HLIST_HEAD(&inode->i_dentry);\t/* buggered by rcu freeing */\n#ifdef CONFIG_FS_POSIX_ACL\n\tinode->i_acl = inode->i_default_acl = ACL_NOT_CACHED;\n#endif\n\n#ifdef CONFIG_FSNOTIFY\n\tinode->i_fsnotify_mask = 0;\n#endif\n\tinode->i_flctx = NULL;\n\tthis_cpu_inc(nr_inodes);\n\n\treturn 0;\nout:\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tinode->i_sb = sb;\n \tinode->i_blkbits = sb->s_blocksize_bits;\n \tinode->i_flags = 0;\n+\tatomic64_set(&inode->i_sequence, 0);\n \tatomic_set(&inode->i_count, 1);\n \tinode->i_op = &empty_iops;\n \tinode->i_fop = &no_open_fops;",
        "function_modified_lines": {
            "added": [
                "\tatomic64_set(&inode->i_sequence, 0);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel\u2019s futex implementation. This flaw allows a local attacker to corrupt system memory or escalate their privileges when creating a futex on a filesystem that is about to be unmounted. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 2519
    },
    {
        "cve_id": "CVE-2018-10876",
        "code_before_change": "static struct buffer_head *\next4_read_inode_bitmap(struct super_block *sb, ext4_group_t block_group)\n{\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct buffer_head *bh = NULL;\n\text4_fsblk_t bitmap_blk;\n\tint err;\n\n\tdesc = ext4_get_group_desc(sb, block_group, NULL);\n\tif (!desc)\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\n\tbitmap_blk = ext4_inode_bitmap(sb, desc);\n\tif ((bitmap_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||\n\t    (bitmap_blk >= ext4_blocks_count(sbi->s_es))) {\n\t\text4_error(sb, \"Invalid inode bitmap blk %llu in \"\n\t\t\t   \"block_group %u\", bitmap_blk, block_group);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\tbh = sb_getblk(sb, bitmap_blk);\n\tif (unlikely(!bh)) {\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t    \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t    block_group, bitmap_blk);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tif (bitmap_uptodate(bh))\n\t\tgoto verify;\n\n\tlock_buffer(bh);\n\tif (bitmap_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\n\text4_lock_group(sb, block_group);\n\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n\t\tmemset(bh->b_data, 0, (EXT4_INODES_PER_GROUP(sb) + 7) / 8);\n\t\text4_mark_bitmap_end(EXT4_INODES_PER_GROUP(sb),\n\t\t\t\t     sb->s_blocksize * 8, bh->b_data);\n\t\tset_bitmap_uptodate(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_verified(bh);\n\t\text4_unlock_group(sb, block_group);\n\t\tunlock_buffer(bh);\n\t\treturn bh;\n\t}\n\text4_unlock_group(sb, block_group);\n\n\tif (buffer_uptodate(bh)) {\n\t\t/*\n\t\t * if not uninit if bh is uptodate,\n\t\t * bitmap is also uptodate\n\t\t */\n\t\tset_bitmap_uptodate(bh);\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\t/*\n\t * submit the buffer_head for reading\n\t */\n\ttrace_ext4_load_inode_bitmap(sb, block_group);\n\tbh->b_end_io = ext4_end_bitmap_read;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\twait_on_buffer(bh);\n\tif (!buffer_uptodate(bh)) {\n\t\tput_bh(bh);\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t   \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t   block_group, bitmap_blk);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\nverify:\n\terr = ext4_validate_inode_bitmap(sb, desc, block_group, bh);\n\tif (err)\n\t\tgoto out;\n\treturn bh;\nout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "static struct buffer_head *\next4_read_inode_bitmap(struct super_block *sb, ext4_group_t block_group)\n{\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct buffer_head *bh = NULL;\n\text4_fsblk_t bitmap_blk;\n\tint err;\n\n\tdesc = ext4_get_group_desc(sb, block_group, NULL);\n\tif (!desc)\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\n\tbitmap_blk = ext4_inode_bitmap(sb, desc);\n\tif ((bitmap_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||\n\t    (bitmap_blk >= ext4_blocks_count(sbi->s_es))) {\n\t\text4_error(sb, \"Invalid inode bitmap blk %llu in \"\n\t\t\t   \"block_group %u\", bitmap_blk, block_group);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\tbh = sb_getblk(sb, bitmap_blk);\n\tif (unlikely(!bh)) {\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t    \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t    block_group, bitmap_blk);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tif (bitmap_uptodate(bh))\n\t\tgoto verify;\n\n\tlock_buffer(bh);\n\tif (bitmap_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\n\text4_lock_group(sb, block_group);\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) {\n\t\tif (block_group == 0) {\n\t\t\text4_unlock_group(sb, block_group);\n\t\t\tunlock_buffer(bh);\n\t\t\text4_error(sb, \"Inode bitmap for bg 0 marked \"\n\t\t\t\t   \"uninitialized\");\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t\tmemset(bh->b_data, 0, (EXT4_INODES_PER_GROUP(sb) + 7) / 8);\n\t\text4_mark_bitmap_end(EXT4_INODES_PER_GROUP(sb),\n\t\t\t\t     sb->s_blocksize * 8, bh->b_data);\n\t\tset_bitmap_uptodate(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_verified(bh);\n\t\text4_unlock_group(sb, block_group);\n\t\tunlock_buffer(bh);\n\t\treturn bh;\n\t}\n\text4_unlock_group(sb, block_group);\n\n\tif (buffer_uptodate(bh)) {\n\t\t/*\n\t\t * if not uninit if bh is uptodate,\n\t\t * bitmap is also uptodate\n\t\t */\n\t\tset_bitmap_uptodate(bh);\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\t/*\n\t * submit the buffer_head for reading\n\t */\n\ttrace_ext4_load_inode_bitmap(sb, block_group);\n\tbh->b_end_io = ext4_end_bitmap_read;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\twait_on_buffer(bh);\n\tif (!buffer_uptodate(bh)) {\n\t\tput_bh(bh);\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t   \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t   block_group, bitmap_blk);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\nverify:\n\terr = ext4_validate_inode_bitmap(sb, desc, block_group, bh);\n\tif (err)\n\t\tgoto out;\n\treturn bh;\nout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -37,7 +37,16 @@\n \t}\n \n \text4_lock_group(sb, block_group);\n-\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n+\tif (ext4_has_group_desc_csum(sb) &&\n+\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) {\n+\t\tif (block_group == 0) {\n+\t\t\text4_unlock_group(sb, block_group);\n+\t\t\tunlock_buffer(bh);\n+\t\t\text4_error(sb, \"Inode bitmap for bg 0 marked \"\n+\t\t\t\t   \"uninitialized\");\n+\t\t\terr = -EFSCORRUPTED;\n+\t\t\tgoto out;\n+\t\t}\n \t\tmemset(bh->b_data, 0, (EXT4_INODES_PER_GROUP(sb) + 7) / 8);\n \t\text4_mark_bitmap_end(EXT4_INODES_PER_GROUP(sb),\n \t\t\t\t     sb->s_blocksize * 8, bh->b_data);",
        "function_modified_lines": {
            "added": [
                "\tif (ext4_has_group_desc_csum(sb) &&",
                "\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) {",
                "\t\tif (block_group == 0) {",
                "\t\t\text4_unlock_group(sb, block_group);",
                "\t\t\tunlock_buffer(bh);",
                "\t\t\text4_error(sb, \"Inode bitmap for bg 0 marked \"",
                "\t\t\t\t   \"uninitialized\");",
                "\t\t\terr = -EFSCORRUPTED;",
                "\t\t\tgoto out;",
                "\t\t}"
            ],
            "deleted": [
                "\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in Linux kernel in the ext4 filesystem code. A use-after-free is possible in ext4_ext_remove_space() function when mounting and operating a crafted ext4 image.",
        "id": 1606
    },
    {
        "cve_id": "CVE-2018-9465",
        "code_before_change": "static long task_close_fd(struct binder_proc *proc, unsigned int fd)\n{\n\tint retval;\n\n\tif (proc->files == NULL)\n\t\treturn -ESRCH;\n\n\tretval = __close_fd(proc->files, fd);\n\t/* can't restart close syscall because file table entry was cleared */\n\tif (unlikely(retval == -ERESTARTSYS ||\n\t\t     retval == -ERESTARTNOINTR ||\n\t\t     retval == -ERESTARTNOHAND ||\n\t\t     retval == -ERESTART_RESTARTBLOCK))\n\t\tretval = -EINTR;\n\n\treturn retval;\n}",
        "code_after_change": "static long task_close_fd(struct binder_proc *proc, unsigned int fd)\n{\n\tint retval;\n\n\tmutex_lock(&proc->files_lock);\n\tif (proc->files == NULL) {\n\t\tretval = -ESRCH;\n\t\tgoto err;\n\t}\n\tretval = __close_fd(proc->files, fd);\n\t/* can't restart close syscall because file table entry was cleared */\n\tif (unlikely(retval == -ERESTARTSYS ||\n\t\t     retval == -ERESTARTNOINTR ||\n\t\t     retval == -ERESTARTNOHAND ||\n\t\t     retval == -ERESTART_RESTARTBLOCK))\n\t\tretval = -EINTR;\nerr:\n\tmutex_unlock(&proc->files_lock);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,9 +2,11 @@\n {\n \tint retval;\n \n-\tif (proc->files == NULL)\n-\t\treturn -ESRCH;\n-\n+\tmutex_lock(&proc->files_lock);\n+\tif (proc->files == NULL) {\n+\t\tretval = -ESRCH;\n+\t\tgoto err;\n+\t}\n \tretval = __close_fd(proc->files, fd);\n \t/* can't restart close syscall because file table entry was cleared */\n \tif (unlikely(retval == -ERESTARTSYS ||\n@@ -12,6 +14,7 @@\n \t\t     retval == -ERESTARTNOHAND ||\n \t\t     retval == -ERESTART_RESTARTBLOCK))\n \t\tretval = -EINTR;\n-\n+err:\n+\tmutex_unlock(&proc->files_lock);\n \treturn retval;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&proc->files_lock);",
                "\tif (proc->files == NULL) {",
                "\t\tretval = -ESRCH;",
                "\t\tgoto err;",
                "\t}",
                "err:",
                "\tmutex_unlock(&proc->files_lock);"
            ],
            "deleted": [
                "\tif (proc->files == NULL)",
                "\t\treturn -ESRCH;",
                "",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In task_get_unused_fd_flags of binder.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation. Product: Android Versions: Android kernel Android ID: A-69164715 References: Upstream kernel.",
        "id": 1872
    },
    {
        "cve_id": "CVE-2018-9465",
        "code_before_change": "static void task_fd_install(\n\tstruct binder_proc *proc, unsigned int fd, struct file *file)\n{\n\tif (proc->files)\n\t\t__fd_install(proc->files, fd, file);\n}",
        "code_after_change": "static void task_fd_install(\n\tstruct binder_proc *proc, unsigned int fd, struct file *file)\n{\n\tmutex_lock(&proc->files_lock);\n\tif (proc->files)\n\t\t__fd_install(proc->files, fd, file);\n\tmutex_unlock(&proc->files_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n static void task_fd_install(\n \tstruct binder_proc *proc, unsigned int fd, struct file *file)\n {\n+\tmutex_lock(&proc->files_lock);\n \tif (proc->files)\n \t\t__fd_install(proc->files, fd, file);\n+\tmutex_unlock(&proc->files_lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&proc->files_lock);",
                "\tmutex_unlock(&proc->files_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In task_get_unused_fd_flags of binder.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation. Product: Android Versions: Android kernel Android ID: A-69164715 References: Upstream kernel.",
        "id": 1868
    },
    {
        "cve_id": "CVE-2018-9465",
        "code_before_change": "static int binder_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tint ret;\n\tstruct binder_proc *proc = filp->private_data;\n\tconst char *failure_string;\n\n\tif (proc->tsk != current->group_leader)\n\t\treturn -EINVAL;\n\n\tif ((vma->vm_end - vma->vm_start) > SZ_4M)\n\t\tvma->vm_end = vma->vm_start + SZ_4M;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     __func__, proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n\n\tif (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {\n\t\tret = -EPERM;\n\t\tfailure_string = \"bad vm_flags\";\n\t\tgoto err_bad_arg;\n\t}\n\tvma->vm_flags = (vma->vm_flags | VM_DONTCOPY) & ~VM_MAYWRITE;\n\tvma->vm_ops = &binder_vm_ops;\n\tvma->vm_private_data = proc;\n\n\tret = binder_alloc_mmap_handler(&proc->alloc, vma);\n\tif (ret)\n\t\treturn ret;\n\tproc->files = get_files_struct(current);\n\treturn 0;\n\nerr_bad_arg:\n\tpr_err(\"binder_mmap: %d %lx-%lx %s failed %d\\n\",\n\t       proc->pid, vma->vm_start, vma->vm_end, failure_string, ret);\n\treturn ret;\n}",
        "code_after_change": "static int binder_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tint ret;\n\tstruct binder_proc *proc = filp->private_data;\n\tconst char *failure_string;\n\n\tif (proc->tsk != current->group_leader)\n\t\treturn -EINVAL;\n\n\tif ((vma->vm_end - vma->vm_start) > SZ_4M)\n\t\tvma->vm_end = vma->vm_start + SZ_4M;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     __func__, proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n\n\tif (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {\n\t\tret = -EPERM;\n\t\tfailure_string = \"bad vm_flags\";\n\t\tgoto err_bad_arg;\n\t}\n\tvma->vm_flags = (vma->vm_flags | VM_DONTCOPY) & ~VM_MAYWRITE;\n\tvma->vm_ops = &binder_vm_ops;\n\tvma->vm_private_data = proc;\n\n\tret = binder_alloc_mmap_handler(&proc->alloc, vma);\n\tif (ret)\n\t\treturn ret;\n\tmutex_lock(&proc->files_lock);\n\tproc->files = get_files_struct(current);\n\tmutex_unlock(&proc->files_lock);\n\treturn 0;\n\nerr_bad_arg:\n\tpr_err(\"binder_mmap: %d %lx-%lx %s failed %d\\n\",\n\t       proc->pid, vma->vm_start, vma->vm_end, failure_string, ret);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,9 @@\n \tret = binder_alloc_mmap_handler(&proc->alloc, vma);\n \tif (ret)\n \t\treturn ret;\n+\tmutex_lock(&proc->files_lock);\n \tproc->files = get_files_struct(current);\n+\tmutex_unlock(&proc->files_lock);\n \treturn 0;\n \n err_bad_arg:",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&proc->files_lock);",
                "\tmutex_unlock(&proc->files_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In task_get_unused_fd_flags of binder.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation. Product: Android Versions: Android kernel Android ID: A-69164715 References: Upstream kernel.",
        "id": 1870
    },
    {
        "cve_id": "CVE-2022-42703",
        "code_before_change": "int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\tint error;\n\n\t/* Don't bother if the parent process has no anon_vma here. */\n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t/* Drop inherited anon_vma, we'll reuse existing or allocate new. */\n\tvma->anon_vma = NULL;\n\n\t/*\n\t * First, attach the new VMA to the parent VMA's anon_vmas,\n\t * so rmap can find non-COWed pages in child processes.\n\t */\n\terror = anon_vma_clone(vma, pvma);\n\tif (error)\n\t\treturn error;\n\n\t/* An existing anon_vma has been reused, all done then. */\n\tif (vma->anon_vma)\n\t\treturn 0;\n\n\t/* Then add our own anon_vma. */\n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t/*\n\t * The root anon_vma's rwsem is the lock actually used when we\n\t * lock any of the anon_vmas in this anon_vma tree.\n\t */\n\tanon_vma->root = pvma->anon_vma->root;\n\tanon_vma->parent = pvma->anon_vma;\n\t/*\n\t * With refcounts, an anon_vma can stay around longer than the\n\t * process it belongs to. The root anon_vma needs to be pinned until\n\t * this anon_vma is freed, because the lock lives in the root.\n\t */\n\tget_anon_vma(anon_vma->root);\n\t/* Mark this anon_vma as the one where our new (COWed) pages go. */\n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma->parent->degree++;\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}",
        "code_after_change": "int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\tint error;\n\n\t/* Don't bother if the parent process has no anon_vma here. */\n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t/* Drop inherited anon_vma, we'll reuse existing or allocate new. */\n\tvma->anon_vma = NULL;\n\n\t/*\n\t * First, attach the new VMA to the parent VMA's anon_vmas,\n\t * so rmap can find non-COWed pages in child processes.\n\t */\n\terror = anon_vma_clone(vma, pvma);\n\tif (error)\n\t\treturn error;\n\n\t/* An existing anon_vma has been reused, all done then. */\n\tif (vma->anon_vma)\n\t\treturn 0;\n\n\t/* Then add our own anon_vma. */\n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tanon_vma->num_active_vmas++;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t/*\n\t * The root anon_vma's rwsem is the lock actually used when we\n\t * lock any of the anon_vmas in this anon_vma tree.\n\t */\n\tanon_vma->root = pvma->anon_vma->root;\n\tanon_vma->parent = pvma->anon_vma;\n\t/*\n\t * With refcounts, an anon_vma can stay around longer than the\n\t * process it belongs to. The root anon_vma needs to be pinned until\n\t * this anon_vma is freed, because the lock lives in the root.\n\t */\n\tget_anon_vma(anon_vma->root);\n\t/* Mark this anon_vma as the one where our new (COWed) pages go. */\n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma->parent->num_children++;\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,6 +27,7 @@\n \tanon_vma = anon_vma_alloc();\n \tif (!anon_vma)\n \t\tgoto out_error;\n+\tanon_vma->num_active_vmas++;\n \tavc = anon_vma_chain_alloc(GFP_KERNEL);\n \tif (!avc)\n \t\tgoto out_error_free_anon_vma;\n@@ -47,7 +48,7 @@\n \tvma->anon_vma = anon_vma;\n \tanon_vma_lock_write(anon_vma);\n \tanon_vma_chain_link(vma, avc, anon_vma);\n-\tanon_vma->parent->degree++;\n+\tanon_vma->parent->num_children++;\n \tanon_vma_unlock_write(anon_vma);\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tanon_vma->num_active_vmas++;",
                "\tanon_vma->parent->num_children++;"
            ],
            "deleted": [
                "\tanon_vma->parent->degree++;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "mm/rmap.c in the Linux kernel before 5.19.7 has a use-after-free related to leaf anon_vma double reuse.",
        "id": 3728
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->work.identity->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
        "code_after_change": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->task->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \t\t\tcontinue;\n \t\tif (req->file && req->file->f_op == &io_uring_fops)\n \t\t\treturn true;\n-\t\tif (req->work.identity->files == files)\n+\t\tif (req->task->files == files)\n \t\t\treturn true;\n \t}\n \treturn false;",
        "function_modified_lines": {
            "added": [
                "\t\tif (req->task->files == files)"
            ],
            "deleted": [
                "\t\tif (req->work.identity->files == files)"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3281
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n}",
        "code_after_change": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\tif (!req->work.creds)\n\t\treq->work.creds = get_current_cred();\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,4 +15,6 @@\n \t\tif (def->unbound_nonreg_file)\n \t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n \t}\n+\tif (!req->work.creds)\n+\t\treq->work.creds = get_current_cred();\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!req->work.creds)",
                "\t\treq->work.creds = get_current_cred();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3282
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n\n\t/* Grab a ref if this isn't our static identity */\n\treq->work.identity = tctx->identity;\n\tif (tctx->identity != &tctx->__identity)\n\t\trefcount_inc(&req->work.identity->count);\n}",
        "code_after_change": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,14 +1,7 @@\n static inline void io_req_init_async(struct io_kiocb *req)\n {\n-\tstruct io_uring_task *tctx = current->io_uring;\n-\n \tif (req->flags & REQ_F_WORK_INITIALIZED)\n \t\treturn;\n \n \t__io_req_init_async(req);\n-\n-\t/* Grab a ref if this isn't our static identity */\n-\treq->work.identity = tctx->identity;\n-\tif (tctx->identity != &tctx->__identity)\n-\t\trefcount_inc(&req->work.identity->count);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct io_uring_task *tctx = current->io_uring;",
                "",
                "",
                "\t/* Grab a ref if this isn't our static identity */",
                "\treq->work.identity = tctx->identity;",
                "\tif (tctx->identity != &tctx->__identity)",
                "\t\trefcount_inc(&req->work.identity->count);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3284
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
        "code_after_change": "int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n \tstruct ipv6_txoptions *opt = NULL;\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ip6_flowlabel *flowlabel = NULL;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n@@ -160,8 +161,10 @@\n \t\t\topt = NULL;\n \t\tconnected = 0;\n \t}\n-\tif (!opt)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -270,6 +273,7 @@\n out:\n \tdst_release(dst);\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \tif (!err)\n \t\treturn len;\n \t/*",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt_to_free = NULL;",
                "\tif (!opt) {",
                "\t\topt = txopt_get(np);",
                "\t\topt_to_free = opt;",
                "\t}",
                "\ttxopt_put(opt_to_free);"
            ],
            "deleted": [
                "\tif (!opt)",
                "\t\topt = np->opt;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1008
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}",
        "code_after_change": "static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,9 @@\n \tfl6->fl6_dport = inet->inet_dport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n \n-\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \n \tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n \tif (!dst) {",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 997
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "code_after_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n \tstruct dccp_sock *dp = dccp_sk(sk);\n \tstruct in6_addr *saddr = NULL, *final_p, final;\n+\tstruct ipv6_txoptions *opt;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n \tint addr_type;\n@@ -106,7 +107,8 @@\n \tfl6.fl6_sport = inet->inet_sport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \tif (IS_ERR(dst)) {\n@@ -126,9 +128,8 @@\n \t__ip6_dst_store(sk, dst, NULL, NULL);\n \n \ticsk->icsk_ext_hdr_len = 0;\n-\tif (np->opt != NULL)\n-\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n-\t\t\t\t\t  np->opt->opt_nflen);\n+\tif (opt)\n+\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n \n \tinet->inet_dport = usin->sin6_port;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\tfinal_p = fl6_update_dst(&fl6, opt, &final);",
                "\tif (opt)",
                "\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);",
                "\tif (np->opt != NULL)",
                "\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +",
                "\t\t\t\t\t  np->opt->opt_nflen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 990
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}",
        "code_after_change": "struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -87,7 +87,7 @@\n \t\tmemset(&fl6, 0, sizeof(fl6));\n \t\tfl6.flowi6_proto = IPPROTO_TCP;\n \t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n-\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n \t\tfl6.saddr = ireq->ir_v6_loc_addr;\n \t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n \t\tfl6.flowi6_mark = ireq->ir_mark;",
        "function_modified_lines": {
            "added": [
                "\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);"
            ],
            "deleted": [
                "\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1004
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}",
        "code_after_change": "int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t       np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,8 @@\n \t/* Restore final destination back after routing done */\n \tfl6.daddr = sk->sk_v6_daddr;\n \n-\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n+\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n+\t\t       np->tclass);\n \trcu_read_unlock();\n \treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),",
                "\t\t       np->tclass);"
            ],
            "deleted": [
                "\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 999
    },
    {
        "cve_id": "CVE-2023-3439",
        "code_before_change": "void mctp_dev_put(struct mctp_dev *mdev)\n{\n\tif (mdev && refcount_dec_and_test(&mdev->refs)) {\n\t\tdev_put(mdev->dev);\n\t\tkfree_rcu(mdev, rcu);\n\t}\n}",
        "code_after_change": "void mctp_dev_put(struct mctp_dev *mdev)\n{\n\tif (mdev && refcount_dec_and_test(&mdev->refs)) {\n\t\tkfree(mdev->addrs);\n\t\tdev_put(mdev->dev);\n\t\tkfree_rcu(mdev, rcu);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,7 @@\n void mctp_dev_put(struct mctp_dev *mdev)\n {\n \tif (mdev && refcount_dec_and_test(&mdev->refs)) {\n+\t\tkfree(mdev->addrs);\n \t\tdev_put(mdev->dev);\n \t\tkfree_rcu(mdev, rcu);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tkfree(mdev->addrs);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the MCTP protocol in the Linux kernel. The function mctp_unregister() reclaims the device's relevant resource when a netcard detaches. However, a running routine may be unaware of this and cause the use-after-free of the mdev->addrs object, potentially leading to a denial of service.",
        "id": 4105
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "int io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll_update *poll_update = io_kiocb_to_cmd(req);\n\tstruct io_cancel_data cd = { .data = poll_update->old_user_data, };\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_hash_bucket *bucket;\n\tstruct io_kiocb *preq;\n\tint ret2, ret = 0;\n\tbool locked;\n\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n\tif (preq)\n\t\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\n\tif (!preq) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (!ret2) {\n\t\tret = -EALREADY;\n\t\tgoto out;\n\t}\n\n\tif (poll_update->update_events || poll_update->update_user_data) {\n\t\t/* only mask one event flags, keep behavior flags */\n\t\tif (poll_update->update_events) {\n\t\t\tstruct io_poll *poll = io_kiocb_to_cmd(preq);\n\n\t\t\tpoll->events &= ~0xffff;\n\t\t\tpoll->events |= poll_update->events & 0xffff;\n\t\t\tpoll->events |= IO_POLL_UNMASK;\n\t\t}\n\t\tif (poll_update->update_user_data)\n\t\t\tpreq->cqe.user_data = poll_update->new_user_data;\n\n\t\tret2 = io_poll_add(preq, issue_flags);\n\t\t/* successfully updated, don't complete poll request */\n\t\tif (!ret2 || ret2 == -EIOCBQUEUED)\n\t\t\tgoto out;\n\t}\n\n\treq_set_fail(preq);\n\tio_req_set_res(preq, -ECANCELED, 0);\n\tlocked = !(issue_flags & IO_URING_F_UNLOCKED);\n\tio_req_task_complete(preq, &locked);\nout:\n\tif (ret < 0) {\n\t\treq_set_fail(req);\n\t\treturn ret;\n\t}\n\t/* complete update request, we're done with it */\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}",
        "code_after_change": "int io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll_update *poll_update = io_kiocb_to_cmd(req);\n\tstruct io_cancel_data cd = { .data = poll_update->old_user_data, };\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_hash_bucket *bucket;\n\tstruct io_kiocb *preq;\n\tint ret2, ret = 0;\n\tbool locked;\n\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\tif (!ret2)\n\t\tgoto found;\n\tif (ret2 != -ENOENT) {\n\t\tret = ret2;\n\t\tgoto out;\n\t}\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table_locked, &bucket);\n\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\tio_ring_submit_unlock(ctx, issue_flags);\n\tif (ret2) {\n\t\tret = ret2;\n\t\tgoto out;\n\t}\n\nfound:\n\tif (poll_update->update_events || poll_update->update_user_data) {\n\t\t/* only mask one event flags, keep behavior flags */\n\t\tif (poll_update->update_events) {\n\t\t\tstruct io_poll *poll = io_kiocb_to_cmd(preq);\n\n\t\t\tpoll->events &= ~0xffff;\n\t\t\tpoll->events |= poll_update->events & 0xffff;\n\t\t\tpoll->events |= IO_POLL_UNMASK;\n\t\t}\n\t\tif (poll_update->update_user_data)\n\t\t\tpreq->cqe.user_data = poll_update->new_user_data;\n\n\t\tret2 = io_poll_add(preq, issue_flags);\n\t\t/* successfully updated, don't complete poll request */\n\t\tif (!ret2 || ret2 == -EIOCBQUEUED)\n\t\t\tgoto out;\n\t}\n\n\treq_set_fail(preq);\n\tio_req_set_res(preq, -ECANCELED, 0);\n\tlocked = !(issue_flags & IO_URING_F_UNLOCKED);\n\tio_req_task_complete(preq, &locked);\nout:\n\tif (ret < 0) {\n\t\treq_set_fail(req);\n\t\treturn ret;\n\t}\n\t/* complete update request, we're done with it */\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,20 +9,28 @@\n \tbool locked;\n \n \tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n-\tif (preq)\n-\t\tret2 = io_poll_disarm(preq);\n+\tret2 = io_poll_disarm(preq);\n \tif (bucket)\n \t\tspin_unlock(&bucket->lock);\n-\n-\tif (!preq) {\n-\t\tret = -ENOENT;\n-\t\tgoto out;\n-\t}\n-\tif (!ret2) {\n-\t\tret = -EALREADY;\n+\tif (!ret2)\n+\t\tgoto found;\n+\tif (ret2 != -ENOENT) {\n+\t\tret = ret2;\n \t\tgoto out;\n \t}\n \n+\tio_ring_submit_lock(ctx, issue_flags);\n+\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table_locked, &bucket);\n+\tret2 = io_poll_disarm(preq);\n+\tif (bucket)\n+\t\tspin_unlock(&bucket->lock);\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\tif (ret2) {\n+\t\tret = ret2;\n+\t\tgoto out;\n+\t}\n+\n+found:\n \tif (poll_update->update_events || poll_update->update_user_data) {\n \t\t/* only mask one event flags, keep behavior flags */\n \t\tif (poll_update->update_events) {",
        "function_modified_lines": {
            "added": [
                "\tret2 = io_poll_disarm(preq);",
                "\tif (!ret2)",
                "\t\tgoto found;",
                "\tif (ret2 != -ENOENT) {",
                "\t\tret = ret2;",
                "\tio_ring_submit_lock(ctx, issue_flags);",
                "\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table_locked, &bucket);",
                "\tret2 = io_poll_disarm(preq);",
                "\tif (bucket)",
                "\t\tspin_unlock(&bucket->lock);",
                "\tio_ring_submit_unlock(ctx, issue_flags);",
                "\tif (ret2) {",
                "\t\tret = ret2;",
                "\t\tgoto out;",
                "\t}",
                "",
                "found:"
            ],
            "deleted": [
                "\tif (preq)",
                "\t\tret2 = io_poll_disarm(preq);",
                "",
                "\tif (!preq) {",
                "\t\tret = -ENOENT;",
                "\t\tgoto out;",
                "\t}",
                "\tif (!ret2) {",
                "\t\tret = -EALREADY;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and\u00a00e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4069
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "int io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n\tint ret;\n\n\tif (!def->pollin && !def->pollout)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!file_can_poll(req->file))\n\t\treturn IO_APOLL_ABORTED;\n\tif ((req->flags & (REQ_F_POLLED|REQ_F_PARTIAL_IO)) == REQ_F_POLLED)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!(req->flags & REQ_F_APOLL_MULTISHOT))\n\t\tmask |= EPOLLONESHOT;\n\n\tif (def->pollin) {\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\t\tif (req->flags & REQ_F_CLEAR_POLLIN)\n\t\t\tmask &= ~EPOLLIN;\n\t} else {\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\tif (def->poll_exclusive)\n\t\tmask |= EPOLLEXCLUSIVE;\n\tif (req->flags & REQ_F_POLLED) {\n\t\tapoll = req->apoll;\n\t\tkfree(apoll->double_poll);\n\t} else if (!(issue_flags & IO_URING_F_UNLOCKED) &&\n\t\t   !list_empty(&ctx->apoll_cache)) {\n\t\tapoll = list_first_entry(&ctx->apoll_cache, struct async_poll,\n\t\t\t\t\t\tpoll.wait.entry);\n\t\tlist_del_init(&apoll->poll.wait.entry);\n\t} else {\n\t\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\t\tif (unlikely(!apoll))\n\t\t\treturn IO_APOLL_ABORTED;\n\t}\n\tapoll->double_poll = NULL;\n\treq->apoll = apoll;\n\treq->flags |= REQ_F_POLLED;\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tio_kbuf_recycle(req, issue_flags);\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask);\n\tif (ret || ipt.error)\n\t\treturn ret ? IO_APOLL_READY : IO_APOLL_ABORTED;\n\n\ttrace_io_uring_poll_arm(ctx, req, req->cqe.user_data, req->opcode,\n\t\t\t\tmask, apoll->poll.events);\n\treturn IO_APOLL_OK;\n}",
        "code_after_change": "int io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n\tint ret;\n\n\t/*\n\t * apoll requests already grab the mutex to complete in the tw handler,\n\t * so removal from the mutex-backed hash is free, use it by default.\n\t */\n\tif (issue_flags & IO_URING_F_UNLOCKED)\n\t\treq->flags &= ~REQ_F_HASH_LOCKED;\n\telse\n\t\treq->flags |= REQ_F_HASH_LOCKED;\n\n\tif (!def->pollin && !def->pollout)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!file_can_poll(req->file))\n\t\treturn IO_APOLL_ABORTED;\n\tif ((req->flags & (REQ_F_POLLED|REQ_F_PARTIAL_IO)) == REQ_F_POLLED)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!(req->flags & REQ_F_APOLL_MULTISHOT))\n\t\tmask |= EPOLLONESHOT;\n\n\tif (def->pollin) {\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\t\tif (req->flags & REQ_F_CLEAR_POLLIN)\n\t\t\tmask &= ~EPOLLIN;\n\t} else {\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\tif (def->poll_exclusive)\n\t\tmask |= EPOLLEXCLUSIVE;\n\tif (req->flags & REQ_F_POLLED) {\n\t\tapoll = req->apoll;\n\t\tkfree(apoll->double_poll);\n\t} else if (!(issue_flags & IO_URING_F_UNLOCKED) &&\n\t\t   !list_empty(&ctx->apoll_cache)) {\n\t\tapoll = list_first_entry(&ctx->apoll_cache, struct async_poll,\n\t\t\t\t\t\tpoll.wait.entry);\n\t\tlist_del_init(&apoll->poll.wait.entry);\n\t} else {\n\t\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\t\tif (unlikely(!apoll))\n\t\t\treturn IO_APOLL_ABORTED;\n\t}\n\tapoll->double_poll = NULL;\n\treq->apoll = apoll;\n\treq->flags |= REQ_F_POLLED;\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tio_kbuf_recycle(req, issue_flags);\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask);\n\tif (ret || ipt.error)\n\t\treturn ret ? IO_APOLL_READY : IO_APOLL_ABORTED;\n\n\ttrace_io_uring_poll_arm(ctx, req, req->cqe.user_data, req->opcode,\n\t\t\t\tmask, apoll->poll.events);\n\treturn IO_APOLL_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,15 @@\n \tstruct io_poll_table ipt;\n \t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n \tint ret;\n+\n+\t/*\n+\t * apoll requests already grab the mutex to complete in the tw handler,\n+\t * so removal from the mutex-backed hash is free, use it by default.\n+\t */\n+\tif (issue_flags & IO_URING_F_UNLOCKED)\n+\t\treq->flags &= ~REQ_F_HASH_LOCKED;\n+\telse\n+\t\treq->flags |= REQ_F_HASH_LOCKED;\n \n \tif (!def->pollin && !def->pollout)\n \t\treturn IO_APOLL_ABORTED;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * apoll requests already grab the mutex to complete in the tw handler,",
                "\t * so removal from the mutex-backed hash is free, use it by default.",
                "\t */",
                "\tif (issue_flags & IO_URING_F_UNLOCKED)",
                "\t\treq->flags &= ~REQ_F_HASH_LOCKED;",
                "\telse",
                "\t\treq->flags |= REQ_F_HASH_LOCKED;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and\u00a00e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4070
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "static __cold void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tunsigned long index;\n\tstruct creds *creds;\n\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\txa_for_each(&ctx->personalities, index, creds)\n\t\tio_unregister_personality(ctx, index);\n\tmutex_unlock(&ctx->uring_lock);\n\n\t/* failed during ring init, it couldn't have issued any requests */\n\tif (ctx->rings) {\n\t\tio_kill_timeouts(ctx, NULL, true);\n\t\tio_poll_remove_all(ctx, NULL, true);\n\t\t/* if we failed setting up the ctx, we might not have any rings */\n\t\tio_iopoll_try_reap_events(ctx);\n\t}\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}",
        "code_after_change": "static __cold void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tunsigned long index;\n\tstruct creds *creds;\n\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\txa_for_each(&ctx->personalities, index, creds)\n\t\tio_unregister_personality(ctx, index);\n\tif (ctx->rings)\n\t\tio_poll_remove_all(ctx, NULL, true);\n\tmutex_unlock(&ctx->uring_lock);\n\n\t/* failed during ring init, it couldn't have issued any requests */\n\tif (ctx->rings) {\n\t\tio_kill_timeouts(ctx, NULL, true);\n\t\t/* if we failed setting up the ctx, we might not have any rings */\n\t\tio_iopoll_try_reap_events(ctx);\n\t}\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,12 +9,13 @@\n \t\t__io_cqring_overflow_flush(ctx, true);\n \txa_for_each(&ctx->personalities, index, creds)\n \t\tio_unregister_personality(ctx, index);\n+\tif (ctx->rings)\n+\t\tio_poll_remove_all(ctx, NULL, true);\n \tmutex_unlock(&ctx->uring_lock);\n \n \t/* failed during ring init, it couldn't have issued any requests */\n \tif (ctx->rings) {\n \t\tio_kill_timeouts(ctx, NULL, true);\n-\t\tio_poll_remove_all(ctx, NULL, true);\n \t\t/* if we failed setting up the ctx, we might not have any rings */\n \t\tio_iopoll_try_reap_events(ctx);\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (ctx->rings)",
                "\t\tio_poll_remove_all(ctx, NULL, true);"
            ],
            "deleted": [
                "\t\tio_poll_remove_all(ctx, NULL, true);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and\u00a00e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4065
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "static bool io_poll_disarm(struct io_kiocb *req)\n{\n\tif (!io_poll_get_ownership(req))\n\t\treturn false;\n\tio_poll_remove_entries(req);\n\thash_del(&req->hash_node);\n\treturn true;\n}",
        "code_after_change": "static int io_poll_disarm(struct io_kiocb *req)\n{\n\tif (!req)\n\t\treturn -ENOENT;\n\tif (!io_poll_get_ownership(req))\n\t\treturn -EALREADY;\n\tio_poll_remove_entries(req);\n\thash_del(&req->hash_node);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,10 @@\n-static bool io_poll_disarm(struct io_kiocb *req)\n+static int io_poll_disarm(struct io_kiocb *req)\n {\n+\tif (!req)\n+\t\treturn -ENOENT;\n \tif (!io_poll_get_ownership(req))\n-\t\treturn false;\n+\t\treturn -EALREADY;\n \tio_poll_remove_entries(req);\n \thash_del(&req->hash_node);\n-\treturn true;\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "static int io_poll_disarm(struct io_kiocb *req)",
                "\tif (!req)",
                "\t\treturn -ENOENT;",
                "\t\treturn -EALREADY;",
                "\treturn 0;"
            ],
            "deleted": [
                "static bool io_poll_disarm(struct io_kiocb *req)",
                "\t\treturn false;",
                "\treturn true;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and\u00a00e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4074
    },
    {
        "cve_id": "CVE-2023-4611",
        "code_before_change": "\nSYSCALL_DEFINE4(set_mempolicy_home_node, unsigned long, start, unsigned long, len,\n\t\tunsigned long, home_node, unsigned long, flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mempolicy *new, *old;\n\tunsigned long end;\n\tint err = -ENOENT;\n\tVMA_ITERATOR(vmi, mm, start);\n\n\tstart = untagged_addr(start);\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\t/*\n\t * flags is used for future extension if any.\n\t */\n\tif (flags != 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Check home_node is online to avoid accessing uninitialized\n\t * NODE_DATA.\n\t */\n\tif (home_node >= MAX_NUMNODES || !node_online(home_node))\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tmmap_write_lock(mm);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\t/*\n\t\t * If any vma in the range got policy other than MPOL_BIND\n\t\t * or MPOL_PREFERRED_MANY we return error. We don't reset\n\t\t * the home node for vmas we already updated before.\n\t\t */\n\t\told = vma_policy(vma);\n\t\tif (!old)\n\t\t\tcontinue;\n\t\tif (old->mode != MPOL_BIND && old->mode != MPOL_PREFERRED_MANY) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tnew = mpol_dup(old);\n\t\tif (IS_ERR(new)) {\n\t\t\terr = PTR_ERR(new);\n\t\t\tbreak;\n\t\t}\n\n\t\tnew->home_node = home_node;\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tmpol_put(new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tmmap_write_unlock(mm);\n\treturn err;\n}",
        "code_after_change": "\nSYSCALL_DEFINE4(set_mempolicy_home_node, unsigned long, start, unsigned long, len,\n\t\tunsigned long, home_node, unsigned long, flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mempolicy *new, *old;\n\tunsigned long end;\n\tint err = -ENOENT;\n\tVMA_ITERATOR(vmi, mm, start);\n\n\tstart = untagged_addr(start);\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\t/*\n\t * flags is used for future extension if any.\n\t */\n\tif (flags != 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Check home_node is online to avoid accessing uninitialized\n\t * NODE_DATA.\n\t */\n\tif (home_node >= MAX_NUMNODES || !node_online(home_node))\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tmmap_write_lock(mm);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\t/*\n\t\t * If any vma in the range got policy other than MPOL_BIND\n\t\t * or MPOL_PREFERRED_MANY we return error. We don't reset\n\t\t * the home node for vmas we already updated before.\n\t\t */\n\t\told = vma_policy(vma);\n\t\tif (!old)\n\t\t\tcontinue;\n\t\tif (old->mode != MPOL_BIND && old->mode != MPOL_PREFERRED_MANY) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tnew = mpol_dup(old);\n\t\tif (IS_ERR(new)) {\n\t\t\terr = PTR_ERR(new);\n\t\t\tbreak;\n\t\t}\n\n\t\tvma_start_write(vma);\n\t\tnew->home_node = home_node;\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tmpol_put(new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tmmap_write_unlock(mm);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -53,6 +53,7 @@\n \t\t\tbreak;\n \t\t}\n \n+\t\tvma_start_write(vma);\n \t\tnew->home_node = home_node;\n \t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n \t\tmpol_put(new);",
        "function_modified_lines": {
            "added": [
                "\t\tvma_start_write(vma);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in mm/mempolicy.c in the memory management subsystem in the Linux Kernel. This issue is caused by a race between mbind() and VMA-locked page fault, and may allow a local attacker to crash the system or lead to a kernel information leak.",
        "id": 4235
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "void nfc_llcp_mac_is_up(struct nfc_dev *dev, u32 target_idx,\n\t\t\tu8 comm_mode, u8 rf_mode)\n{\n\tstruct nfc_llcp_local *local;\n\n\tpr_debug(\"rf mode %d\\n\", rf_mode);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->target_idx = target_idx;\n\tlocal->comm_mode = comm_mode;\n\tlocal->rf_mode = rf_mode;\n\n\tif (rf_mode == NFC_RF_INITIATOR) {\n\t\tpr_debug(\"Queueing Tx work\\n\");\n\n\t\tschedule_work(&local->tx_work);\n\t} else {\n\t\tmod_timer(&local->link_timer,\n\t\t\t  jiffies + msecs_to_jiffies(local->remote_lto));\n\t}\n}",
        "code_after_change": "void nfc_llcp_mac_is_up(struct nfc_dev *dev, u32 target_idx,\n\t\t\tu8 comm_mode, u8 rf_mode)\n{\n\tstruct nfc_llcp_local *local;\n\n\tpr_debug(\"rf mode %d\\n\", rf_mode);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->target_idx = target_idx;\n\tlocal->comm_mode = comm_mode;\n\tlocal->rf_mode = rf_mode;\n\n\tif (rf_mode == NFC_RF_INITIATOR) {\n\t\tpr_debug(\"Queueing Tx work\\n\");\n\n\t\tschedule_work(&local->tx_work);\n\t} else {\n\t\tmod_timer(&local->link_timer,\n\t\t\t  jiffies + msecs_to_jiffies(local->remote_lto));\n\t}\n\n\tnfc_llcp_local_put(local);\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,4 +21,6 @@\n \t\tmod_timer(&local->link_timer,\n \t\t\t  jiffies + msecs_to_jiffies(local->remote_lto));\n \t}\n+\n+\tnfc_llcp_local_put(local);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tnfc_llcp_local_put(local);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4144
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "struct nfc_llcp_local *nfc_llcp_find_local(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlist_for_each_entry(local, &llcp_devices, list)\n\t\tif (local->dev == dev)\n\t\t\treturn local;\n\n\tpr_debug(\"No device found\\n\");\n\n\treturn NULL;\n}",
        "code_after_change": "struct nfc_llcp_local *nfc_llcp_find_local(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_llcp_local *res = NULL;\n\n\tspin_lock(&llcp_devices_lock);\n\tlist_for_each_entry(local, &llcp_devices, list)\n\t\tif (local->dev == dev) {\n\t\t\tres = nfc_llcp_local_get(local);\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&llcp_devices_lock);\n\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,15 @@\n struct nfc_llcp_local *nfc_llcp_find_local(struct nfc_dev *dev)\n {\n \tstruct nfc_llcp_local *local;\n+\tstruct nfc_llcp_local *res = NULL;\n \n+\tspin_lock(&llcp_devices_lock);\n \tlist_for_each_entry(local, &llcp_devices, list)\n-\t\tif (local->dev == dev)\n-\t\t\treturn local;\n+\t\tif (local->dev == dev) {\n+\t\t\tres = nfc_llcp_local_get(local);\n+\t\t\tbreak;\n+\t\t}\n+\tspin_unlock(&llcp_devices_lock);\n \n-\tpr_debug(\"No device found\\n\");\n-\n-\treturn NULL;\n+\treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct nfc_llcp_local *res = NULL;",
                "\tspin_lock(&llcp_devices_lock);",
                "\t\tif (local->dev == dev) {",
                "\t\t\tres = nfc_llcp_local_get(local);",
                "\t\t\tbreak;",
                "\t\t}",
                "\tspin_unlock(&llcp_devices_lock);",
                "\treturn res;"
            ],
            "deleted": [
                "\t\tif (local->dev == dev)",
                "\t\t\treturn local;",
                "\tpr_debug(\"No device found\\n\");",
                "",
                "\treturn NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4145
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "void nfc_llcp_unregister_device(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local = nfc_llcp_find_local(dev);\n\n\tif (local == NULL) {\n\t\tpr_debug(\"No such device\\n\");\n\t\treturn;\n\t}\n\n\tlocal_cleanup(local);\n\n\tnfc_llcp_local_put(local);\n}",
        "code_after_change": "void nfc_llcp_unregister_device(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local = nfc_llcp_remove_local(dev);\n\n\tif (local == NULL) {\n\t\tpr_debug(\"No such device\\n\");\n\t\treturn;\n\t}\n\n\tlocal_cleanup(local);\n\n\tnfc_llcp_local_put(local);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n void nfc_llcp_unregister_device(struct nfc_dev *dev)\n {\n-\tstruct nfc_llcp_local *local = nfc_llcp_find_local(dev);\n+\tstruct nfc_llcp_local *local = nfc_llcp_remove_local(dev);\n \n \tif (local == NULL) {\n \t\tpr_debug(\"No such device\\n\");",
        "function_modified_lines": {
            "added": [
                "\tstruct nfc_llcp_local *local = nfc_llcp_remove_local(dev);"
            ],
            "deleted": [
                "\tstruct nfc_llcp_local *local = nfc_llcp_find_local(dev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4146
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "void nfc_llcp_mac_is_down(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->remote_miu = LLCP_DEFAULT_MIU;\n\tlocal->remote_lto = LLCP_DEFAULT_LTO;\n\n\t/* Close and purge all existing sockets */\n\tnfc_llcp_socket_release(local, true, 0);\n}",
        "code_after_change": "void nfc_llcp_mac_is_down(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->remote_miu = LLCP_DEFAULT_MIU;\n\tlocal->remote_lto = LLCP_DEFAULT_LTO;\n\n\t/* Close and purge all existing sockets */\n\tnfc_llcp_socket_release(local, true, 0);\n\n\tnfc_llcp_local_put(local);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,4 +11,6 @@\n \n \t/* Close and purge all existing sockets */\n \tnfc_llcp_socket_release(local, true, 0);\n+\n+\tnfc_llcp_local_put(local);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tnfc_llcp_local_put(local);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4152
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "static int llcp_sock_connect(struct socket *sock, struct sockaddr *_addr,\n\t\t\t     int len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct sockaddr_nfc_llcp *addr = (struct sockaddr_nfc_llcp *)_addr;\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tint ret = 0;\n\n\tpr_debug(\"sock %p sk %p flags 0x%x\\n\", sock, sk, flags);\n\n\tif (!addr || len < sizeof(*addr) || addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tif (addr->service_name_len == 0 && addr->dsap == 0)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"addr dev_idx=%u target_idx=%u protocol=%u\\n\", addr->dev_idx,\n\t\t addr->target_idx, addr->nfc_protocol);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == LLCP_CONNECTED) {\n\t\tret = -EISCONN;\n\t\tgoto error;\n\t}\n\tif (sk->sk_state == LLCP_CONNECTING) {\n\t\tret = -EINPROGRESS;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(addr->dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tdevice_lock(&dev->dev);\n\tif (dev->dep_link_up == false) {\n\t\tret = -ENOLINK;\n\t\tdevice_unlock(&dev->dev);\n\t\tgoto put_dev;\n\t}\n\tdevice_unlock(&dev->dev);\n\n\tif (local->rf_mode == NFC_RF_INITIATOR &&\n\t    addr->target_idx != local->target_idx) {\n\t\tret = -ENOLINK;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = nfc_llcp_local_get(local);\n\tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_put_local;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\n\tif (addr->service_name_len == 0)\n\t\tllcp_sock->dsap = addr->dsap;\n\telse\n\t\tllcp_sock->dsap = LLCP_SAP_SDP;\n\tllcp_sock->nfc_protocol = addr->nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    addr->service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(addr->service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_release;\n\t}\n\n\tnfc_llcp_sock_link(&local->connecting_sockets, sk);\n\n\tret = nfc_llcp_send_connect(llcp_sock);\n\tif (ret)\n\t\tgoto sock_unlink;\n\n\tsk->sk_state = LLCP_CONNECTING;\n\n\tret = sock_wait_state(sk, LLCP_CONNECTED,\n\t\t\t      sock_sndtimeo(sk, flags & O_NONBLOCK));\n\tif (ret && ret != -EINPROGRESS)\n\t\tgoto sock_unlink;\n\n\trelease_sock(sk);\n\n\treturn ret;\n\nsock_unlink:\n\tnfc_llcp_sock_unlink(&local->connecting_sockets, sk);\n\tkfree(llcp_sock->service_name);\n\tllcp_sock->service_name = NULL;\n\nsock_llcp_release:\n\tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n\nsock_llcp_put_local:\n\tnfc_llcp_local_put(llcp_sock->local);\n\tllcp_sock->local = NULL;\n\tllcp_sock->dev = NULL;\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "code_after_change": "static int llcp_sock_connect(struct socket *sock, struct sockaddr *_addr,\n\t\t\t     int len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct sockaddr_nfc_llcp *addr = (struct sockaddr_nfc_llcp *)_addr;\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tint ret = 0;\n\n\tpr_debug(\"sock %p sk %p flags 0x%x\\n\", sock, sk, flags);\n\n\tif (!addr || len < sizeof(*addr) || addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tif (addr->service_name_len == 0 && addr->dsap == 0)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"addr dev_idx=%u target_idx=%u protocol=%u\\n\", addr->dev_idx,\n\t\t addr->target_idx, addr->nfc_protocol);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == LLCP_CONNECTED) {\n\t\tret = -EISCONN;\n\t\tgoto error;\n\t}\n\tif (sk->sk_state == LLCP_CONNECTING) {\n\t\tret = -EINPROGRESS;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(addr->dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tdevice_lock(&dev->dev);\n\tif (dev->dep_link_up == false) {\n\t\tret = -ENOLINK;\n\t\tdevice_unlock(&dev->dev);\n\t\tgoto sock_llcp_put_local;\n\t}\n\tdevice_unlock(&dev->dev);\n\n\tif (local->rf_mode == NFC_RF_INITIATOR &&\n\t    addr->target_idx != local->target_idx) {\n\t\tret = -ENOLINK;\n\t\tgoto sock_llcp_put_local;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = local;\n\tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_nullify;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\n\tif (addr->service_name_len == 0)\n\t\tllcp_sock->dsap = addr->dsap;\n\telse\n\t\tllcp_sock->dsap = LLCP_SAP_SDP;\n\tllcp_sock->nfc_protocol = addr->nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    addr->service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(addr->service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_release;\n\t}\n\n\tnfc_llcp_sock_link(&local->connecting_sockets, sk);\n\n\tret = nfc_llcp_send_connect(llcp_sock);\n\tif (ret)\n\t\tgoto sock_unlink;\n\n\tsk->sk_state = LLCP_CONNECTING;\n\n\tret = sock_wait_state(sk, LLCP_CONNECTED,\n\t\t\t      sock_sndtimeo(sk, flags & O_NONBLOCK));\n\tif (ret && ret != -EINPROGRESS)\n\t\tgoto sock_unlink;\n\n\trelease_sock(sk);\n\n\treturn ret;\n\nsock_unlink:\n\tnfc_llcp_sock_unlink(&local->connecting_sockets, sk);\n\tkfree(llcp_sock->service_name);\n\tllcp_sock->service_name = NULL;\n\nsock_llcp_release:\n\tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n\nsock_llcp_nullify:\n\tllcp_sock->local = NULL;\n\tllcp_sock->dev = NULL;\n\nsock_llcp_put_local:\n\tnfc_llcp_local_put(local);\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -46,22 +46,22 @@\n \tif (dev->dep_link_up == false) {\n \t\tret = -ENOLINK;\n \t\tdevice_unlock(&dev->dev);\n-\t\tgoto put_dev;\n+\t\tgoto sock_llcp_put_local;\n \t}\n \tdevice_unlock(&dev->dev);\n \n \tif (local->rf_mode == NFC_RF_INITIATOR &&\n \t    addr->target_idx != local->target_idx) {\n \t\tret = -ENOLINK;\n-\t\tgoto put_dev;\n+\t\tgoto sock_llcp_put_local;\n \t}\n \n \tllcp_sock->dev = dev;\n-\tllcp_sock->local = nfc_llcp_local_get(local);\n+\tllcp_sock->local = local;\n \tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n \tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n \t\tret = -ENOMEM;\n-\t\tgoto sock_llcp_put_local;\n+\t\tgoto sock_llcp_nullify;\n \t}\n \n \tllcp_sock->reserved_ssap = llcp_sock->ssap;\n@@ -107,10 +107,12 @@\n sock_llcp_release:\n \tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n \n-sock_llcp_put_local:\n-\tnfc_llcp_local_put(llcp_sock->local);\n+sock_llcp_nullify:\n \tllcp_sock->local = NULL;\n \tllcp_sock->dev = NULL;\n+\n+sock_llcp_put_local:\n+\tnfc_llcp_local_put(local);\n \n put_dev:\n \tnfc_put_device(dev);",
        "function_modified_lines": {
            "added": [
                "\t\tgoto sock_llcp_put_local;",
                "\t\tgoto sock_llcp_put_local;",
                "\tllcp_sock->local = local;",
                "\t\tgoto sock_llcp_nullify;",
                "sock_llcp_nullify:",
                "",
                "sock_llcp_put_local:",
                "\tnfc_llcp_local_put(local);"
            ],
            "deleted": [
                "\t\tgoto put_dev;",
                "\t\tgoto put_dev;",
                "\tllcp_sock->local = nfc_llcp_local_get(local);",
                "\t\tgoto sock_llcp_put_local;",
                "sock_llcp_put_local:",
                "\tnfc_llcp_local_put(llcp_sock->local);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4153
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "int nfc_llcp_send_symm(struct nfc_dev *dev)\n{\n\tstruct sk_buff *skb;\n\tstruct nfc_llcp_local *local;\n\tu16 size = 0;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\tsize += LLCP_HEADER_SIZE;\n\tsize += dev->tx_headroom + dev->tx_tailroom + NFC_HEADER_SIZE;\n\n\tskb = alloc_skb(size, GFP_KERNEL);\n\tif (skb == NULL)\n\t\treturn -ENOMEM;\n\n\tskb_reserve(skb, dev->tx_headroom + NFC_HEADER_SIZE);\n\n\tskb = llcp_add_header(skb, 0, 0, LLCP_PDU_SYMM);\n\n\t__net_timestamp(skb);\n\n\tnfc_llcp_send_to_raw_sock(local, skb, NFC_DIRECTION_TX);\n\n\treturn nfc_data_exchange(dev, local->target_idx, skb,\n\t\t\t\t nfc_llcp_recv, local);\n}",
        "code_after_change": "int nfc_llcp_send_symm(struct nfc_dev *dev)\n{\n\tstruct sk_buff *skb;\n\tstruct nfc_llcp_local *local;\n\tu16 size = 0;\n\tint err;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\tsize += LLCP_HEADER_SIZE;\n\tsize += dev->tx_headroom + dev->tx_tailroom + NFC_HEADER_SIZE;\n\n\tskb = alloc_skb(size, GFP_KERNEL);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tskb_reserve(skb, dev->tx_headroom + NFC_HEADER_SIZE);\n\n\tskb = llcp_add_header(skb, 0, 0, LLCP_PDU_SYMM);\n\n\t__net_timestamp(skb);\n\n\tnfc_llcp_send_to_raw_sock(local, skb, NFC_DIRECTION_TX);\n\n\terr = nfc_data_exchange(dev, local->target_idx, skb,\n\t\t\t\t nfc_llcp_recv, local);\nout:\n\tnfc_llcp_local_put(local);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \tstruct sk_buff *skb;\n \tstruct nfc_llcp_local *local;\n \tu16 size = 0;\n+\tint err;\n \n \tlocal = nfc_llcp_find_local(dev);\n \tif (local == NULL)\n@@ -12,8 +13,10 @@\n \tsize += dev->tx_headroom + dev->tx_tailroom + NFC_HEADER_SIZE;\n \n \tskb = alloc_skb(size, GFP_KERNEL);\n-\tif (skb == NULL)\n-\t\treturn -ENOMEM;\n+\tif (skb == NULL) {\n+\t\terr = -ENOMEM;\n+\t\tgoto out;\n+\t}\n \n \tskb_reserve(skb, dev->tx_headroom + NFC_HEADER_SIZE);\n \n@@ -23,6 +26,9 @@\n \n \tnfc_llcp_send_to_raw_sock(local, skb, NFC_DIRECTION_TX);\n \n-\treturn nfc_data_exchange(dev, local->target_idx, skb,\n+\terr = nfc_data_exchange(dev, local->target_idx, skb,\n \t\t\t\t nfc_llcp_recv, local);\n+out:\n+\tnfc_llcp_local_put(local);\n+\treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tint err;",
                "\tif (skb == NULL) {",
                "\t\terr = -ENOMEM;",
                "\t\tgoto out;",
                "\t}",
                "\terr = nfc_data_exchange(dev, local->target_idx, skb,",
                "out:",
                "\tnfc_llcp_local_put(local);",
                "\treturn err;"
            ],
            "deleted": [
                "\tif (skb == NULL)",
                "\t\treturn -ENOMEM;",
                "\treturn nfc_data_exchange(dev, local->target_idx, skb,"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4143
    },
    {
        "cve_id": "CVE-2019-11487",
        "code_before_change": "static int link_pipe(struct pipe_inode_info *ipipe,\n\t\t     struct pipe_inode_info *opipe,\n\t\t     size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, i = 0, nbuf;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If we have iterated all input buffers or ran out of\n\t\t * output room, break.\n\t\t */\n\t\tif (i >= ipipe->nrbufs || opipe->nrbufs >= opipe->buffers)\n\t\t\tbreak;\n\n\t\tibuf = ipipe->bufs + ((ipipe->curbuf + i) & (ipipe->buffers-1));\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\n\t\t/*\n\t\t * Get a reference to this pipe buffer,\n\t\t * so we can copy the contents over.\n\t\t */\n\t\tpipe_buf_get(ipipe, ibuf);\n\n\t\tobuf = opipe->bufs + nbuf;\n\t\t*obuf = *ibuf;\n\n\t\t/*\n\t\t * Don't inherit the gift flag, we need to\n\t\t * prevent multiple steals of this page.\n\t\t */\n\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\tif (obuf->len > len)\n\t\t\tobuf->len = len;\n\n\t\topipe->nrbufs++;\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t\ti++;\n\t} while (len);\n\n\t/*\n\t * return EAGAIN if we have the potential of some data in the\n\t * future, otherwise just return 0\n\t */\n\tif (!ret && ipipe->waiting_writers && (flags & SPLICE_F_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\treturn ret;\n}",
        "code_after_change": "static int link_pipe(struct pipe_inode_info *ipipe,\n\t\t     struct pipe_inode_info *opipe,\n\t\t     size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, i = 0, nbuf;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If we have iterated all input buffers or ran out of\n\t\t * output room, break.\n\t\t */\n\t\tif (i >= ipipe->nrbufs || opipe->nrbufs >= opipe->buffers)\n\t\t\tbreak;\n\n\t\tibuf = ipipe->bufs + ((ipipe->curbuf + i) & (ipipe->buffers-1));\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\n\t\t/*\n\t\t * Get a reference to this pipe buffer,\n\t\t * so we can copy the contents over.\n\t\t */\n\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n\t\t\tif (ret == 0)\n\t\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tobuf = opipe->bufs + nbuf;\n\t\t*obuf = *ibuf;\n\n\t\t/*\n\t\t * Don't inherit the gift flag, we need to\n\t\t * prevent multiple steals of this page.\n\t\t */\n\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\tif (obuf->len > len)\n\t\t\tobuf->len = len;\n\n\t\topipe->nrbufs++;\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t\ti++;\n\t} while (len);\n\n\t/*\n\t * return EAGAIN if we have the potential of some data in the\n\t * future, otherwise just return 0\n\t */\n\tif (!ret && ipipe->waiting_writers && (flags & SPLICE_F_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,7 +34,11 @@\n \t\t * Get a reference to this pipe buffer,\n \t\t * so we can copy the contents over.\n \t\t */\n-\t\tpipe_buf_get(ipipe, ibuf);\n+\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n+\t\t\tif (ret == 0)\n+\t\t\t\tret = -EFAULT;\n+\t\t\tbreak;\n+\t\t}\n \n \t\tobuf = opipe->bufs + nbuf;\n \t\t*obuf = *ibuf;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!pipe_buf_get(ipipe, ibuf)) {",
                "\t\t\tif (ret == 0)",
                "\t\t\t\tret = -EFAULT;",
                "\t\t\tbreak;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tpipe_buf_get(ipipe, ibuf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests.",
        "id": 1920
    },
    {
        "cve_id": "CVE-2022-45919",
        "code_before_change": "static int dvb_ca_en50221_io_release(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_ca_private *ca = dvbdev->priv;\n\tint err;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\t/* mark the CA device as closed */\n\tca->open = 0;\n\tdvb_ca_en50221_thread_update_delay(ca);\n\n\terr = dvb_generic_release(inode, file);\n\n\tmodule_put(ca->pub->owner);\n\n\tdvb_ca_private_put(ca);\n\n\treturn err;\n}",
        "code_after_change": "static int dvb_ca_en50221_io_release(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_ca_private *ca = dvbdev->priv;\n\tint err;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\tmutex_lock(&ca->remove_mutex);\n\n\t/* mark the CA device as closed */\n\tca->open = 0;\n\tdvb_ca_en50221_thread_update_delay(ca);\n\n\terr = dvb_generic_release(inode, file);\n\n\tmodule_put(ca->pub->owner);\n\n\tdvb_ca_private_put(ca);\n\n\tif (dvbdev->users == 1 && ca->exit == 1) {\n\t\tmutex_unlock(&ca->remove_mutex);\n\t\twake_up(&dvbdev->wait_queue);\n\t} else {\n\t\tmutex_unlock(&ca->remove_mutex);\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,8 @@\n \tint err;\n \n \tdprintk(\"%s\\n\", __func__);\n+\n+\tmutex_lock(&ca->remove_mutex);\n \n \t/* mark the CA device as closed */\n \tca->open = 0;\n@@ -16,5 +18,12 @@\n \n \tdvb_ca_private_put(ca);\n \n+\tif (dvbdev->users == 1 && ca->exit == 1) {\n+\t\tmutex_unlock(&ca->remove_mutex);\n+\t\twake_up(&dvbdev->wait_queue);\n+\t} else {\n+\t\tmutex_unlock(&ca->remove_mutex);\n+\t}\n+\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tmutex_lock(&ca->remove_mutex);",
                "\tif (dvbdev->users == 1 && ca->exit == 1) {",
                "\t\tmutex_unlock(&ca->remove_mutex);",
                "\t\twake_up(&dvbdev->wait_queue);",
                "\t} else {",
                "\t\tmutex_unlock(&ca->remove_mutex);",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.10. In drivers/media/dvb-core/dvb_ca_en50221.c, a use-after-free can occur is there is a disconnect after an open, because of the lack of a wait_event.",
        "id": 3756
    },
    {
        "cve_id": "CVE-2023-32233",
        "code_before_change": "void nf_tables_deactivate_set(const struct nft_ctx *ctx, struct nft_set *set,\n\t\t\t      struct nft_set_binding *binding,\n\t\t\t      enum nft_trans_phase phase)\n{\n\tswitch (phase) {\n\tcase NFT_TRANS_PREPARE:\n\t\tset->use--;\n\t\treturn;\n\tcase NFT_TRANS_ABORT:\n\tcase NFT_TRANS_RELEASE:\n\t\tset->use--;\n\t\tfallthrough;\n\tdefault:\n\t\tnf_tables_unbind_set(ctx, set, binding,\n\t\t\t\t     phase == NFT_TRANS_COMMIT);\n\t}\n}",
        "code_after_change": "void nf_tables_deactivate_set(const struct nft_ctx *ctx, struct nft_set *set,\n\t\t\t      struct nft_set_binding *binding,\n\t\t\t      enum nft_trans_phase phase)\n{\n\tswitch (phase) {\n\tcase NFT_TRANS_PREPARE:\n\t\tif (nft_set_is_anonymous(set))\n\t\t\tnft_deactivate_next(ctx->net, set);\n\n\t\tset->use--;\n\t\treturn;\n\tcase NFT_TRANS_ABORT:\n\tcase NFT_TRANS_RELEASE:\n\t\tset->use--;\n\t\tfallthrough;\n\tdefault:\n\t\tnf_tables_unbind_set(ctx, set, binding,\n\t\t\t\t     phase == NFT_TRANS_COMMIT);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n {\n \tswitch (phase) {\n \tcase NFT_TRANS_PREPARE:\n+\t\tif (nft_set_is_anonymous(set))\n+\t\t\tnft_deactivate_next(ctx->net, set);\n+\n \t\tset->use--;\n \t\treturn;\n \tcase NFT_TRANS_ABORT:",
        "function_modified_lines": {
            "added": [
                "\t\tif (nft_set_is_anonymous(set))",
                "\t\t\tnft_deactivate_next(ctx->net, set);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 6.3.1, a use-after-free in Netfilter nf_tables when processing batch requests can be abused to perform arbitrary read and write operations on kernel memory. Unprivileged local users can obtain root privileges. This occurs because anonymous sets are mishandled.",
        "id": 4006
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ncopy_entries_to_user(unsigned int total_size,\n\t\t     const struct xt_table *table,\n\t\t     void __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct ipt_entry *e;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tint ret = 0;\n\tconst void *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tunsigned int i;\n\t\tconst struct xt_entry_match *m;\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct ipt_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tfor (i = sizeof(struct ipt_entry);\n\t\t     i < e->target_offset;\n\t\t     i += m->u.match_size) {\n\t\t\tm = (void *)e + i;\n\n\t\t\tif (xt_match_to_user(m, userptr + off + i)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_counters;\n\t\t\t}\n\t\t}\n\n\t\tt = ipt_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int\ncopy_entries_to_user(unsigned int total_size,\n\t\t     const struct xt_table *table,\n\t\t     void __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct ipt_entry *e;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tint ret = 0;\n\tconst void *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tunsigned int i;\n\t\tconst struct xt_entry_match *m;\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct ipt_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tfor (i = sizeof(struct ipt_entry);\n\t\t     i < e->target_offset;\n\t\t     i += m->u.match_size) {\n\t\t\tm = (void *)e + i;\n\n\t\t\tif (xt_match_to_user(m, userptr + off + i)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_counters;\n\t\t\t}\n\t\t}\n\n\t\tt = ipt_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tunsigned int off, num;\n \tconst struct ipt_entry *e;\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tint ret = 0;\n \tconst void *loc_cpu_entry;\n ",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2783
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ncopy_entries_to_user(unsigned int total_size,\n\t\t     const struct xt_table *table,\n\t\t     void __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct ip6t_entry *e;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tint ret = 0;\n\tconst void *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tunsigned int i;\n\t\tconst struct xt_entry_match *m;\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct ip6t_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tfor (i = sizeof(struct ip6t_entry);\n\t\t     i < e->target_offset;\n\t\t     i += m->u.match_size) {\n\t\t\tm = (void *)e + i;\n\n\t\t\tif (xt_match_to_user(m, userptr + off + i)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_counters;\n\t\t\t}\n\t\t}\n\n\t\tt = ip6t_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int\ncopy_entries_to_user(unsigned int total_size,\n\t\t     const struct xt_table *table,\n\t\t     void __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct ip6t_entry *e;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tint ret = 0;\n\tconst void *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tunsigned int i;\n\t\tconst struct xt_entry_match *m;\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct ip6t_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tfor (i = sizeof(struct ip6t_entry);\n\t\t     i < e->target_offset;\n\t\t     i += m->u.match_size) {\n\t\t\tm = (void *)e + i;\n\n\t\t\tif (xt_match_to_user(m, userptr + off + i)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_counters;\n\t\t\t}\n\t\t}\n\n\t\tt = ip6t_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tunsigned int off, num;\n \tconst struct ip6t_entry *e;\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tint ret = 0;\n \tconst void *loc_cpu_entry;\n ",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2789
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ncompat_copy_entries_to_user(unsigned int total_size, struct xt_table *table,\n\t\t\t    void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct ipt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int\ncompat_copy_entries_to_user(unsigned int total_size, struct xt_table *table,\n\t\t\t    void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct ipt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t    void __user *userptr)\n {\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tvoid __user *pos;\n \tunsigned int size;\n \tint ret = 0;",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2781
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int compat_copy_entries_to_user(unsigned int total_size,\n\t\t\t\t       struct xt_table *table,\n\t\t\t\t       void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct arpt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int compat_copy_entries_to_user(unsigned int total_size,\n\t\t\t\t       struct xt_table *table,\n\t\t\t\t       void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct arpt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t\t       void __user *userptr)\n {\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tvoid __user *pos;\n \tunsigned int size;\n \tint ret = 0;",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2773
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int do_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct arpt_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\n\tt = xt_find_table_lock(net, NFPROTO_ARP, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = t->private;\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter,  private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
        "code_after_change": "static int do_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct arpt_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\n\tt = xt_find_table_lock(net, NFPROTO_ARP, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = xt_table_get_private_protected(t);\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter,  private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,7 +20,7 @@\n \t}\n \n \tlocal_bh_disable();\n-\tprivate = t->private;\n+\tprivate = xt_table_get_private_protected(t);\n \tif (private->number != tmp.num_counters) {\n \t\tret = -EINVAL;\n \t\tgoto unlock_up_free;",
        "function_modified_lines": {
            "added": [
                "\tprivate = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\tprivate = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2774
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "static void unpin_sdma_pages(struct sdma_mmu_node *node)\n{\n\tif (node->npages) {\n\t\tunpin_vector_pages(node->pq->mm, node->pages, 0, node->npages);\n\t\tatomic_sub(node->npages, &node->pq->n_locked);\n\t}\n}",
        "code_after_change": "static void unpin_sdma_pages(struct sdma_mmu_node *node)\n{\n\tif (node->npages) {\n\t\tunpin_vector_pages(mm_from_sdma_node(node), node->pages, 0,\n\t\t\t\t   node->npages);\n\t\tatomic_sub(node->npages, &node->pq->n_locked);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,8 @@\n static void unpin_sdma_pages(struct sdma_mmu_node *node)\n {\n \tif (node->npages) {\n-\t\tunpin_vector_pages(node->pq->mm, node->pages, 0, node->npages);\n+\t\tunpin_vector_pages(mm_from_sdma_node(node), node->pages, 0,\n+\t\t\t\t   node->npages);\n \t\tatomic_sub(node->npages, &node->pq->n_locked);\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tunpin_vector_pages(mm_from_sdma_node(node), node->pages, 0,",
                "\t\t\t\t   node->npages);"
            ],
            "deleted": [
                "\t\tunpin_vector_pages(node->pq->mm, node->pages, 0, node->npages);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2652
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "void hfi1_mmu_rb_evict(struct mmu_rb_handler *handler, void *evict_arg)\n{\n\tstruct mmu_rb_node *rbnode, *ptr;\n\tstruct list_head del_list;\n\tunsigned long flags;\n\tbool stop = false;\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tlist_for_each_entry_safe_reverse(rbnode, ptr, &handler->lru_list,\n\t\t\t\t\t list) {\n\t\tif (handler->ops->evict(handler->ops_arg, rbnode, evict_arg,\n\t\t\t\t\t&stop)) {\n\t\t\t__mmu_int_rb_remove(rbnode, &handler->root);\n\t\t\t/* move from LRU list to delete list */\n\t\t\tlist_move(&rbnode->list, &del_list);\n\t\t}\n\t\tif (stop)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\twhile (!list_empty(&del_list)) {\n\t\trbnode = list_first_entry(&del_list, struct mmu_rb_node, list);\n\t\tlist_del(&rbnode->list);\n\t\thandler->ops->remove(handler->ops_arg, rbnode);\n\t}\n}",
        "code_after_change": "void hfi1_mmu_rb_evict(struct mmu_rb_handler *handler, void *evict_arg)\n{\n\tstruct mmu_rb_node *rbnode, *ptr;\n\tstruct list_head del_list;\n\tunsigned long flags;\n\tbool stop = false;\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn;\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tlist_for_each_entry_safe_reverse(rbnode, ptr, &handler->lru_list,\n\t\t\t\t\t list) {\n\t\tif (handler->ops->evict(handler->ops_arg, rbnode, evict_arg,\n\t\t\t\t\t&stop)) {\n\t\t\t__mmu_int_rb_remove(rbnode, &handler->root);\n\t\t\t/* move from LRU list to delete list */\n\t\t\tlist_move(&rbnode->list, &del_list);\n\t\t}\n\t\tif (stop)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\twhile (!list_empty(&del_list)) {\n\t\trbnode = list_first_entry(&del_list, struct mmu_rb_node, list);\n\t\tlist_del(&rbnode->list);\n\t\thandler->ops->remove(handler->ops_arg, rbnode);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \tstruct list_head del_list;\n \tunsigned long flags;\n \tbool stop = false;\n+\n+\tif (current->mm != handler->mn.mm)\n+\t\treturn;\n \n \tINIT_LIST_HEAD(&del_list);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (current->mm != handler->mn.mm)",
                "\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2644
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "void hfi1_mmu_rb_remove(struct mmu_rb_handler *handler,\n\t\t\tstruct mmu_rb_node *node)\n{\n\tunsigned long flags;\n\n\t/* Validity of handler and node pointers has been checked by caller. */\n\ttrace_hfi1_mmu_rb_remove(node->addr, node->len);\n\tspin_lock_irqsave(&handler->lock, flags);\n\t__mmu_int_rb_remove(node, &handler->root);\n\tlist_del(&node->list); /* remove from LRU list */\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\thandler->ops->remove(handler->ops_arg, node);\n}",
        "code_after_change": "void hfi1_mmu_rb_remove(struct mmu_rb_handler *handler,\n\t\t\tstruct mmu_rb_node *node)\n{\n\tunsigned long flags;\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn;\n\n\t/* Validity of handler and node pointers has been checked by caller. */\n\ttrace_hfi1_mmu_rb_remove(node->addr, node->len);\n\tspin_lock_irqsave(&handler->lock, flags);\n\t__mmu_int_rb_remove(node, &handler->root);\n\tlist_del(&node->list); /* remove from LRU list */\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\thandler->ops->remove(handler->ops_arg, node);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,9 @@\n \t\t\tstruct mmu_rb_node *node)\n {\n \tunsigned long flags;\n+\n+\tif (current->mm != handler->mn.mm)\n+\t\treturn;\n \n \t/* Validity of handler and node pointers has been checked by caller. */\n \ttrace_hfi1_mmu_rb_remove(node->addr, node->len);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (current->mm != handler->mn.mm)",
                "\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2646
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "void hfi1_mmu_rb_unregister(struct mmu_rb_handler *handler)\n{\n\tstruct mmu_rb_node *rbnode;\n\tstruct rb_node *node;\n\tunsigned long flags;\n\tstruct list_head del_list;\n\n\t/* Unregister first so we don't get any more notifications. */\n\tmmu_notifier_unregister(&handler->mn, handler->mm);\n\n\t/*\n\t * Make sure the wq delete handler is finished running.  It will not\n\t * be triggered once the mmu notifiers are unregistered above.\n\t */\n\tflush_work(&handler->del_work);\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\twhile ((node = rb_first_cached(&handler->root))) {\n\t\trbnode = rb_entry(node, struct mmu_rb_node, node);\n\t\trb_erase_cached(node, &handler->root);\n\t\t/* move from LRU list to delete list */\n\t\tlist_move(&rbnode->list, &del_list);\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\tdo_remove(handler, &del_list);\n\n\tkfree(handler);\n}",
        "code_after_change": "void hfi1_mmu_rb_unregister(struct mmu_rb_handler *handler)\n{\n\tstruct mmu_rb_node *rbnode;\n\tstruct rb_node *node;\n\tunsigned long flags;\n\tstruct list_head del_list;\n\n\t/* Unregister first so we don't get any more notifications. */\n\tmmu_notifier_unregister(&handler->mn, handler->mn.mm);\n\n\t/*\n\t * Make sure the wq delete handler is finished running.  It will not\n\t * be triggered once the mmu notifiers are unregistered above.\n\t */\n\tflush_work(&handler->del_work);\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\twhile ((node = rb_first_cached(&handler->root))) {\n\t\trbnode = rb_entry(node, struct mmu_rb_node, node);\n\t\trb_erase_cached(node, &handler->root);\n\t\t/* move from LRU list to delete list */\n\t\tlist_move(&rbnode->list, &del_list);\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\tdo_remove(handler, &del_list);\n\n\tkfree(handler);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tstruct list_head del_list;\n \n \t/* Unregister first so we don't get any more notifications. */\n-\tmmu_notifier_unregister(&handler->mn, handler->mm);\n+\tmmu_notifier_unregister(&handler->mn, handler->mn.mm);\n \n \t/*\n \t * Make sure the wq delete handler is finished running.  It will not",
        "function_modified_lines": {
            "added": [
                "\tmmu_notifier_unregister(&handler->mn, handler->mn.mm);"
            ],
            "deleted": [
                "\tmmu_notifier_unregister(&handler->mn, handler->mm);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2647
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "static void unpin_rcv_pages(struct hfi1_filedata *fd,\n\t\t\t    struct tid_user_buf *tidbuf,\n\t\t\t    struct tid_rb_node *node,\n\t\t\t    unsigned int idx,\n\t\t\t    unsigned int npages,\n\t\t\t    bool mapped)\n{\n\tstruct page **pages;\n\tstruct hfi1_devdata *dd = fd->uctxt->dd;\n\n\tif (mapped) {\n\t\tpci_unmap_single(dd->pcidev, node->dma_addr,\n\t\t\t\t node->npages * PAGE_SIZE, PCI_DMA_FROMDEVICE);\n\t\tpages = &node->pages[idx];\n\t} else {\n\t\tpages = &tidbuf->pages[idx];\n\t}\n\thfi1_release_user_pages(fd->mm, pages, npages, mapped);\n\tfd->tid_n_pinned -= npages;\n}",
        "code_after_change": "static void unpin_rcv_pages(struct hfi1_filedata *fd,\n\t\t\t    struct tid_user_buf *tidbuf,\n\t\t\t    struct tid_rb_node *node,\n\t\t\t    unsigned int idx,\n\t\t\t    unsigned int npages,\n\t\t\t    bool mapped)\n{\n\tstruct page **pages;\n\tstruct hfi1_devdata *dd = fd->uctxt->dd;\n\tstruct mm_struct *mm;\n\n\tif (mapped) {\n\t\tpci_unmap_single(dd->pcidev, node->dma_addr,\n\t\t\t\t node->npages * PAGE_SIZE, PCI_DMA_FROMDEVICE);\n\t\tpages = &node->pages[idx];\n\t\tmm = mm_from_tid_node(node);\n\t} else {\n\t\tpages = &tidbuf->pages[idx];\n\t\tmm = current->mm;\n\t}\n\thfi1_release_user_pages(mm, pages, npages, mapped);\n\tfd->tid_n_pinned -= npages;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,14 +7,17 @@\n {\n \tstruct page **pages;\n \tstruct hfi1_devdata *dd = fd->uctxt->dd;\n+\tstruct mm_struct *mm;\n \n \tif (mapped) {\n \t\tpci_unmap_single(dd->pcidev, node->dma_addr,\n \t\t\t\t node->npages * PAGE_SIZE, PCI_DMA_FROMDEVICE);\n \t\tpages = &node->pages[idx];\n+\t\tmm = mm_from_tid_node(node);\n \t} else {\n \t\tpages = &tidbuf->pages[idx];\n+\t\tmm = current->mm;\n \t}\n-\thfi1_release_user_pages(fd->mm, pages, npages, mapped);\n+\thfi1_release_user_pages(mm, pages, npages, mapped);\n \tfd->tid_n_pinned -= npages;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct mm_struct *mm;",
                "\t\tmm = mm_from_tid_node(node);",
                "\t\tmm = current->mm;",
                "\thfi1_release_user_pages(mm, pages, npages, mapped);"
            ],
            "deleted": [
                "\thfi1_release_user_pages(fd->mm, pages, npages, mapped);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2649
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "static int pin_rcv_pages(struct hfi1_filedata *fd, struct tid_user_buf *tidbuf)\n{\n\tint pinned;\n\tunsigned int npages;\n\tunsigned long vaddr = tidbuf->vaddr;\n\tstruct page **pages = NULL;\n\tstruct hfi1_devdata *dd = fd->uctxt->dd;\n\n\t/* Get the number of pages the user buffer spans */\n\tnpages = num_user_pages(vaddr, tidbuf->length);\n\tif (!npages)\n\t\treturn -EINVAL;\n\n\tif (npages > fd->uctxt->expected_count) {\n\t\tdd_dev_err(dd, \"Expected buffer too big\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Allocate the array of struct page pointers needed for pinning */\n\tpages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Pin all the pages of the user buffer. If we can't pin all the\n\t * pages, accept the amount pinned so far and program only that.\n\t * User space knows how to deal with partially programmed buffers.\n\t */\n\tif (!hfi1_can_pin_pages(dd, fd->mm, fd->tid_n_pinned, npages)) {\n\t\tkfree(pages);\n\t\treturn -ENOMEM;\n\t}\n\n\tpinned = hfi1_acquire_user_pages(fd->mm, vaddr, npages, true, pages);\n\tif (pinned <= 0) {\n\t\tkfree(pages);\n\t\treturn pinned;\n\t}\n\ttidbuf->pages = pages;\n\ttidbuf->npages = npages;\n\tfd->tid_n_pinned += pinned;\n\treturn pinned;\n}",
        "code_after_change": "static int pin_rcv_pages(struct hfi1_filedata *fd, struct tid_user_buf *tidbuf)\n{\n\tint pinned;\n\tunsigned int npages;\n\tunsigned long vaddr = tidbuf->vaddr;\n\tstruct page **pages = NULL;\n\tstruct hfi1_devdata *dd = fd->uctxt->dd;\n\n\t/* Get the number of pages the user buffer spans */\n\tnpages = num_user_pages(vaddr, tidbuf->length);\n\tif (!npages)\n\t\treturn -EINVAL;\n\n\tif (npages > fd->uctxt->expected_count) {\n\t\tdd_dev_err(dd, \"Expected buffer too big\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Allocate the array of struct page pointers needed for pinning */\n\tpages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Pin all the pages of the user buffer. If we can't pin all the\n\t * pages, accept the amount pinned so far and program only that.\n\t * User space knows how to deal with partially programmed buffers.\n\t */\n\tif (!hfi1_can_pin_pages(dd, current->mm, fd->tid_n_pinned, npages)) {\n\t\tkfree(pages);\n\t\treturn -ENOMEM;\n\t}\n\n\tpinned = hfi1_acquire_user_pages(current->mm, vaddr, npages, true, pages);\n\tif (pinned <= 0) {\n\t\tkfree(pages);\n\t\treturn pinned;\n\t}\n\ttidbuf->pages = pages;\n\ttidbuf->npages = npages;\n\tfd->tid_n_pinned += pinned;\n\treturn pinned;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,12 +26,12 @@\n \t * pages, accept the amount pinned so far and program only that.\n \t * User space knows how to deal with partially programmed buffers.\n \t */\n-\tif (!hfi1_can_pin_pages(dd, fd->mm, fd->tid_n_pinned, npages)) {\n+\tif (!hfi1_can_pin_pages(dd, current->mm, fd->tid_n_pinned, npages)) {\n \t\tkfree(pages);\n \t\treturn -ENOMEM;\n \t}\n \n-\tpinned = hfi1_acquire_user_pages(fd->mm, vaddr, npages, true, pages);\n+\tpinned = hfi1_acquire_user_pages(current->mm, vaddr, npages, true, pages);\n \tif (pinned <= 0) {\n \t\tkfree(pages);\n \t\treturn pinned;",
        "function_modified_lines": {
            "added": [
                "\tif (!hfi1_can_pin_pages(dd, current->mm, fd->tid_n_pinned, npages)) {",
                "\tpinned = hfi1_acquire_user_pages(current->mm, vaddr, npages, true, pages);"
            ],
            "deleted": [
                "\tif (!hfi1_can_pin_pages(dd, fd->mm, fd->tid_n_pinned, npages)) {",
                "\tpinned = hfi1_acquire_user_pages(fd->mm, vaddr, npages, true, pages);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2650
    },
    {
        "cve_id": "CVE-2023-25012",
        "code_before_change": "static void bigben_set_led(struct led_classdev *led,\n\tenum led_brightness value)\n{\n\tstruct device *dev = led->dev->parent;\n\tstruct hid_device *hid = to_hid_device(dev);\n\tstruct bigben_device *bigben = hid_get_drvdata(hid);\n\tint n;\n\tbool work;\n\tunsigned long flags;\n\n\tif (!bigben) {\n\t\thid_err(hid, \"no device data\\n\");\n\t\treturn;\n\t}\n\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tif (led == bigben->leds[n]) {\n\t\t\tspin_lock_irqsave(&bigben->lock, flags);\n\t\t\tif (value == LED_OFF) {\n\t\t\t\twork = (bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state &= ~BIT(n);\n\t\t\t} else {\n\t\t\t\twork = !(bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state |= BIT(n);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\t\t\tif (work) {\n\t\t\t\tbigben->work_led = true;\n\t\t\t\tschedule_work(&bigben->worker);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n}",
        "code_after_change": "static void bigben_set_led(struct led_classdev *led,\n\tenum led_brightness value)\n{\n\tstruct device *dev = led->dev->parent;\n\tstruct hid_device *hid = to_hid_device(dev);\n\tstruct bigben_device *bigben = hid_get_drvdata(hid);\n\tint n;\n\tbool work;\n\tunsigned long flags;\n\n\tif (!bigben) {\n\t\thid_err(hid, \"no device data\\n\");\n\t\treturn;\n\t}\n\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tif (led == bigben->leds[n]) {\n\t\t\tspin_lock_irqsave(&bigben->lock, flags);\n\t\t\tif (value == LED_OFF) {\n\t\t\t\twork = (bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state &= ~BIT(n);\n\t\t\t} else {\n\t\t\t\twork = !(bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state |= BIT(n);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\t\t\tif (work) {\n\t\t\t\tbigben->work_led = true;\n\t\t\t\tbigben_schedule_work(bigben);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,7 +27,7 @@\n \n \t\t\tif (work) {\n \t\t\t\tbigben->work_led = true;\n-\t\t\t\tschedule_work(&bigben->worker);\n+\t\t\t\tbigben_schedule_work(bigben);\n \t\t\t}\n \t\t\treturn;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tbigben_schedule_work(bigben);"
            ],
            "deleted": [
                "\t\t\t\tschedule_work(&bigben->worker);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel through 6.1.9 has a Use-After-Free in bigben_remove in drivers/hid/hid-bigbenff.c via a crafted USB device because the LED controllers remain registered for too long.",
        "id": 3957
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_resource_context_res_add(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tstruct vmw_resource *ctx)\n{\n\tstruct list_head *binding_list;\n\tstruct vmw_ctx_bindinfo *entry;\n\tint ret = 0;\n\tstruct vmw_resource *res;\n\tu32 i;\n\tu32 cotable_max = has_sm5_context(ctx->dev_priv) ?\n\t\tSVGA_COTABLE_MAX : SVGA_COTABLE_DX10_MAX;\n\n\t/* Add all cotables to the validation list. */\n\tif (has_sm4_context(dev_priv) &&\n\t    vmw_res_type(ctx) == vmw_res_dx_context) {\n\t\tfor (i = 0; i < cotable_max; ++i) {\n\t\t\tres = vmw_context_cotable(ctx, i);\n\t\t\tif (IS_ERR(res))\n\t\t\t\tcontinue;\n\n\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t\t\t    VMW_RES_DIRTY_SET);\n\t\t\tif (unlikely(ret != 0))\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* Add all resources bound to the context to the validation list */\n\tmutex_lock(&dev_priv->binding_mutex);\n\tbinding_list = vmw_context_binding_list(ctx);\n\n\tlist_for_each_entry(entry, binding_list, ctx_list) {\n\t\tif (vmw_res_type(entry->res) == vmw_res_view)\n\t\t\tret = vmw_view_res_val_add(sw_context, entry->res);\n\t\telse\n\t\t\tret = vmw_execbuf_res_noctx_val_add\n\t\t\t\t(sw_context, entry->res,\n\t\t\t\t vmw_binding_dirtying(entry->bt));\n\t\tif (unlikely(ret != 0))\n\t\t\tbreak;\n\t}\n\n\tif (has_sm4_context(dev_priv) &&\n\t    vmw_res_type(ctx) == vmw_res_dx_context) {\n\t\tstruct vmw_buffer_object *dx_query_mob;\n\n\t\tdx_query_mob = vmw_context_get_dx_query_mob(ctx);\n\t\tif (dx_query_mob)\n\t\t\tret = vmw_validation_add_bo(sw_context->ctx,\n\t\t\t\t\t\t    dx_query_mob, true, false);\n\t}\n\n\tmutex_unlock(&dev_priv->binding_mutex);\n\treturn ret;\n}",
        "code_after_change": "static int vmw_resource_context_res_add(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tstruct vmw_resource *ctx)\n{\n\tstruct list_head *binding_list;\n\tstruct vmw_ctx_bindinfo *entry;\n\tint ret = 0;\n\tstruct vmw_resource *res;\n\tu32 i;\n\tu32 cotable_max = has_sm5_context(ctx->dev_priv) ?\n\t\tSVGA_COTABLE_MAX : SVGA_COTABLE_DX10_MAX;\n\n\t/* Add all cotables to the validation list. */\n\tif (has_sm4_context(dev_priv) &&\n\t    vmw_res_type(ctx) == vmw_res_dx_context) {\n\t\tfor (i = 0; i < cotable_max; ++i) {\n\t\t\tres = vmw_context_cotable(ctx, i);\n\t\t\tif (IS_ERR(res))\n\t\t\t\tcontinue;\n\n\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n\t\t\t\t\t\t      VMW_RES_DIRTY_SET,\n\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\t\tif (unlikely(ret != 0))\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* Add all resources bound to the context to the validation list */\n\tmutex_lock(&dev_priv->binding_mutex);\n\tbinding_list = vmw_context_binding_list(ctx);\n\n\tlist_for_each_entry(entry, binding_list, ctx_list) {\n\t\tif (vmw_res_type(entry->res) == vmw_res_view)\n\t\t\tret = vmw_view_res_val_add(sw_context, entry->res);\n\t\telse\n\t\t\tret = vmw_execbuf_res_val_add(sw_context, entry->res,\n\t\t\t\t\t\t      vmw_binding_dirtying(entry->bt),\n\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\tif (unlikely(ret != 0))\n\t\t\tbreak;\n\t}\n\n\tif (has_sm4_context(dev_priv) &&\n\t    vmw_res_type(ctx) == vmw_res_dx_context) {\n\t\tstruct vmw_buffer_object *dx_query_mob;\n\n\t\tdx_query_mob = vmw_context_get_dx_query_mob(ctx);\n\t\tif (dx_query_mob)\n\t\t\tret = vmw_validation_add_bo(sw_context->ctx,\n\t\t\t\t\t\t    dx_query_mob, true, false);\n\t}\n\n\tmutex_unlock(&dev_priv->binding_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,8 +18,9 @@\n \t\t\tif (IS_ERR(res))\n \t\t\t\tcontinue;\n \n-\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n-\t\t\t\t\t\t\t    VMW_RES_DIRTY_SET);\n+\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n+\t\t\t\t\t\t      VMW_RES_DIRTY_SET,\n+\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n \t\t\tif (unlikely(ret != 0))\n \t\t\t\treturn ret;\n \t\t}\n@@ -33,9 +34,9 @@\n \t\tif (vmw_res_type(entry->res) == vmw_res_view)\n \t\t\tret = vmw_view_res_val_add(sw_context, entry->res);\n \t\telse\n-\t\t\tret = vmw_execbuf_res_noctx_val_add\n-\t\t\t\t(sw_context, entry->res,\n-\t\t\t\t vmw_binding_dirtying(entry->bt));\n+\t\t\tret = vmw_execbuf_res_val_add(sw_context, entry->res,\n+\t\t\t\t\t\t      vmw_binding_dirtying(entry->bt),\n+\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n \t\tif (unlikely(ret != 0))\n \t\t\tbreak;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,",
                "\t\t\t\t\t\t      VMW_RES_DIRTY_SET,",
                "\t\t\t\t\t\t      vmw_val_add_flag_noctx);",
                "\t\t\tret = vmw_execbuf_res_val_add(sw_context, entry->res,",
                "\t\t\t\t\t\t      vmw_binding_dirtying(entry->bt),",
                "\t\t\t\t\t\t      vmw_val_add_flag_noctx);"
            ],
            "deleted": [
                "\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,",
                "\t\t\t\t\t\t\t    VMW_RES_DIRTY_SET);",
                "\t\t\tret = vmw_execbuf_res_noctx_val_add",
                "\t\t\t\t(sw_context, entry->res,",
                "\t\t\t\t vmw_binding_dirtying(entry->bt));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3687
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_cmd_dx_set_shader(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXSetShader);\n\tSVGA3dShaderType max_allowed = has_sm5_context(dev_priv) ?\n\t\tSVGA3D_SHADERTYPE_MAX : SVGA3D_SHADERTYPE_DX10_MAX;\n\tstruct vmw_resource *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_node = VMW_GET_CTX_NODE(sw_context);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tint ret = 0;\n\n\tif (!ctx_node)\n\t\treturn -EINVAL;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= max_allowed ||\n\t    cmd->body.type < SVGA3D_SHADERTYPE_MIN) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->body.shaderId != SVGA3D_INVALID_ID) {\n\t\tres = vmw_shader_lookup(sw_context->man, cmd->body.shaderId, 0);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find shader for binding.\\n\");\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_dx_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\n\tvmw_binding_add(ctx_node->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_cmd_dx_set_shader(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXSetShader);\n\tSVGA3dShaderType max_allowed = has_sm5_context(dev_priv) ?\n\t\tSVGA3D_SHADERTYPE_MAX : SVGA3D_SHADERTYPE_DX10_MAX;\n\tstruct vmw_resource *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_node = VMW_GET_CTX_NODE(sw_context);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tint ret = 0;\n\n\tif (!ctx_node)\n\t\treturn -EINVAL;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= max_allowed ||\n\t    cmd->body.type < SVGA3D_SHADERTYPE_MIN) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->body.shaderId != SVGA3D_INVALID_ID) {\n\t\tres = vmw_shader_lookup(sw_context->man, cmd->body.shaderId, 0);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find shader for binding.\\n\");\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n\t\t\t\t\t      VMW_RES_DIRTY_NONE,\n\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_dx_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\n\tvmw_binding_add(ctx_node->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,8 +29,9 @@\n \t\t\treturn PTR_ERR(res);\n \t\t}\n \n-\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n-\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n+\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n+\t\t\t\t\t      VMW_RES_DIRTY_NONE,\n+\t\t\t\t\t      vmw_val_add_flag_noctx);\n \t\tif (ret)\n \t\t\treturn ret;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tret = vmw_execbuf_res_val_add(sw_context, res,",
                "\t\t\t\t\t      VMW_RES_DIRTY_NONE,",
                "\t\t\t\t\t      vmw_val_add_flag_noctx);"
            ],
            "deleted": [
                "\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,",
                "\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3683
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int\nvmw_cmd_res_check(struct vmw_private *dev_priv,\n\t\t  struct vmw_sw_context *sw_context,\n\t\t  enum vmw_res_type res_type,\n\t\t  u32 dirty,\n\t\t  const struct vmw_user_resource_conv *converter,\n\t\t  uint32_t *id_loc,\n\t\t  struct vmw_resource **p_res)\n{\n\tstruct vmw_res_cache_entry *rcache = &sw_context->res_cache[res_type];\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tif (p_res)\n\t\t*p_res = NULL;\n\n\tif (*id_loc == SVGA3D_INVALID_ID) {\n\t\tif (res_type == vmw_res_context) {\n\t\t\tVMW_DEBUG_USER(\"Illegal context invalid id.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (likely(rcache->valid_handle && *id_loc == rcache->handle)) {\n\t\tres = rcache->res;\n\t\tif (dirty)\n\t\t\tvmw_validation_res_set_dirty(sw_context->ctx,\n\t\t\t\t\t\t     rcache->private, dirty);\n\t} else {\n\t\tunsigned int size = vmw_execbuf_res_size(dev_priv, res_type);\n\n\t\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tres = vmw_user_resource_noref_lookup_handle\n\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find/use resource 0x%08x.\\n\",\n\t\t\t\t       (unsigned int) *id_loc);\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_noref_val_add(sw_context, res, dirty);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tif (rcache->valid && rcache->res == res) {\n\t\t\trcache->valid_handle = true;\n\t\t\trcache->handle = *id_loc;\n\t\t}\n\t}\n\n\tret = vmw_resource_relocation_add(sw_context, res,\n\t\t\t\t\t  vmw_ptr_diff(sw_context->buf_start,\n\t\t\t\t\t\t       id_loc),\n\t\t\t\t\t  vmw_res_rel_normal);\n\tif (p_res)\n\t\t*p_res = res;\n\n\treturn 0;\n}",
        "code_after_change": "static int\nvmw_cmd_res_check(struct vmw_private *dev_priv,\n\t\t  struct vmw_sw_context *sw_context,\n\t\t  enum vmw_res_type res_type,\n\t\t  u32 dirty,\n\t\t  const struct vmw_user_resource_conv *converter,\n\t\t  uint32_t *id_loc,\n\t\t  struct vmw_resource **p_res)\n{\n\tstruct vmw_res_cache_entry *rcache = &sw_context->res_cache[res_type];\n\tstruct vmw_resource *res;\n\tint ret = 0;\n\tbool needs_unref = false;\n\n\tif (p_res)\n\t\t*p_res = NULL;\n\n\tif (*id_loc == SVGA3D_INVALID_ID) {\n\t\tif (res_type == vmw_res_context) {\n\t\t\tVMW_DEBUG_USER(\"Illegal context invalid id.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (likely(rcache->valid_handle && *id_loc == rcache->handle)) {\n\t\tres = rcache->res;\n\t\tif (dirty)\n\t\t\tvmw_validation_res_set_dirty(sw_context->ctx,\n\t\t\t\t\t\t     rcache->private, dirty);\n\t} else {\n\t\tunsigned int size = vmw_execbuf_res_size(dev_priv, res_type);\n\n\t\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = vmw_user_resource_lookup_handle\n\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter, &res);\n\t\tif (ret != 0) {\n\t\t\tVMW_DEBUG_USER(\"Could not find/use resource 0x%08x.\\n\",\n\t\t\t\t       (unsigned int) *id_loc);\n\t\t\treturn ret;\n\t\t}\n\t\tneeds_unref = true;\n\n\t\tret = vmw_execbuf_res_val_add(sw_context, res, dirty, vmw_val_add_flag_none);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto res_check_done;\n\n\t\tif (rcache->valid && rcache->res == res) {\n\t\t\trcache->valid_handle = true;\n\t\t\trcache->handle = *id_loc;\n\t\t}\n\t}\n\n\tret = vmw_resource_relocation_add(sw_context, res,\n\t\t\t\t\t  vmw_ptr_diff(sw_context->buf_start,\n\t\t\t\t\t\t       id_loc),\n\t\t\t\t\t  vmw_res_rel_normal);\n\tif (p_res)\n\t\t*p_res = res;\n\nres_check_done:\n\tif (needs_unref)\n\t\tvmw_resource_unreference(&res);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,8 @@\n {\n \tstruct vmw_res_cache_entry *rcache = &sw_context->res_cache[res_type];\n \tstruct vmw_resource *res;\n-\tint ret;\n+\tint ret = 0;\n+\tbool needs_unref = false;\n \n \tif (p_res)\n \t\t*p_res = NULL;\n@@ -34,17 +35,18 @@\n \t\tif (ret)\n \t\t\treturn ret;\n \n-\t\tres = vmw_user_resource_noref_lookup_handle\n-\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter);\n-\t\tif (IS_ERR(res)) {\n+\t\tret = vmw_user_resource_lookup_handle\n+\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter, &res);\n+\t\tif (ret != 0) {\n \t\t\tVMW_DEBUG_USER(\"Could not find/use resource 0x%08x.\\n\",\n \t\t\t\t       (unsigned int) *id_loc);\n-\t\t\treturn PTR_ERR(res);\n+\t\t\treturn ret;\n \t\t}\n+\t\tneeds_unref = true;\n \n-\t\tret = vmw_execbuf_res_noref_val_add(sw_context, res, dirty);\n+\t\tret = vmw_execbuf_res_val_add(sw_context, res, dirty, vmw_val_add_flag_none);\n \t\tif (unlikely(ret != 0))\n-\t\t\treturn ret;\n+\t\t\tgoto res_check_done;\n \n \t\tif (rcache->valid && rcache->res == res) {\n \t\t\trcache->valid_handle = true;\n@@ -59,5 +61,9 @@\n \tif (p_res)\n \t\t*p_res = res;\n \n-\treturn 0;\n+res_check_done:\n+\tif (needs_unref)\n+\t\tvmw_resource_unreference(&res);\n+\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint ret = 0;",
                "\tbool needs_unref = false;",
                "\t\tret = vmw_user_resource_lookup_handle",
                "\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter, &res);",
                "\t\tif (ret != 0) {",
                "\t\t\treturn ret;",
                "\t\tneeds_unref = true;",
                "\t\tret = vmw_execbuf_res_val_add(sw_context, res, dirty, vmw_val_add_flag_none);",
                "\t\t\tgoto res_check_done;",
                "res_check_done:",
                "\tif (needs_unref)",
                "\t\tvmw_resource_unreference(&res);",
                "",
                "\treturn ret;"
            ],
            "deleted": [
                "\tint ret;",
                "\t\tres = vmw_user_resource_noref_lookup_handle",
                "\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter);",
                "\t\tif (IS_ERR(res)) {",
                "\t\t\treturn PTR_ERR(res);",
                "\t\tret = vmw_execbuf_res_noref_val_add(sw_context, res, dirty);",
                "\t\t\treturn ret;",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3686
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "void rose_stop_idletimer(struct sock *sk)\n{\n\tdel_timer(&rose_sk(sk)->idletimer);\n}",
        "code_after_change": "void rose_stop_idletimer(struct sock *sk)\n{\n\tsk_stop_timer(sk, &rose_sk(sk)->idletimer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n void rose_stop_idletimer(struct sock *sk)\n {\n-\tdel_timer(&rose_sk(sk)->idletimer);\n+\tsk_stop_timer(sk, &rose_sk(sk)->idletimer);\n }",
        "function_modified_lines": {
            "added": [
                "\tsk_stop_timer(sk, &rose_sk(sk)->idletimer);"
            ],
            "deleted": [
                "\tdel_timer(&rose_sk(sk)->idletimer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3435
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "static void rose_heartbeat_expiry(struct timer_list *t)\n{\n\tstruct sock *sk = from_timer(sk, t, sk_timer);\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tbh_lock_sock(sk);\n\tswitch (rose->state) {\n\tcase ROSE_STATE_0:\n\t\t/* Magic here: If we listen() and a new link dies before it\n\t\t   is accepted() it isn't 'dead' so doesn't get removed. */\n\t\tif (sock_flag(sk, SOCK_DESTROY) ||\n\t\t    (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_DEAD))) {\n\t\t\tbh_unlock_sock(sk);\n\t\t\trose_destroy_socket(sk);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase ROSE_STATE_3:\n\t\t/*\n\t\t * Check for the state of the receive buffer.\n\t\t */\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < (sk->sk_rcvbuf / 2) &&\n\t\t    (rose->condition & ROSE_COND_OWN_RX_BUSY)) {\n\t\t\trose->condition &= ~ROSE_COND_OWN_RX_BUSY;\n\t\t\trose->condition &= ~ROSE_COND_ACK_PENDING;\n\t\t\trose->vl         = rose->vr;\n\t\t\trose_write_internal(sk, ROSE_RR);\n\t\t\trose_stop_timer(sk);\t/* HB */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\trose_start_heartbeat(sk);\n\tbh_unlock_sock(sk);\n}",
        "code_after_change": "static void rose_heartbeat_expiry(struct timer_list *t)\n{\n\tstruct sock *sk = from_timer(sk, t, sk_timer);\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tbh_lock_sock(sk);\n\tswitch (rose->state) {\n\tcase ROSE_STATE_0:\n\t\t/* Magic here: If we listen() and a new link dies before it\n\t\t   is accepted() it isn't 'dead' so doesn't get removed. */\n\t\tif (sock_flag(sk, SOCK_DESTROY) ||\n\t\t    (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_DEAD))) {\n\t\t\tbh_unlock_sock(sk);\n\t\t\trose_destroy_socket(sk);\n\t\t\tsock_put(sk);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase ROSE_STATE_3:\n\t\t/*\n\t\t * Check for the state of the receive buffer.\n\t\t */\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < (sk->sk_rcvbuf / 2) &&\n\t\t    (rose->condition & ROSE_COND_OWN_RX_BUSY)) {\n\t\t\trose->condition &= ~ROSE_COND_OWN_RX_BUSY;\n\t\t\trose->condition &= ~ROSE_COND_ACK_PENDING;\n\t\t\trose->vl         = rose->vr;\n\t\t\trose_write_internal(sk, ROSE_RR);\n\t\t\trose_stop_timer(sk);\t/* HB */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\trose_start_heartbeat(sk);\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,7 @@\n \t\t    (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_DEAD))) {\n \t\t\tbh_unlock_sock(sk);\n \t\t\trose_destroy_socket(sk);\n+\t\t\tsock_put(sk);\n \t\t\treturn;\n \t\t}\n \t\tbreak;\n@@ -34,4 +35,5 @@\n \n \trose_start_heartbeat(sk);\n \tbh_unlock_sock(sk);\n+\tsock_put(sk);\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\tsock_put(sk);",
                "\tsock_put(sk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3432
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static int aio_prep_rw(struct kiocb *req, const struct iocb *iocb)\n{\n\tint ret;\n\n\treq->ki_filp = fget(iocb->aio_fildes);\n\tif (unlikely(!req->ki_filp))\n\t\treturn -EBADF;\n\treq->ki_complete = aio_complete_rw;\n\treq->private = NULL;\n\treq->ki_pos = iocb->aio_offset;\n\treq->ki_flags = iocb_flags(req->ki_filp);\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD)\n\t\treq->ki_flags |= IOCB_EVENTFD;\n\treq->ki_hint = ki_hint_validate(file_write_hint(req->ki_filp));\n\tif (iocb->aio_flags & IOCB_FLAG_IOPRIO) {\n\t\t/*\n\t\t * If the IOCB_FLAG_IOPRIO flag of aio_flags is set, then\n\t\t * aio_reqprio is interpreted as an I/O scheduling\n\t\t * class and priority.\n\t\t */\n\t\tret = ioprio_check_cap(iocb->aio_reqprio);\n\t\tif (ret) {\n\t\t\tpr_debug(\"aio ioprio check cap error: %d\\n\", ret);\n\t\t\tgoto out_fput;\n\t\t}\n\n\t\treq->ki_ioprio = iocb->aio_reqprio;\n\t} else\n\t\treq->ki_ioprio = get_current_ioprio();\n\n\tret = kiocb_set_rw_flags(req, iocb->aio_rw_flags);\n\tif (unlikely(ret))\n\t\tgoto out_fput;\n\n\treq->ki_flags &= ~IOCB_HIPRI; /* no one is going to poll for this I/O */\n\treturn 0;\n\nout_fput:\n\tfput(req->ki_filp);\n\treturn ret;\n}",
        "code_after_change": "static int aio_prep_rw(struct kiocb *req, const struct iocb *iocb)\n{\n\tint ret;\n\n\treq->ki_complete = aio_complete_rw;\n\treq->private = NULL;\n\treq->ki_pos = iocb->aio_offset;\n\treq->ki_flags = iocb_flags(req->ki_filp);\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD)\n\t\treq->ki_flags |= IOCB_EVENTFD;\n\treq->ki_hint = ki_hint_validate(file_write_hint(req->ki_filp));\n\tif (iocb->aio_flags & IOCB_FLAG_IOPRIO) {\n\t\t/*\n\t\t * If the IOCB_FLAG_IOPRIO flag of aio_flags is set, then\n\t\t * aio_reqprio is interpreted as an I/O scheduling\n\t\t * class and priority.\n\t\t */\n\t\tret = ioprio_check_cap(iocb->aio_reqprio);\n\t\tif (ret) {\n\t\t\tpr_debug(\"aio ioprio check cap error: %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\n\t\treq->ki_ioprio = iocb->aio_reqprio;\n\t} else\n\t\treq->ki_ioprio = get_current_ioprio();\n\n\tret = kiocb_set_rw_flags(req, iocb->aio_rw_flags);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\treq->ki_flags &= ~IOCB_HIPRI; /* no one is going to poll for this I/O */\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,9 +2,6 @@\n {\n \tint ret;\n \n-\treq->ki_filp = fget(iocb->aio_fildes);\n-\tif (unlikely(!req->ki_filp))\n-\t\treturn -EBADF;\n \treq->ki_complete = aio_complete_rw;\n \treq->private = NULL;\n \treq->ki_pos = iocb->aio_offset;\n@@ -21,7 +18,7 @@\n \t\tret = ioprio_check_cap(iocb->aio_reqprio);\n \t\tif (ret) {\n \t\t\tpr_debug(\"aio ioprio check cap error: %d\\n\", ret);\n-\t\t\tgoto out_fput;\n+\t\t\treturn ret;\n \t\t}\n \n \t\treq->ki_ioprio = iocb->aio_reqprio;\n@@ -30,12 +27,8 @@\n \n \tret = kiocb_set_rw_flags(req, iocb->aio_rw_flags);\n \tif (unlikely(ret))\n-\t\tgoto out_fput;\n+\t\treturn ret;\n \n \treq->ki_flags &= ~IOCB_HIPRI; /* no one is going to poll for this I/O */\n \treturn 0;\n-\n-out_fput:\n-\tfput(req->ki_filp);\n-\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\treturn ret;",
                "\t\treturn ret;"
            ],
            "deleted": [
                "\treq->ki_filp = fget(iocb->aio_fildes);",
                "\tif (unlikely(!req->ki_filp))",
                "\t\treturn -EBADF;",
                "\t\t\tgoto out_fput;",
                "\t\tgoto out_fput;",
                "",
                "out_fput:",
                "\tfput(req->ki_filp);",
                "\treturn ret;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1884
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static inline void aio_poll_complete(struct aio_kiocb *iocb, __poll_t mask)\n{\n\tstruct file *file = iocb->poll.file;\n\n\taio_complete(iocb, mangle_poll(mask), 0);\n\tfput(file);\n}",
        "code_after_change": "static inline void aio_poll_complete(struct aio_kiocb *iocb, __poll_t mask)\n{\n\taio_complete(iocb, mangle_poll(mask), 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,4 @@\n static inline void aio_poll_complete(struct aio_kiocb *iocb, __poll_t mask)\n {\n-\tstruct file *file = iocb->poll.file;\n-\n \taio_complete(iocb, mangle_poll(mask), 0);\n-\tfput(file);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct file *file = iocb->poll.file;",
                "",
                "\tfput(file);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1886
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static inline void iocb_put(struct aio_kiocb *iocb)\n{\n\tif (refcount_read(&iocb->ki_refcnt) == 0 ||\n\t    refcount_dec_and_test(&iocb->ki_refcnt)) {\n\t\tpercpu_ref_put(&iocb->ki_ctx->reqs);\n\t\tkmem_cache_free(kiocb_cachep, iocb);\n\t}\n}",
        "code_after_change": "static inline void iocb_put(struct aio_kiocb *iocb)\n{\n\tif (refcount_read(&iocb->ki_refcnt) == 0 ||\n\t    refcount_dec_and_test(&iocb->ki_refcnt)) {\n\t\tif (iocb->ki_filp)\n\t\t\tfput(iocb->ki_filp);\n\t\tpercpu_ref_put(&iocb->ki_ctx->reqs);\n\t\tkmem_cache_free(kiocb_cachep, iocb);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,8 @@\n {\n \tif (refcount_read(&iocb->ki_refcnt) == 0 ||\n \t    refcount_dec_and_test(&iocb->ki_refcnt)) {\n+\t\tif (iocb->ki_filp)\n+\t\t\tfput(iocb->ki_filp);\n \t\tpercpu_ref_put(&iocb->ki_ctx->reqs);\n \t\tkmem_cache_free(kiocb_cachep, iocb);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (iocb->ki_filp)",
                "\t\t\tfput(iocb->ki_filp);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1892
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static ssize_t aio_write(struct kiocb *req, const struct iocb *iocb,\n\t\t\t bool vectored, bool compat)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\tstruct file *file;\n\tssize_t ret;\n\n\tret = aio_prep_rw(req, iocb);\n\tif (ret)\n\t\treturn ret;\n\tfile = req->ki_filp;\n\n\tret = -EBADF;\n\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\tgoto out_fput;\n\tret = -EINVAL;\n\tif (unlikely(!file->f_op->write_iter))\n\t\tgoto out_fput;\n\n\tret = aio_setup_rw(WRITE, iocb, &iovec, vectored, compat, &iter);\n\tif (ret)\n\t\tgoto out_fput;\n\tret = rw_verify_area(WRITE, file, &req->ki_pos, iov_iter_count(&iter));\n\tif (!ret) {\n\t\t/*\n\t\t * Open-code file_start_write here to grab freeze protection,\n\t\t * which will be released by another thread in\n\t\t * aio_complete_rw().  Fool lockdep by telling it the lock got\n\t\t * released so that it doesn't complain about the held lock when\n\t\t * we return to userspace.\n\t\t */\n\t\tif (S_ISREG(file_inode(file)->i_mode)) {\n\t\t\t__sb_start_write(file_inode(file)->i_sb, SB_FREEZE_WRITE, true);\n\t\t\t__sb_writers_release(file_inode(file)->i_sb, SB_FREEZE_WRITE);\n\t\t}\n\t\treq->ki_flags |= IOCB_WRITE;\n\t\taio_rw_done(req, call_write_iter(file, req, &iter));\n\t}\n\tkfree(iovec);\nout_fput:\n\tif (unlikely(ret))\n\t\tfput(file);\n\treturn ret;\n}",
        "code_after_change": "static ssize_t aio_write(struct kiocb *req, const struct iocb *iocb,\n\t\t\t bool vectored, bool compat)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\tstruct file *file;\n\tssize_t ret;\n\n\tret = aio_prep_rw(req, iocb);\n\tif (ret)\n\t\treturn ret;\n\tfile = req->ki_filp;\n\n\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\tif (unlikely(!file->f_op->write_iter))\n\t\treturn -EINVAL;\n\n\tret = aio_setup_rw(WRITE, iocb, &iovec, vectored, compat, &iter);\n\tif (ret)\n\t\treturn ret;\n\tret = rw_verify_area(WRITE, file, &req->ki_pos, iov_iter_count(&iter));\n\tif (!ret) {\n\t\t/*\n\t\t * Open-code file_start_write here to grab freeze protection,\n\t\t * which will be released by another thread in\n\t\t * aio_complete_rw().  Fool lockdep by telling it the lock got\n\t\t * released so that it doesn't complain about the held lock when\n\t\t * we return to userspace.\n\t\t */\n\t\tif (S_ISREG(file_inode(file)->i_mode)) {\n\t\t\t__sb_start_write(file_inode(file)->i_sb, SB_FREEZE_WRITE, true);\n\t\t\t__sb_writers_release(file_inode(file)->i_sb, SB_FREEZE_WRITE);\n\t\t}\n\t\treq->ki_flags |= IOCB_WRITE;\n\t\taio_rw_done(req, call_write_iter(file, req, &iter));\n\t}\n\tkfree(iovec);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,16 +11,14 @@\n \t\treturn ret;\n \tfile = req->ki_filp;\n \n-\tret = -EBADF;\n \tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n-\t\tgoto out_fput;\n-\tret = -EINVAL;\n+\t\treturn -EBADF;\n \tif (unlikely(!file->f_op->write_iter))\n-\t\tgoto out_fput;\n+\t\treturn -EINVAL;\n \n \tret = aio_setup_rw(WRITE, iocb, &iovec, vectored, compat, &iter);\n \tif (ret)\n-\t\tgoto out_fput;\n+\t\treturn ret;\n \tret = rw_verify_area(WRITE, file, &req->ki_pos, iov_iter_count(&iter));\n \tif (!ret) {\n \t\t/*\n@@ -38,8 +36,5 @@\n \t\taio_rw_done(req, call_write_iter(file, req, &iter));\n \t}\n \tkfree(iovec);\n-out_fput:\n-\tif (unlikely(ret))\n-\t\tfput(file);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\treturn -EBADF;",
                "\t\treturn -EINVAL;",
                "\t\treturn ret;"
            ],
            "deleted": [
                "\tret = -EBADF;",
                "\t\tgoto out_fput;",
                "\tret = -EINVAL;",
                "\t\tgoto out_fput;",
                "\t\tgoto out_fput;",
                "out_fput:",
                "\tif (unlikely(ret))",
                "\t\tfput(file);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1889
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static void aio_fsync_work(struct work_struct *work)\n{\n\tstruct fsync_iocb *req = container_of(work, struct fsync_iocb, work);\n\tint ret;\n\n\tret = vfs_fsync(req->file, req->datasync);\n\tfput(req->file);\n\taio_complete(container_of(req, struct aio_kiocb, fsync), ret, 0);\n}",
        "code_after_change": "static void aio_fsync_work(struct work_struct *work)\n{\n\tstruct fsync_iocb *req = container_of(work, struct fsync_iocb, work);\n\tint ret;\n\n\tret = vfs_fsync(req->file, req->datasync);\n\taio_complete(container_of(req, struct aio_kiocb, fsync), ret, 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,5 @@\n \tint ret;\n \n \tret = vfs_fsync(req->file, req->datasync);\n-\tfput(req->file);\n \taio_complete(container_of(req, struct aio_kiocb, fsync), ret, 0);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tfput(req->file);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1891
    },
    {
        "cve_id": "CVE-2019-19767",
        "code_before_change": "static int __ext4_expand_extra_isize(struct inode *inode,\n\t\t\t\t     unsigned int new_extra_isize,\n\t\t\t\t     struct ext4_iloc *iloc,\n\t\t\t\t     handle_t *handle, int *no_expand)\n{\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tint error;\n\n\traw_inode = ext4_raw_inode(iloc);\n\n\theader = IHDR(inode, raw_inode);\n\n\t/* No extended attributes present */\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||\n\t    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\tmemset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE +\n\t\t       EXT4_I(inode)->i_extra_isize, 0,\n\t\t       new_extra_isize - EXT4_I(inode)->i_extra_isize);\n\t\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\t\treturn 0;\n\t}\n\n\t/* try to expand with EAs present */\n\terror = ext4_expand_extra_isize_ea(inode, new_extra_isize,\n\t\t\t\t\t   raw_inode, handle);\n\tif (error) {\n\t\t/*\n\t\t * Inode size expansion failed; don't try again\n\t\t */\n\t\t*no_expand = 1;\n\t}\n\n\treturn error;\n}",
        "code_after_change": "static int __ext4_expand_extra_isize(struct inode *inode,\n\t\t\t\t     unsigned int new_extra_isize,\n\t\t\t\t     struct ext4_iloc *iloc,\n\t\t\t\t     handle_t *handle, int *no_expand)\n{\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tunsigned int inode_size = EXT4_INODE_SIZE(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint error;\n\n\t/* this was checked at iget time, but double check for good measure */\n\tif ((EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > inode_size) ||\n\t    (ei->i_extra_isize & 3)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extra_isize %u (inode size %u)\",\n\t\t\t\t ei->i_extra_isize,\n\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\treturn -EFSCORRUPTED;\n\t}\n\tif ((new_extra_isize < ei->i_extra_isize) ||\n\t    (new_extra_isize < 4) ||\n\t    (new_extra_isize > inode_size - EXT4_GOOD_OLD_INODE_SIZE))\n\t\treturn -EINVAL;\t/* Should never happen */\n\n\traw_inode = ext4_raw_inode(iloc);\n\n\theader = IHDR(inode, raw_inode);\n\n\t/* No extended attributes present */\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||\n\t    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\tmemset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE +\n\t\t       EXT4_I(inode)->i_extra_isize, 0,\n\t\t       new_extra_isize - EXT4_I(inode)->i_extra_isize);\n\t\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\t\treturn 0;\n\t}\n\n\t/* try to expand with EAs present */\n\terror = ext4_expand_extra_isize_ea(inode, new_extra_isize,\n\t\t\t\t\t   raw_inode, handle);\n\tif (error) {\n\t\t/*\n\t\t * Inode size expansion failed; don't try again\n\t\t */\n\t\t*no_expand = 1;\n\t}\n\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,22 @@\n {\n \tstruct ext4_inode *raw_inode;\n \tstruct ext4_xattr_ibody_header *header;\n+\tunsigned int inode_size = EXT4_INODE_SIZE(inode->i_sb);\n+\tstruct ext4_inode_info *ei = EXT4_I(inode);\n \tint error;\n+\n+\t/* this was checked at iget time, but double check for good measure */\n+\tif ((EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > inode_size) ||\n+\t    (ei->i_extra_isize & 3)) {\n+\t\tEXT4_ERROR_INODE(inode, \"bad extra_isize %u (inode size %u)\",\n+\t\t\t\t ei->i_extra_isize,\n+\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n+\t\treturn -EFSCORRUPTED;\n+\t}\n+\tif ((new_extra_isize < ei->i_extra_isize) ||\n+\t    (new_extra_isize < 4) ||\n+\t    (new_extra_isize > inode_size - EXT4_GOOD_OLD_INODE_SIZE))\n+\t\treturn -EINVAL;\t/* Should never happen */\n \n \traw_inode = ext4_raw_inode(iloc);\n ",
        "function_modified_lines": {
            "added": [
                "\tunsigned int inode_size = EXT4_INODE_SIZE(inode->i_sb);",
                "\tstruct ext4_inode_info *ei = EXT4_I(inode);",
                "",
                "\t/* this was checked at iget time, but double check for good measure */",
                "\tif ((EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > inode_size) ||",
                "\t    (ei->i_extra_isize & 3)) {",
                "\t\tEXT4_ERROR_INODE(inode, \"bad extra_isize %u (inode size %u)\",",
                "\t\t\t\t ei->i_extra_isize,",
                "\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));",
                "\t\treturn -EFSCORRUPTED;",
                "\t}",
                "\tif ((new_extra_isize < ei->i_extra_isize) ||",
                "\t    (new_extra_isize < 4) ||",
                "\t    (new_extra_isize > inode_size - EXT4_GOOD_OLD_INODE_SIZE))",
                "\t\treturn -EINVAL;\t/* Should never happen */"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.4.2 mishandles ext4_expand_extra_isize, as demonstrated by use-after-free errors in __ext4_expand_extra_isize and ext4_xattr_set_entry, related to fs/ext4/inode.c and fs/ext4/super.c, aka CID-4ea99936a163.",
        "id": 2223
    },
    {
        "cve_id": "CVE-2020-7053",
        "code_before_change": "int i915_gem_context_destroy_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t   struct drm_file *file)\n{\n\tstruct drm_i915_gem_context_destroy *args = data;\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\tint ret;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tif (args->ctx_id == DEFAULT_CONTEXT_HANDLE)\n\t\treturn -ENOENT;\n\n\tctx = i915_gem_context_lookup(file_priv, args->ctx_id);\n\tif (!ctx)\n\t\treturn -ENOENT;\n\n\tret = mutex_lock_interruptible(&dev->struct_mutex);\n\tif (ret)\n\t\tgoto out;\n\n\tidr_remove(&file_priv->context_idr, ctx->user_handle);\n\tcontext_close(ctx);\n\n\tmutex_unlock(&dev->struct_mutex);\n\nout:\n\ti915_gem_context_put(ctx);\n\treturn 0;\n}",
        "code_after_change": "int i915_gem_context_destroy_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t   struct drm_file *file)\n{\n\tstruct drm_i915_gem_context_destroy *args = data;\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tif (args->ctx_id == DEFAULT_CONTEXT_HANDLE)\n\t\treturn -ENOENT;\n\n\tif (mutex_lock_interruptible(&file_priv->context_idr_lock))\n\t\treturn -EINTR;\n\n\tctx = idr_remove(&file_priv->context_idr, args->ctx_id);\n\tmutex_unlock(&file_priv->context_idr_lock);\n\tif (!ctx)\n\t\treturn -ENOENT;\n\n\tmutex_lock(&dev->struct_mutex);\n\tcontext_close(ctx);\n\tmutex_unlock(&dev->struct_mutex);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tstruct drm_i915_gem_context_destroy *args = data;\n \tstruct drm_i915_file_private *file_priv = file->driver_priv;\n \tstruct i915_gem_context *ctx;\n-\tint ret;\n \n \tif (args->pad != 0)\n \t\treturn -EINVAL;\n@@ -12,20 +11,17 @@\n \tif (args->ctx_id == DEFAULT_CONTEXT_HANDLE)\n \t\treturn -ENOENT;\n \n-\tctx = i915_gem_context_lookup(file_priv, args->ctx_id);\n+\tif (mutex_lock_interruptible(&file_priv->context_idr_lock))\n+\t\treturn -EINTR;\n+\n+\tctx = idr_remove(&file_priv->context_idr, args->ctx_id);\n+\tmutex_unlock(&file_priv->context_idr_lock);\n \tif (!ctx)\n \t\treturn -ENOENT;\n \n-\tret = mutex_lock_interruptible(&dev->struct_mutex);\n-\tif (ret)\n-\t\tgoto out;\n-\n-\tidr_remove(&file_priv->context_idr, ctx->user_handle);\n+\tmutex_lock(&dev->struct_mutex);\n \tcontext_close(ctx);\n-\n \tmutex_unlock(&dev->struct_mutex);\n \n-out:\n-\ti915_gem_context_put(ctx);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (mutex_lock_interruptible(&file_priv->context_idr_lock))",
                "\t\treturn -EINTR;",
                "",
                "\tctx = idr_remove(&file_priv->context_idr, args->ctx_id);",
                "\tmutex_unlock(&file_priv->context_idr_lock);",
                "\tmutex_lock(&dev->struct_mutex);"
            ],
            "deleted": [
                "\tint ret;",
                "\tctx = i915_gem_context_lookup(file_priv, args->ctx_id);",
                "\tret = mutex_lock_interruptible(&dev->struct_mutex);",
                "\tif (ret)",
                "\t\tgoto out;",
                "",
                "\tidr_remove(&file_priv->context_idr, ctx->user_handle);",
                "",
                "out:",
                "\ti915_gem_context_put(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.14 longterm through 4.14.165 and 4.19 longterm through 4.19.96 (and 5.x before 5.2), there is a use-after-free (write) in the i915_ppgtt_close function in drivers/gpu/drm/i915/i915_gem_gtt.c, aka CID-7dc40713618c. This is related to i915_gem_context_destroy_ioctl in drivers/gpu/drm/i915/i915_gem_context.c.",
        "id": 2798
    },
    {
        "cve_id": "CVE-2020-7053",
        "code_before_change": "static int gem_context_register(struct i915_gem_context *ctx,\n\t\t\t\tstruct drm_i915_file_private *fpriv)\n{\n\tint ret;\n\n\tctx->file_priv = fpriv;\n\tif (ctx->ppgtt)\n\t\tctx->ppgtt->vm.file = fpriv;\n\n\tctx->pid = get_task_pid(current, PIDTYPE_PID);\n\tctx->name = kasprintf(GFP_KERNEL, \"%s[%d]\",\n\t\t\t      current->comm, pid_nr(ctx->pid));\n\tif (!ctx->name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_pid;\n\t}\n\n\t/* And finally expose ourselves to userspace via the idr */\n\tret = idr_alloc(&fpriv->context_idr, ctx,\n\t\t\tDEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto err_name;\n\n\tctx->user_handle = ret;\n\n\treturn 0;\n\nerr_name:\n\tkfree(fetch_and_zero(&ctx->name));\nerr_pid:\n\tput_pid(fetch_and_zero(&ctx->pid));\n\treturn ret;\n}",
        "code_after_change": "static int gem_context_register(struct i915_gem_context *ctx,\n\t\t\t\tstruct drm_i915_file_private *fpriv)\n{\n\tint ret;\n\n\tctx->file_priv = fpriv;\n\tif (ctx->ppgtt)\n\t\tctx->ppgtt->vm.file = fpriv;\n\n\tctx->pid = get_task_pid(current, PIDTYPE_PID);\n\tctx->name = kasprintf(GFP_KERNEL, \"%s[%d]\",\n\t\t\t      current->comm, pid_nr(ctx->pid));\n\tif (!ctx->name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_pid;\n\t}\n\n\t/* And finally expose ourselves to userspace via the idr */\n\tmutex_lock(&fpriv->context_idr_lock);\n\tret = idr_alloc(&fpriv->context_idr, ctx,\n\t\t\tDEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);\n\tif (ret >= 0)\n\t\tctx->user_handle = ret;\n\tmutex_unlock(&fpriv->context_idr_lock);\n\tif (ret < 0)\n\t\tgoto err_name;\n\n\treturn 0;\n\nerr_name:\n\tkfree(fetch_and_zero(&ctx->name));\nerr_pid:\n\tput_pid(fetch_and_zero(&ctx->pid));\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,12 +16,14 @@\n \t}\n \n \t/* And finally expose ourselves to userspace via the idr */\n+\tmutex_lock(&fpriv->context_idr_lock);\n \tret = idr_alloc(&fpriv->context_idr, ctx,\n \t\t\tDEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);\n+\tif (ret >= 0)\n+\t\tctx->user_handle = ret;\n+\tmutex_unlock(&fpriv->context_idr_lock);\n \tif (ret < 0)\n \t\tgoto err_name;\n-\n-\tctx->user_handle = ret;\n \n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&fpriv->context_idr_lock);",
                "\tif (ret >= 0)",
                "\t\tctx->user_handle = ret;",
                "\tmutex_unlock(&fpriv->context_idr_lock);"
            ],
            "deleted": [
                "",
                "\tctx->user_handle = ret;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.14 longterm through 4.14.165 and 4.19 longterm through 4.19.96 (and 5.x before 5.2), there is a use-after-free (write) in the i915_ppgtt_close function in drivers/gpu/drm/i915/i915_gem_gtt.c, aka CID-7dc40713618c. This is related to i915_gem_context_destroy_ioctl in drivers/gpu/drm/i915/i915_gem_context.c.",
        "id": 2799
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static int nft_immediate_init(const struct nft_ctx *ctx,\n\t\t\t      const struct nft_expr *expr,\n\t\t\t      const struct nlattr * const tb[])\n{\n\tstruct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tstruct nft_data_desc desc = {\n\t\t.size\t= sizeof(priv->data),\n\t};\n\tint err;\n\n\tif (tb[NFTA_IMMEDIATE_DREG] == NULL ||\n\t    tb[NFTA_IMMEDIATE_DATA] == NULL)\n\t\treturn -EINVAL;\n\n\tdesc.type = nft_reg_to_type(tb[NFTA_IMMEDIATE_DREG]);\n\terr = nft_data_init(ctx, &priv->data, &desc, tb[NFTA_IMMEDIATE_DATA]);\n\tif (err < 0)\n\t\treturn err;\n\n\tpriv->dlen = desc.len;\n\n\terr = nft_parse_register_store(ctx, tb[NFTA_IMMEDIATE_DREG],\n\t\t\t\t       &priv->dreg, &priv->data, desc.type,\n\t\t\t\t       desc.len);\n\tif (err < 0)\n\t\tgoto err1;\n\n\tif (priv->dreg == NFT_REG_VERDICT) {\n\t\tstruct nft_chain *chain = priv->data.verdict.chain;\n\n\t\tswitch (priv->data.verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tif (nft_chain_is_bound(chain)) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tgoto err1;\n\t\t\t}\n\t\t\tchain->bound = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr1:\n\tnft_data_release(&priv->data, desc.type);\n\treturn err;\n}",
        "code_after_change": "static int nft_immediate_init(const struct nft_ctx *ctx,\n\t\t\t      const struct nft_expr *expr,\n\t\t\t      const struct nlattr * const tb[])\n{\n\tstruct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tstruct nft_data_desc desc = {\n\t\t.size\t= sizeof(priv->data),\n\t};\n\tint err;\n\n\tif (tb[NFTA_IMMEDIATE_DREG] == NULL ||\n\t    tb[NFTA_IMMEDIATE_DATA] == NULL)\n\t\treturn -EINVAL;\n\n\tdesc.type = nft_reg_to_type(tb[NFTA_IMMEDIATE_DREG]);\n\terr = nft_data_init(ctx, &priv->data, &desc, tb[NFTA_IMMEDIATE_DATA]);\n\tif (err < 0)\n\t\treturn err;\n\n\tpriv->dlen = desc.len;\n\n\terr = nft_parse_register_store(ctx, tb[NFTA_IMMEDIATE_DREG],\n\t\t\t\t       &priv->dreg, &priv->data, desc.type,\n\t\t\t\t       desc.len);\n\tif (err < 0)\n\t\tgoto err1;\n\n\tif (priv->dreg == NFT_REG_VERDICT) {\n\t\tstruct nft_chain *chain = priv->data.verdict.chain;\n\n\t\tswitch (priv->data.verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\terr = nf_tables_bind_chain(ctx, chain);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr1:\n\tnft_data_release(&priv->data, desc.type);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,11 +31,9 @@\n \t\tswitch (priv->data.verdict.code) {\n \t\tcase NFT_JUMP:\n \t\tcase NFT_GOTO:\n-\t\t\tif (nft_chain_is_bound(chain)) {\n-\t\t\t\terr = -EBUSY;\n-\t\t\t\tgoto err1;\n-\t\t\t}\n-\t\t\tchain->bound = true;\n+\t\t\terr = nf_tables_bind_chain(ctx, chain);\n+\t\t\tif (err < 0)\n+\t\t\t\treturn err;\n \t\t\tbreak;\n \t\tdefault:\n \t\t\tbreak;",
        "function_modified_lines": {
            "added": [
                "\t\t\terr = nf_tables_bind_chain(ctx, chain);",
                "\t\t\tif (err < 0)",
                "\t\t\t\treturn err;"
            ],
            "deleted": [
                "\t\t\tif (nft_chain_is_bound(chain)) {",
                "\t\t\t\terr = -EBUSY;",
                "\t\t\t\tgoto err1;",
                "\t\t\t}",
                "\t\t\tchain->bound = true;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4129
    },
    {
        "cve_id": "CVE-2022-1882",
        "code_before_change": "int remove_watch_from_object(struct watch_list *wlist, struct watch_queue *wq,\n\t\t\t     u64 id, bool all)\n{\n\tstruct watch_notification_removal n;\n\tstruct watch_queue *wqueue;\n\tstruct watch *watch;\n\tint ret = -EBADSLT;\n\n\trcu_read_lock();\n\nagain:\n\tspin_lock(&wlist->lock);\n\thlist_for_each_entry(watch, &wlist->watchers, list_node) {\n\t\tif (all ||\n\t\t    (watch->id == id && rcu_access_pointer(watch->queue) == wq))\n\t\t\tgoto found;\n\t}\n\tspin_unlock(&wlist->lock);\n\tgoto out;\n\nfound:\n\tret = 0;\n\thlist_del_init_rcu(&watch->list_node);\n\trcu_assign_pointer(watch->watch_list, NULL);\n\tspin_unlock(&wlist->lock);\n\n\t/* We now own the reference on watch that used to belong to wlist. */\n\n\tn.watch.type = WATCH_TYPE_META;\n\tn.watch.subtype = WATCH_META_REMOVAL_NOTIFICATION;\n\tn.watch.info = watch->info_id | watch_sizeof(n.watch);\n\tn.id = id;\n\tif (id != 0)\n\t\tn.watch.info = watch->info_id | watch_sizeof(n);\n\n\twqueue = rcu_dereference(watch->queue);\n\n\t/* We don't need the watch list lock for the next bit as RCU is\n\t * protecting *wqueue from deallocation.\n\t */\n\tif (wqueue) {\n\t\tpost_one_notification(wqueue, &n.watch);\n\n\t\tspin_lock_bh(&wqueue->lock);\n\n\t\tif (!hlist_unhashed(&watch->queue_node)) {\n\t\t\thlist_del_init_rcu(&watch->queue_node);\n\t\t\tput_watch(watch);\n\t\t}\n\n\t\tspin_unlock_bh(&wqueue->lock);\n\t}\n\n\tif (wlist->release_watch) {\n\t\tvoid (*release_watch)(struct watch *);\n\n\t\trelease_watch = wlist->release_watch;\n\t\trcu_read_unlock();\n\t\t(*release_watch)(watch);\n\t\trcu_read_lock();\n\t}\n\tput_watch(watch);\n\n\tif (all && !hlist_empty(&wlist->watchers))\n\t\tgoto again;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}",
        "code_after_change": "int remove_watch_from_object(struct watch_list *wlist, struct watch_queue *wq,\n\t\t\t     u64 id, bool all)\n{\n\tstruct watch_notification_removal n;\n\tstruct watch_queue *wqueue;\n\tstruct watch *watch;\n\tint ret = -EBADSLT;\n\n\trcu_read_lock();\n\nagain:\n\tspin_lock(&wlist->lock);\n\thlist_for_each_entry(watch, &wlist->watchers, list_node) {\n\t\tif (all ||\n\t\t    (watch->id == id && rcu_access_pointer(watch->queue) == wq))\n\t\t\tgoto found;\n\t}\n\tspin_unlock(&wlist->lock);\n\tgoto out;\n\nfound:\n\tret = 0;\n\thlist_del_init_rcu(&watch->list_node);\n\trcu_assign_pointer(watch->watch_list, NULL);\n\tspin_unlock(&wlist->lock);\n\n\t/* We now own the reference on watch that used to belong to wlist. */\n\n\tn.watch.type = WATCH_TYPE_META;\n\tn.watch.subtype = WATCH_META_REMOVAL_NOTIFICATION;\n\tn.watch.info = watch->info_id | watch_sizeof(n.watch);\n\tn.id = id;\n\tif (id != 0)\n\t\tn.watch.info = watch->info_id | watch_sizeof(n);\n\n\twqueue = rcu_dereference(watch->queue);\n\n\tif (lock_wqueue(wqueue)) {\n\t\tpost_one_notification(wqueue, &n.watch);\n\n\t\tif (!hlist_unhashed(&watch->queue_node)) {\n\t\t\thlist_del_init_rcu(&watch->queue_node);\n\t\t\tput_watch(watch);\n\t\t}\n\n\t\tunlock_wqueue(wqueue);\n\t}\n\n\tif (wlist->release_watch) {\n\t\tvoid (*release_watch)(struct watch *);\n\n\t\trelease_watch = wlist->release_watch;\n\t\trcu_read_unlock();\n\t\t(*release_watch)(watch);\n\t\trcu_read_lock();\n\t}\n\tput_watch(watch);\n\n\tif (all && !hlist_empty(&wlist->watchers))\n\t\tgoto again;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,20 +35,15 @@\n \n \twqueue = rcu_dereference(watch->queue);\n \n-\t/* We don't need the watch list lock for the next bit as RCU is\n-\t * protecting *wqueue from deallocation.\n-\t */\n-\tif (wqueue) {\n+\tif (lock_wqueue(wqueue)) {\n \t\tpost_one_notification(wqueue, &n.watch);\n-\n-\t\tspin_lock_bh(&wqueue->lock);\n \n \t\tif (!hlist_unhashed(&watch->queue_node)) {\n \t\t\thlist_del_init_rcu(&watch->queue_node);\n \t\t\tput_watch(watch);\n \t\t}\n \n-\t\tspin_unlock_bh(&wqueue->lock);\n+\t\tunlock_wqueue(wqueue);\n \t}\n \n \tif (wlist->release_watch) {",
        "function_modified_lines": {
            "added": [
                "\tif (lock_wqueue(wqueue)) {",
                "\t\tunlock_wqueue(wqueue);"
            ],
            "deleted": [
                "\t/* We don't need the watch list lock for the next bit as RCU is",
                "\t * protecting *wqueue from deallocation.",
                "\t */",
                "\tif (wqueue) {",
                "",
                "\t\tspin_lock_bh(&wqueue->lock);",
                "\t\tspin_unlock_bh(&wqueue->lock);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s pipes functionality in how a user performs manipulations with the pipe post_one_notification() after free_pipe_info() that is already called. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3299
    },
    {
        "cve_id": "CVE-2022-1882",
        "code_before_change": "static bool post_one_notification(struct watch_queue *wqueue,\n\t\t\t\t  struct watch_notification *n)\n{\n\tvoid *p;\n\tstruct pipe_inode_info *pipe = wqueue->pipe;\n\tstruct pipe_buffer *buf;\n\tstruct page *page;\n\tunsigned int head, tail, mask, note, offset, len;\n\tbool done = false;\n\n\tif (!pipe)\n\t\treturn false;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\n\tif (wqueue->defunct)\n\t\tgoto out;\n\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tif (pipe_full(head, tail, pipe->ring_size))\n\t\tgoto lost;\n\n\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);\n\tif (note >= wqueue->nr_notes)\n\t\tgoto lost;\n\n\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];\n\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;\n\tget_page(page);\n\tlen = n->info & WATCH_INFO_LENGTH;\n\tp = kmap_atomic(page);\n\tmemcpy(p + offset, n, len);\n\tkunmap_atomic(p);\n\n\tbuf = &pipe->bufs[head & mask];\n\tbuf->page = page;\n\tbuf->private = (unsigned long)wqueue;\n\tbuf->ops = &watch_queue_pipe_buf_ops;\n\tbuf->offset = offset;\n\tbuf->len = len;\n\tbuf->flags = PIPE_BUF_FLAG_WHOLE;\n\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */\n\n\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tBUG();\n\t}\n\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tdone = true;\n\nout:\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\tif (done)\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\treturn done;\n\nlost:\n\tbuf = &pipe->bufs[(head - 1) & mask];\n\tbuf->flags |= PIPE_BUF_FLAG_LOSS;\n\tgoto out;\n}",
        "code_after_change": "static bool post_one_notification(struct watch_queue *wqueue,\n\t\t\t\t  struct watch_notification *n)\n{\n\tvoid *p;\n\tstruct pipe_inode_info *pipe = wqueue->pipe;\n\tstruct pipe_buffer *buf;\n\tstruct page *page;\n\tunsigned int head, tail, mask, note, offset, len;\n\tbool done = false;\n\n\tif (!pipe)\n\t\treturn false;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tif (pipe_full(head, tail, pipe->ring_size))\n\t\tgoto lost;\n\n\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);\n\tif (note >= wqueue->nr_notes)\n\t\tgoto lost;\n\n\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];\n\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;\n\tget_page(page);\n\tlen = n->info & WATCH_INFO_LENGTH;\n\tp = kmap_atomic(page);\n\tmemcpy(p + offset, n, len);\n\tkunmap_atomic(p);\n\n\tbuf = &pipe->bufs[head & mask];\n\tbuf->page = page;\n\tbuf->private = (unsigned long)wqueue;\n\tbuf->ops = &watch_queue_pipe_buf_ops;\n\tbuf->offset = offset;\n\tbuf->len = len;\n\tbuf->flags = PIPE_BUF_FLAG_WHOLE;\n\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */\n\n\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tBUG();\n\t}\n\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tdone = true;\n\nout:\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\tif (done)\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\treturn done;\n\nlost:\n\tbuf = &pipe->bufs[(head - 1) & mask];\n\tbuf->flags |= PIPE_BUF_FLAG_LOSS;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,9 +12,6 @@\n \t\treturn false;\n \n \tspin_lock_irq(&pipe->rd_wait.lock);\n-\n-\tif (wqueue->defunct)\n-\t\tgoto out;\n \n \tmask = pipe->ring_size - 1;\n \thead = pipe->head;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (wqueue->defunct)",
                "\t\tgoto out;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s pipes functionality in how a user performs manipulations with the pipe post_one_notification() after free_pipe_info() that is already called. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3297
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static ssize_t sysfs_blk_trace_attr_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct hd_struct *p = dev_to_part(dev);\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tssize_t ret = -ENXIO;\n\n\tbdev = bdget(part_devt(p));\n\tif (bdev == NULL)\n\t\tgoto out;\n\n\tq = blk_trace_get_queue(bdev);\n\tif (q == NULL)\n\t\tgoto out_bdput;\n\n\tmutex_lock(&q->blk_trace_mutex);\n\n\tif (attr == &dev_attr_enable) {\n\t\tret = sprintf(buf, \"%u\\n\", !!q->blk_trace);\n\t\tgoto out_unlock_bdev;\n\t}\n\n\tif (q->blk_trace == NULL)\n\t\tret = sprintf(buf, \"disabled\\n\");\n\telse if (attr == &dev_attr_act_mask)\n\t\tret = blk_trace_mask2str(buf, q->blk_trace->act_mask);\n\telse if (attr == &dev_attr_pid)\n\t\tret = sprintf(buf, \"%u\\n\", q->blk_trace->pid);\n\telse if (attr == &dev_attr_start_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->start_lba);\n\telse if (attr == &dev_attr_end_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->end_lba);\n\nout_unlock_bdev:\n\tmutex_unlock(&q->blk_trace_mutex);\nout_bdput:\n\tbdput(bdev);\nout:\n\treturn ret;\n}",
        "code_after_change": "static ssize_t sysfs_blk_trace_attr_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct hd_struct *p = dev_to_part(dev);\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tstruct blk_trace *bt;\n\tssize_t ret = -ENXIO;\n\n\tbdev = bdget(part_devt(p));\n\tif (bdev == NULL)\n\t\tgoto out;\n\n\tq = blk_trace_get_queue(bdev);\n\tif (q == NULL)\n\t\tgoto out_bdput;\n\n\tmutex_lock(&q->blk_trace_mutex);\n\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n\tif (attr == &dev_attr_enable) {\n\t\tret = sprintf(buf, \"%u\\n\", !!bt);\n\t\tgoto out_unlock_bdev;\n\t}\n\n\tif (bt == NULL)\n\t\tret = sprintf(buf, \"disabled\\n\");\n\telse if (attr == &dev_attr_act_mask)\n\t\tret = blk_trace_mask2str(buf, bt->act_mask);\n\telse if (attr == &dev_attr_pid)\n\t\tret = sprintf(buf, \"%u\\n\", bt->pid);\n\telse if (attr == &dev_attr_start_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", bt->start_lba);\n\telse if (attr == &dev_attr_end_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", bt->end_lba);\n\nout_unlock_bdev:\n\tmutex_unlock(&q->blk_trace_mutex);\nout_bdput:\n\tbdput(bdev);\nout:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct hd_struct *p = dev_to_part(dev);\n \tstruct request_queue *q;\n \tstruct block_device *bdev;\n+\tstruct blk_trace *bt;\n \tssize_t ret = -ENXIO;\n \n \tbdev = bdget(part_devt(p));\n@@ -17,21 +18,23 @@\n \n \tmutex_lock(&q->blk_trace_mutex);\n \n+\tbt = rcu_dereference_protected(q->blk_trace,\n+\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n \tif (attr == &dev_attr_enable) {\n-\t\tret = sprintf(buf, \"%u\\n\", !!q->blk_trace);\n+\t\tret = sprintf(buf, \"%u\\n\", !!bt);\n \t\tgoto out_unlock_bdev;\n \t}\n \n-\tif (q->blk_trace == NULL)\n+\tif (bt == NULL)\n \t\tret = sprintf(buf, \"disabled\\n\");\n \telse if (attr == &dev_attr_act_mask)\n-\t\tret = blk_trace_mask2str(buf, q->blk_trace->act_mask);\n+\t\tret = blk_trace_mask2str(buf, bt->act_mask);\n \telse if (attr == &dev_attr_pid)\n-\t\tret = sprintf(buf, \"%u\\n\", q->blk_trace->pid);\n+\t\tret = sprintf(buf, \"%u\\n\", bt->pid);\n \telse if (attr == &dev_attr_start_lba)\n-\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->start_lba);\n+\t\tret = sprintf(buf, \"%llu\\n\", bt->start_lba);\n \telse if (attr == &dev_attr_end_lba)\n-\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->end_lba);\n+\t\tret = sprintf(buf, \"%llu\\n\", bt->end_lba);\n \n out_unlock_bdev:\n \tmutex_unlock(&q->blk_trace_mutex);",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\tbt = rcu_dereference_protected(q->blk_trace,",
                "\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));",
                "\t\tret = sprintf(buf, \"%u\\n\", !!bt);",
                "\tif (bt == NULL)",
                "\t\tret = blk_trace_mask2str(buf, bt->act_mask);",
                "\t\tret = sprintf(buf, \"%u\\n\", bt->pid);",
                "\t\tret = sprintf(buf, \"%llu\\n\", bt->start_lba);",
                "\t\tret = sprintf(buf, \"%llu\\n\", bt->end_lba);"
            ],
            "deleted": [
                "\t\tret = sprintf(buf, \"%u\\n\", !!q->blk_trace);",
                "\tif (q->blk_trace == NULL)",
                "\t\tret = blk_trace_mask2str(buf, q->blk_trace->act_mask);",
                "\t\tret = sprintf(buf, \"%u\\n\", q->blk_trace->pid);",
                "\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->start_lba);",
                "\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->end_lba);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2231
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static int __blk_trace_startstop(struct request_queue *q, int start)\n{\n\tint ret;\n\tstruct blk_trace *bt = q->blk_trace;\n\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\t/*\n\t * For starting a trace, we can transition from a setup or stopped\n\t * trace. For stopping a trace, the state must be running\n\t */\n\tret = -EINVAL;\n\tif (start) {\n\t\tif (bt->trace_state == Blktrace_setup ||\n\t\t    bt->trace_state == Blktrace_stopped) {\n\t\t\tblktrace_seq++;\n\t\t\tsmp_mb();\n\t\t\tbt->trace_state = Blktrace_running;\n\t\t\tspin_lock_irq(&running_trace_lock);\n\t\t\tlist_add(&bt->running_list, &running_trace_list);\n\t\t\tspin_unlock_irq(&running_trace_lock);\n\n\t\t\ttrace_note_time(bt);\n\t\t\tret = 0;\n\t\t}\n\t} else {\n\t\tif (bt->trace_state == Blktrace_running) {\n\t\t\tbt->trace_state = Blktrace_stopped;\n\t\t\tspin_lock_irq(&running_trace_lock);\n\t\t\tlist_del_init(&bt->running_list);\n\t\t\tspin_unlock_irq(&running_trace_lock);\n\t\t\trelay_flush(bt->rchan);\n\t\t\tret = 0;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int __blk_trace_startstop(struct request_queue *q, int start)\n{\n\tint ret;\n\tstruct blk_trace *bt;\n\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\t/*\n\t * For starting a trace, we can transition from a setup or stopped\n\t * trace. For stopping a trace, the state must be running\n\t */\n\tret = -EINVAL;\n\tif (start) {\n\t\tif (bt->trace_state == Blktrace_setup ||\n\t\t    bt->trace_state == Blktrace_stopped) {\n\t\t\tblktrace_seq++;\n\t\t\tsmp_mb();\n\t\t\tbt->trace_state = Blktrace_running;\n\t\t\tspin_lock_irq(&running_trace_lock);\n\t\t\tlist_add(&bt->running_list, &running_trace_list);\n\t\t\tspin_unlock_irq(&running_trace_lock);\n\n\t\t\ttrace_note_time(bt);\n\t\t\tret = 0;\n\t\t}\n\t} else {\n\t\tif (bt->trace_state == Blktrace_running) {\n\t\t\tbt->trace_state = Blktrace_stopped;\n\t\t\tspin_lock_irq(&running_trace_lock);\n\t\t\tlist_del_init(&bt->running_list);\n\t\t\tspin_unlock_irq(&running_trace_lock);\n\t\t\trelay_flush(bt->rchan);\n\t\t\tret = 0;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,10 @@\n static int __blk_trace_startstop(struct request_queue *q, int start)\n {\n \tint ret;\n-\tstruct blk_trace *bt = q->blk_trace;\n+\tstruct blk_trace *bt;\n \n+\tbt = rcu_dereference_protected(q->blk_trace,\n+\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n \tif (bt == NULL)\n \t\treturn -EINVAL;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\tbt = rcu_dereference_protected(q->blk_trace,",
                "\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = q->blk_trace;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2240
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static inline bool blk_trace_note_message_enabled(struct request_queue *q)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\tif (likely(!bt))\n\t\treturn false;\n\treturn bt->act_mask & BLK_TC_NOTIFY;\n}",
        "code_after_change": "static inline bool blk_trace_note_message_enabled(struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\tbool ret;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tret = bt && (bt->act_mask & BLK_TC_NOTIFY);\n\trcu_read_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,11 @@\n static inline bool blk_trace_note_message_enabled(struct request_queue *q)\n {\n-\tstruct blk_trace *bt = q->blk_trace;\n-\tif (likely(!bt))\n-\t\treturn false;\n-\treturn bt->act_mask & BLK_TC_NOTIFY;\n+\tstruct blk_trace *bt;\n+\tbool ret;\n+\n+\trcu_read_lock();\n+\tbt = rcu_dereference(q->blk_trace);\n+\tret = bt && (bt->act_mask & BLK_TC_NOTIFY);\n+\trcu_read_unlock();\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\tbool ret;",
                "",
                "\trcu_read_lock();",
                "\tbt = rcu_dereference(q->blk_trace);",
                "\tret = bt && (bt->act_mask & BLK_TC_NOTIFY);",
                "\trcu_read_unlock();",
                "\treturn ret;"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = q->blk_trace;",
                "\tif (likely(!bt))",
                "\t\treturn false;",
                "\treturn bt->act_mask & BLK_TC_NOTIFY;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2225
    },
    {
        "cve_id": "CVE-2022-41218",
        "code_before_change": "static int dvb_demux_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dmxdev *dmxdev = dvbdev->priv;\n\tint i;\n\tstruct dmxdev_filter *dmxdevfilter;\n\n\tif (!dmxdev->filter)\n\t\treturn -EINVAL;\n\n\tif (mutex_lock_interruptible(&dmxdev->mutex))\n\t\treturn -ERESTARTSYS;\n\n\tfor (i = 0; i < dmxdev->filternum; i++)\n\t\tif (dmxdev->filter[i].state == DMXDEV_STATE_FREE)\n\t\t\tbreak;\n\n\tif (i == dmxdev->filternum) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -EMFILE;\n\t}\n\n\tdmxdevfilter = &dmxdev->filter[i];\n\tmutex_init(&dmxdevfilter->mutex);\n\tfile->private_data = dmxdevfilter;\n\n#ifdef CONFIG_DVB_MMAP\n\tdmxdev->may_do_mmap = 1;\n#else\n\tdmxdev->may_do_mmap = 0;\n#endif\n\n\tdvb_ringbuffer_init(&dmxdevfilter->buffer, NULL, 8192);\n\tdvb_vb2_init(&dmxdevfilter->vb2_ctx, \"demux_filter\",\n\t\t     file->f_flags & O_NONBLOCK);\n\tdmxdevfilter->type = DMXDEV_TYPE_NONE;\n\tdvb_dmxdev_filter_state_set(dmxdevfilter, DMXDEV_STATE_ALLOCATED);\n\ttimer_setup(&dmxdevfilter->timer, dvb_dmxdev_filter_timeout, 0);\n\n\tdvbdev->users++;\n\n\tmutex_unlock(&dmxdev->mutex);\n\treturn 0;\n}",
        "code_after_change": "static int dvb_demux_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dmxdev *dmxdev = dvbdev->priv;\n\tint i;\n\tstruct dmxdev_filter *dmxdevfilter;\n\n\tif (!dmxdev->filter)\n\t\treturn -EINVAL;\n\n\tif (mutex_lock_interruptible(&dmxdev->mutex))\n\t\treturn -ERESTARTSYS;\n\n\tif (dmxdev->exit) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tfor (i = 0; i < dmxdev->filternum; i++)\n\t\tif (dmxdev->filter[i].state == DMXDEV_STATE_FREE)\n\t\t\tbreak;\n\n\tif (i == dmxdev->filternum) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -EMFILE;\n\t}\n\n\tdmxdevfilter = &dmxdev->filter[i];\n\tmutex_init(&dmxdevfilter->mutex);\n\tfile->private_data = dmxdevfilter;\n\n#ifdef CONFIG_DVB_MMAP\n\tdmxdev->may_do_mmap = 1;\n#else\n\tdmxdev->may_do_mmap = 0;\n#endif\n\n\tdvb_ringbuffer_init(&dmxdevfilter->buffer, NULL, 8192);\n\tdvb_vb2_init(&dmxdevfilter->vb2_ctx, \"demux_filter\",\n\t\t     file->f_flags & O_NONBLOCK);\n\tdmxdevfilter->type = DMXDEV_TYPE_NONE;\n\tdvb_dmxdev_filter_state_set(dmxdevfilter, DMXDEV_STATE_ALLOCATED);\n\ttimer_setup(&dmxdevfilter->timer, dvb_dmxdev_filter_timeout, 0);\n\n\tdvbdev->users++;\n\n\tmutex_unlock(&dmxdev->mutex);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,11 @@\n \n \tif (mutex_lock_interruptible(&dmxdev->mutex))\n \t\treturn -ERESTARTSYS;\n+\n+\tif (dmxdev->exit) {\n+\t\tmutex_unlock(&dmxdev->mutex);\n+\t\treturn -ENODEV;\n+\t}\n \n \tfor (i = 0; i < dmxdev->filternum; i++)\n \t\tif (dmxdev->filter[i].state == DMXDEV_STATE_FREE)",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (dmxdev->exit) {",
                "\t\tmutex_unlock(&dmxdev->mutex);",
                "\t\treturn -ENODEV;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In drivers/media/dvb-core/dmxdev.c in the Linux kernel through 5.19.10, there is a use-after-free caused by refcount races, affecting dvb_demux_open and dvb_dmxdev_release.",
        "id": 3712
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static inline int l2cap_move_channel_req(struct l2cap_conn *conn,\n\t\t\t\t\t struct l2cap_cmd_hdr *cmd,\n\t\t\t\t\t u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_req *req = data;\n\tstruct l2cap_move_chan_rsp rsp;\n\tstruct l2cap_chan *chan;\n\tu16 icid = 0;\n\tu16 result = L2CAP_MR_NOT_ALLOWED;\n\n\tif (cmd_len != sizeof(*req))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(req->icid);\n\n\tBT_DBG(\"icid 0x%4.4x, dest_amp_id %d\", icid, req->dest_amp_id);\n\n\tif (!(conn->local_fixed_chan & L2CAP_FC_A2MP))\n\t\treturn -EINVAL;\n\n\tchan = l2cap_get_chan_by_dcid(conn, icid);\n\tif (!chan) {\n\t\trsp.icid = cpu_to_le16(icid);\n\t\trsp.result = cpu_to_le16(L2CAP_MR_NOT_ALLOWED);\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_MOVE_CHAN_RSP,\n\t\t\t       sizeof(rsp), &rsp);\n\t\treturn 0;\n\t}\n\n\tchan->ident = cmd->ident;\n\n\tif (chan->scid < L2CAP_CID_DYN_START ||\n\t    chan->chan_policy == BT_CHANNEL_POLICY_BREDR_ONLY ||\n\t    (chan->mode != L2CAP_MODE_ERTM &&\n\t     chan->mode != L2CAP_MODE_STREAMING)) {\n\t\tresult = L2CAP_MR_NOT_ALLOWED;\n\t\tgoto send_move_response;\n\t}\n\n\tif (chan->local_amp_id == req->dest_amp_id) {\n\t\tresult = L2CAP_MR_SAME_ID;\n\t\tgoto send_move_response;\n\t}\n\n\tif (req->dest_amp_id != AMP_ID_BREDR) {\n\t\tstruct hci_dev *hdev;\n\t\thdev = hci_dev_get(req->dest_amp_id);\n\t\tif (!hdev || hdev->dev_type != HCI_AMP ||\n\t\t    !test_bit(HCI_UP, &hdev->flags)) {\n\t\t\tif (hdev)\n\t\t\t\thci_dev_put(hdev);\n\n\t\t\tresult = L2CAP_MR_BAD_ID;\n\t\t\tgoto send_move_response;\n\t\t}\n\t\thci_dev_put(hdev);\n\t}\n\n\t/* Detect a move collision.  Only send a collision response\n\t * if this side has \"lost\", otherwise proceed with the move.\n\t * The winner has the larger bd_addr.\n\t */\n\tif ((__chan_is_moving(chan) ||\n\t     chan->move_role != L2CAP_MOVE_ROLE_NONE) &&\n\t    bacmp(&conn->hcon->src, &conn->hcon->dst) > 0) {\n\t\tresult = L2CAP_MR_COLLISION;\n\t\tgoto send_move_response;\n\t}\n\n\tchan->move_role = L2CAP_MOVE_ROLE_RESPONDER;\n\tl2cap_move_setup(chan);\n\tchan->move_id = req->dest_amp_id;\n\n\tif (req->dest_amp_id == AMP_ID_BREDR) {\n\t\t/* Moving to BR/EDR */\n\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOCAL_BUSY;\n\t\t\tresult = L2CAP_MR_PEND;\n\t\t} else {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_CONFIRM;\n\t\t\tresult = L2CAP_MR_SUCCESS;\n\t\t}\n\t} else {\n\t\tchan->move_state = L2CAP_MOVE_WAIT_PREPARE;\n\t\t/* Placeholder - uncomment when amp functions are available */\n\t\t/*amp_accept_physical(chan, req->dest_amp_id);*/\n\t\tresult = L2CAP_MR_PEND;\n\t}\n\nsend_move_response:\n\tl2cap_send_move_chan_rsp(chan, result);\n\n\tl2cap_chan_unlock(chan);\n\n\treturn 0;\n}",
        "code_after_change": "static inline int l2cap_move_channel_req(struct l2cap_conn *conn,\n\t\t\t\t\t struct l2cap_cmd_hdr *cmd,\n\t\t\t\t\t u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_req *req = data;\n\tstruct l2cap_move_chan_rsp rsp;\n\tstruct l2cap_chan *chan;\n\tu16 icid = 0;\n\tu16 result = L2CAP_MR_NOT_ALLOWED;\n\n\tif (cmd_len != sizeof(*req))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(req->icid);\n\n\tBT_DBG(\"icid 0x%4.4x, dest_amp_id %d\", icid, req->dest_amp_id);\n\n\tif (!(conn->local_fixed_chan & L2CAP_FC_A2MP))\n\t\treturn -EINVAL;\n\n\tchan = l2cap_get_chan_by_dcid(conn, icid);\n\tif (!chan) {\n\t\trsp.icid = cpu_to_le16(icid);\n\t\trsp.result = cpu_to_le16(L2CAP_MR_NOT_ALLOWED);\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_MOVE_CHAN_RSP,\n\t\t\t       sizeof(rsp), &rsp);\n\t\treturn 0;\n\t}\n\n\tchan->ident = cmd->ident;\n\n\tif (chan->scid < L2CAP_CID_DYN_START ||\n\t    chan->chan_policy == BT_CHANNEL_POLICY_BREDR_ONLY ||\n\t    (chan->mode != L2CAP_MODE_ERTM &&\n\t     chan->mode != L2CAP_MODE_STREAMING)) {\n\t\tresult = L2CAP_MR_NOT_ALLOWED;\n\t\tgoto send_move_response;\n\t}\n\n\tif (chan->local_amp_id == req->dest_amp_id) {\n\t\tresult = L2CAP_MR_SAME_ID;\n\t\tgoto send_move_response;\n\t}\n\n\tif (req->dest_amp_id != AMP_ID_BREDR) {\n\t\tstruct hci_dev *hdev;\n\t\thdev = hci_dev_get(req->dest_amp_id);\n\t\tif (!hdev || hdev->dev_type != HCI_AMP ||\n\t\t    !test_bit(HCI_UP, &hdev->flags)) {\n\t\t\tif (hdev)\n\t\t\t\thci_dev_put(hdev);\n\n\t\t\tresult = L2CAP_MR_BAD_ID;\n\t\t\tgoto send_move_response;\n\t\t}\n\t\thci_dev_put(hdev);\n\t}\n\n\t/* Detect a move collision.  Only send a collision response\n\t * if this side has \"lost\", otherwise proceed with the move.\n\t * The winner has the larger bd_addr.\n\t */\n\tif ((__chan_is_moving(chan) ||\n\t     chan->move_role != L2CAP_MOVE_ROLE_NONE) &&\n\t    bacmp(&conn->hcon->src, &conn->hcon->dst) > 0) {\n\t\tresult = L2CAP_MR_COLLISION;\n\t\tgoto send_move_response;\n\t}\n\n\tchan->move_role = L2CAP_MOVE_ROLE_RESPONDER;\n\tl2cap_move_setup(chan);\n\tchan->move_id = req->dest_amp_id;\n\n\tif (req->dest_amp_id == AMP_ID_BREDR) {\n\t\t/* Moving to BR/EDR */\n\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOCAL_BUSY;\n\t\t\tresult = L2CAP_MR_PEND;\n\t\t} else {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_CONFIRM;\n\t\t\tresult = L2CAP_MR_SUCCESS;\n\t\t}\n\t} else {\n\t\tchan->move_state = L2CAP_MOVE_WAIT_PREPARE;\n\t\t/* Placeholder - uncomment when amp functions are available */\n\t\t/*amp_accept_physical(chan, req->dest_amp_id);*/\n\t\tresult = L2CAP_MR_PEND;\n\t}\n\nsend_move_response:\n\tl2cap_send_move_chan_rsp(chan, result);\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -91,6 +91,7 @@\n \tl2cap_send_move_chan_rsp(chan, result);\n \n \tl2cap_chan_unlock(chan);\n+\tl2cap_chan_put(chan);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tl2cap_chan_put(chan);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3391
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static struct l2cap_chan *l2cap_get_chan_by_scid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_scid(conn, cid);\n\tif (c)\n\t\tl2cap_chan_lock(c);\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
        "code_after_change": "static struct l2cap_chan *l2cap_get_chan_by_scid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_scid(conn, cid);\n\tif (c) {\n\t\t/* Only lock if chan reference is not 0 */\n\t\tc = l2cap_chan_hold_unless_zero(c);\n\t\tif (c)\n\t\t\tl2cap_chan_lock(c);\n\t}\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,12 @@\n \n \tmutex_lock(&conn->chan_lock);\n \tc = __l2cap_get_chan_by_scid(conn, cid);\n-\tif (c)\n-\t\tl2cap_chan_lock(c);\n+\tif (c) {\n+\t\t/* Only lock if chan reference is not 0 */\n+\t\tc = l2cap_chan_hold_unless_zero(c);\n+\t\tif (c)\n+\t\t\tl2cap_chan_lock(c);\n+\t}\n \tmutex_unlock(&conn->chan_lock);\n \n \treturn c;",
        "function_modified_lines": {
            "added": [
                "\tif (c) {",
                "\t\t/* Only lock if chan reference is not 0 */",
                "\t\tc = l2cap_chan_hold_unless_zero(c);",
                "\t\tif (c)",
                "\t\t\tl2cap_chan_lock(c);",
                "\t}"
            ],
            "deleted": [
                "\tif (c)",
                "\t\tl2cap_chan_lock(c);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3389
    },
    {
        "cve_id": "CVE-2018-16884",
        "code_before_change": "static int\nsvc_process_common(struct svc_rqst *rqstp, struct kvec *argv, struct kvec *resv)\n{\n\tstruct svc_program\t*progp;\n\tconst struct svc_version *versp = NULL;\t/* compiler food */\n\tconst struct svc_procedure *procp = NULL;\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\t__be32\t\t\t*statp;\n\tu32\t\t\tprog, vers, proc;\n\t__be32\t\t\tauth_stat, rpc_stat;\n\tint\t\t\tauth_res;\n\t__be32\t\t\t*reply_statp;\n\n\trpc_stat = rpc_success;\n\n\tif (argv->iov_len < 6*4)\n\t\tgoto err_short_len;\n\n\t/* Will be turned off by GSS integrity and privacy services */\n\tset_bit(RQ_SPLICE_OK, &rqstp->rq_flags);\n\t/* Will be turned off only when NFSv4 Sessions are used */\n\tset_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\tclear_bit(RQ_DROPME, &rqstp->rq_flags);\n\n\t/* Setup reply header */\n\trqstp->rq_xprt->xpt_ops->xpo_prep_reply_hdr(rqstp);\n\n\tsvc_putu32(resv, rqstp->rq_xid);\n\n\tvers = svc_getnl(argv);\n\n\t/* First words of reply: */\n\tsvc_putnl(resv, 1);\t\t/* REPLY */\n\n\tif (vers != 2)\t\t/* RPC version number */\n\t\tgoto err_bad_rpc;\n\n\t/* Save position in case we later decide to reject: */\n\treply_statp = resv->iov_base + resv->iov_len;\n\n\tsvc_putnl(resv, 0);\t\t/* ACCEPT */\n\n\trqstp->rq_prog = prog = svc_getnl(argv);\t/* program number */\n\trqstp->rq_vers = vers = svc_getnl(argv);\t/* version number */\n\trqstp->rq_proc = proc = svc_getnl(argv);\t/* procedure number */\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next)\n\t\tif (prog == progp->pg_prog)\n\t\t\tbreak;\n\n\t/*\n\t * Decode auth data, and add verifier to reply buffer.\n\t * We do this before anything else in order to get a decent\n\t * auth verifier.\n\t */\n\tauth_res = svc_authenticate(rqstp, &auth_stat);\n\t/* Also give the program a chance to reject this call: */\n\tif (auth_res == SVC_OK && progp) {\n\t\tauth_stat = rpc_autherr_badcred;\n\t\tauth_res = progp->pg_authenticate(rqstp);\n\t}\n\tswitch (auth_res) {\n\tcase SVC_OK:\n\t\tbreak;\n\tcase SVC_GARBAGE:\n\t\tgoto err_garbage;\n\tcase SVC_SYSERR:\n\t\trpc_stat = rpc_system_err;\n\t\tgoto err_bad;\n\tcase SVC_DENIED:\n\t\tgoto err_bad_auth;\n\tcase SVC_CLOSE:\n\t\tgoto close;\n\tcase SVC_DROP:\n\t\tgoto dropit;\n\tcase SVC_COMPLETE:\n\t\tgoto sendit;\n\t}\n\n\tif (progp == NULL)\n\t\tgoto err_bad_prog;\n\n\tif (vers >= progp->pg_nvers ||\n\t  !(versp = progp->pg_vers[vers]))\n\t\tgoto err_bad_vers;\n\n\t/*\n\t * Some protocol versions (namely NFSv4) require some form of\n\t * congestion control.  (See RFC 7530 section 3.1 paragraph 2)\n\t * In other words, UDP is not allowed. We mark those when setting\n\t * up the svc_xprt, and verify that here.\n\t *\n\t * The spec is not very clear about what error should be returned\n\t * when someone tries to access a server that is listening on UDP\n\t * for lower versions. RPC_PROG_MISMATCH seems to be the closest\n\t * fit.\n\t */\n\tif (versp->vs_need_cong_ctrl &&\n\t    !test_bit(XPT_CONG_CTRL, &rqstp->rq_xprt->xpt_flags))\n\t\tgoto err_bad_vers;\n\n\tprocp = versp->vs_proc + proc;\n\tif (proc >= versp->vs_nproc || !procp->pc_func)\n\t\tgoto err_bad_proc;\n\trqstp->rq_procinfo = procp;\n\n\t/* Syntactic check complete */\n\tserv->sv_stats->rpccnt++;\n\ttrace_svc_process(rqstp, progp->pg_name);\n\n\t/* Build the reply header. */\n\tstatp = resv->iov_base +resv->iov_len;\n\tsvc_putnl(resv, RPC_SUCCESS);\n\n\t/* Bump per-procedure stats counter */\n\tversp->vs_count[proc]++;\n\n\t/* Initialize storage for argp and resp */\n\tmemset(rqstp->rq_argp, 0, procp->pc_argsize);\n\tmemset(rqstp->rq_resp, 0, procp->pc_ressize);\n\n\t/* un-reserve some of the out-queue now that we have a\n\t * better idea of reply size\n\t */\n\tif (procp->pc_xdrressize)\n\t\tsvc_reserve_auth(rqstp, procp->pc_xdrressize<<2);\n\n\t/* Call the function that processes the request. */\n\tif (!versp->vs_dispatch) {\n\t\t/*\n\t\t * Decode arguments\n\t\t * XXX: why do we ignore the return value?\n\t\t */\n\t\tif (procp->pc_decode &&\n\t\t    !procp->pc_decode(rqstp, argv->iov_base))\n\t\t\tgoto err_garbage;\n\n\t\t*statp = procp->pc_func(rqstp);\n\n\t\t/* Encode reply */\n\t\tif (*statp == rpc_drop_reply ||\n\t\t    test_bit(RQ_DROPME, &rqstp->rq_flags)) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto dropit;\n\t\t}\n\t\tif (*statp == rpc_autherr_badcred) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto err_bad_auth;\n\t\t}\n\t\tif (*statp == rpc_success && procp->pc_encode &&\n\t\t    !procp->pc_encode(rqstp, resv->iov_base + resv->iov_len)) {\n\t\t\tdprintk(\"svc: failed to encode reply\\n\");\n\t\t\t/* serv->sv_stats->rpcsystemerr++; */\n\t\t\t*statp = rpc_system_err;\n\t\t}\n\t} else {\n\t\tdprintk(\"svc: calling dispatcher\\n\");\n\t\tif (!versp->vs_dispatch(rqstp, statp)) {\n\t\t\t/* Release reply info */\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto dropit;\n\t\t}\n\t}\n\n\t/* Check RPC status result */\n\tif (*statp != rpc_success)\n\t\tresv->iov_len = ((void*)statp)  - resv->iov_base + 4;\n\n\t/* Release reply info */\n\tif (procp->pc_release)\n\t\tprocp->pc_release(rqstp);\n\n\tif (procp->pc_encode == NULL)\n\t\tgoto dropit;\n\n sendit:\n\tif (svc_authorise(rqstp))\n\t\tgoto close;\n\treturn 1;\t\t/* Caller can now send it */\n\n dropit:\n\tsvc_authorise(rqstp);\t/* doesn't hurt to call this twice */\n\tdprintk(\"svc: svc_process dropit\\n\");\n\treturn 0;\n\n close:\n\tif (test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n\t\tsvc_close_xprt(rqstp->rq_xprt);\n\tdprintk(\"svc: svc_process close\\n\");\n\treturn 0;\n\nerr_short_len:\n\tsvc_printk(rqstp, \"short len %zd, dropping request\\n\",\n\t\t\targv->iov_len);\n\tgoto close;\n\nerr_bad_rpc:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 0);\t/* RPC_MISMATCH */\n\tsvc_putnl(resv, 2);\t/* Only RPCv2 supported */\n\tsvc_putnl(resv, 2);\n\tgoto sendit;\n\nerr_bad_auth:\n\tdprintk(\"svc: authentication failed (%d)\\n\", ntohl(auth_stat));\n\tserv->sv_stats->rpcbadauth++;\n\t/* Restore write pointer to location of accept status: */\n\txdr_ressize_check(rqstp, reply_statp);\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 1);\t/* AUTH_ERROR */\n\tsvc_putnl(resv, ntohl(auth_stat));\t/* status */\n\tgoto sendit;\n\nerr_bad_prog:\n\tdprintk(\"svc: unknown program %d\\n\", prog);\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_UNAVAIL);\n\tgoto sendit;\n\nerr_bad_vers:\n\tsvc_printk(rqstp, \"unknown version (%d for prog %d, %s)\\n\",\n\t\t       vers, prog, progp->pg_name);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_MISMATCH);\n\tsvc_putnl(resv, progp->pg_lovers);\n\tsvc_putnl(resv, progp->pg_hivers);\n\tgoto sendit;\n\nerr_bad_proc:\n\tsvc_printk(rqstp, \"unknown procedure (%d)\\n\", proc);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROC_UNAVAIL);\n\tgoto sendit;\n\nerr_garbage:\n\tsvc_printk(rqstp, \"failed to decode args\\n\");\n\n\trpc_stat = rpc_garbage_args;\nerr_bad:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, ntohl(rpc_stat));\n\tgoto sendit;\n}",
        "code_after_change": "static int\nsvc_process_common(struct svc_rqst *rqstp, struct kvec *argv, struct kvec *resv)\n{\n\tstruct svc_program\t*progp;\n\tconst struct svc_version *versp = NULL;\t/* compiler food */\n\tconst struct svc_procedure *procp = NULL;\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\t__be32\t\t\t*statp;\n\tu32\t\t\tprog, vers, proc;\n\t__be32\t\t\tauth_stat, rpc_stat;\n\tint\t\t\tauth_res;\n\t__be32\t\t\t*reply_statp;\n\n\trpc_stat = rpc_success;\n\n\tif (argv->iov_len < 6*4)\n\t\tgoto err_short_len;\n\n\t/* Will be turned off by GSS integrity and privacy services */\n\tset_bit(RQ_SPLICE_OK, &rqstp->rq_flags);\n\t/* Will be turned off only when NFSv4 Sessions are used */\n\tset_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\tclear_bit(RQ_DROPME, &rqstp->rq_flags);\n\n\t/* Setup reply header */\n\tif (rqstp->rq_prot == IPPROTO_TCP)\n\t\tsvc_tcp_prep_reply_hdr(rqstp);\n\n\tsvc_putu32(resv, rqstp->rq_xid);\n\n\tvers = svc_getnl(argv);\n\n\t/* First words of reply: */\n\tsvc_putnl(resv, 1);\t\t/* REPLY */\n\n\tif (vers != 2)\t\t/* RPC version number */\n\t\tgoto err_bad_rpc;\n\n\t/* Save position in case we later decide to reject: */\n\treply_statp = resv->iov_base + resv->iov_len;\n\n\tsvc_putnl(resv, 0);\t\t/* ACCEPT */\n\n\trqstp->rq_prog = prog = svc_getnl(argv);\t/* program number */\n\trqstp->rq_vers = vers = svc_getnl(argv);\t/* version number */\n\trqstp->rq_proc = proc = svc_getnl(argv);\t/* procedure number */\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next)\n\t\tif (prog == progp->pg_prog)\n\t\t\tbreak;\n\n\t/*\n\t * Decode auth data, and add verifier to reply buffer.\n\t * We do this before anything else in order to get a decent\n\t * auth verifier.\n\t */\n\tauth_res = svc_authenticate(rqstp, &auth_stat);\n\t/* Also give the program a chance to reject this call: */\n\tif (auth_res == SVC_OK && progp) {\n\t\tauth_stat = rpc_autherr_badcred;\n\t\tauth_res = progp->pg_authenticate(rqstp);\n\t}\n\tswitch (auth_res) {\n\tcase SVC_OK:\n\t\tbreak;\n\tcase SVC_GARBAGE:\n\t\tgoto err_garbage;\n\tcase SVC_SYSERR:\n\t\trpc_stat = rpc_system_err;\n\t\tgoto err_bad;\n\tcase SVC_DENIED:\n\t\tgoto err_bad_auth;\n\tcase SVC_CLOSE:\n\t\tgoto close;\n\tcase SVC_DROP:\n\t\tgoto dropit;\n\tcase SVC_COMPLETE:\n\t\tgoto sendit;\n\t}\n\n\tif (progp == NULL)\n\t\tgoto err_bad_prog;\n\n\tif (vers >= progp->pg_nvers ||\n\t  !(versp = progp->pg_vers[vers]))\n\t\tgoto err_bad_vers;\n\n\t/*\n\t * Some protocol versions (namely NFSv4) require some form of\n\t * congestion control.  (See RFC 7530 section 3.1 paragraph 2)\n\t * In other words, UDP is not allowed. We mark those when setting\n\t * up the svc_xprt, and verify that here.\n\t *\n\t * The spec is not very clear about what error should be returned\n\t * when someone tries to access a server that is listening on UDP\n\t * for lower versions. RPC_PROG_MISMATCH seems to be the closest\n\t * fit.\n\t */\n\tif (versp->vs_need_cong_ctrl && rqstp->rq_xprt &&\n\t    !test_bit(XPT_CONG_CTRL, &rqstp->rq_xprt->xpt_flags))\n\t\tgoto err_bad_vers;\n\n\tprocp = versp->vs_proc + proc;\n\tif (proc >= versp->vs_nproc || !procp->pc_func)\n\t\tgoto err_bad_proc;\n\trqstp->rq_procinfo = procp;\n\n\t/* Syntactic check complete */\n\tserv->sv_stats->rpccnt++;\n\ttrace_svc_process(rqstp, progp->pg_name);\n\n\t/* Build the reply header. */\n\tstatp = resv->iov_base +resv->iov_len;\n\tsvc_putnl(resv, RPC_SUCCESS);\n\n\t/* Bump per-procedure stats counter */\n\tversp->vs_count[proc]++;\n\n\t/* Initialize storage for argp and resp */\n\tmemset(rqstp->rq_argp, 0, procp->pc_argsize);\n\tmemset(rqstp->rq_resp, 0, procp->pc_ressize);\n\n\t/* un-reserve some of the out-queue now that we have a\n\t * better idea of reply size\n\t */\n\tif (procp->pc_xdrressize)\n\t\tsvc_reserve_auth(rqstp, procp->pc_xdrressize<<2);\n\n\t/* Call the function that processes the request. */\n\tif (!versp->vs_dispatch) {\n\t\t/*\n\t\t * Decode arguments\n\t\t * XXX: why do we ignore the return value?\n\t\t */\n\t\tif (procp->pc_decode &&\n\t\t    !procp->pc_decode(rqstp, argv->iov_base))\n\t\t\tgoto err_garbage;\n\n\t\t*statp = procp->pc_func(rqstp);\n\n\t\t/* Encode reply */\n\t\tif (*statp == rpc_drop_reply ||\n\t\t    test_bit(RQ_DROPME, &rqstp->rq_flags)) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto dropit;\n\t\t}\n\t\tif (*statp == rpc_autherr_badcred) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto err_bad_auth;\n\t\t}\n\t\tif (*statp == rpc_success && procp->pc_encode &&\n\t\t    !procp->pc_encode(rqstp, resv->iov_base + resv->iov_len)) {\n\t\t\tdprintk(\"svc: failed to encode reply\\n\");\n\t\t\t/* serv->sv_stats->rpcsystemerr++; */\n\t\t\t*statp = rpc_system_err;\n\t\t}\n\t} else {\n\t\tdprintk(\"svc: calling dispatcher\\n\");\n\t\tif (!versp->vs_dispatch(rqstp, statp)) {\n\t\t\t/* Release reply info */\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto dropit;\n\t\t}\n\t}\n\n\t/* Check RPC status result */\n\tif (*statp != rpc_success)\n\t\tresv->iov_len = ((void*)statp)  - resv->iov_base + 4;\n\n\t/* Release reply info */\n\tif (procp->pc_release)\n\t\tprocp->pc_release(rqstp);\n\n\tif (procp->pc_encode == NULL)\n\t\tgoto dropit;\n\n sendit:\n\tif (svc_authorise(rqstp))\n\t\tgoto close;\n\treturn 1;\t\t/* Caller can now send it */\n\n dropit:\n\tsvc_authorise(rqstp);\t/* doesn't hurt to call this twice */\n\tdprintk(\"svc: svc_process dropit\\n\");\n\treturn 0;\n\n close:\n\tif (rqstp->rq_xprt && test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n\t\tsvc_close_xprt(rqstp->rq_xprt);\n\tdprintk(\"svc: svc_process close\\n\");\n\treturn 0;\n\nerr_short_len:\n\tsvc_printk(rqstp, \"short len %zd, dropping request\\n\",\n\t\t\targv->iov_len);\n\tgoto close;\n\nerr_bad_rpc:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 0);\t/* RPC_MISMATCH */\n\tsvc_putnl(resv, 2);\t/* Only RPCv2 supported */\n\tsvc_putnl(resv, 2);\n\tgoto sendit;\n\nerr_bad_auth:\n\tdprintk(\"svc: authentication failed (%d)\\n\", ntohl(auth_stat));\n\tserv->sv_stats->rpcbadauth++;\n\t/* Restore write pointer to location of accept status: */\n\txdr_ressize_check(rqstp, reply_statp);\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 1);\t/* AUTH_ERROR */\n\tsvc_putnl(resv, ntohl(auth_stat));\t/* status */\n\tgoto sendit;\n\nerr_bad_prog:\n\tdprintk(\"svc: unknown program %d\\n\", prog);\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_UNAVAIL);\n\tgoto sendit;\n\nerr_bad_vers:\n\tsvc_printk(rqstp, \"unknown version (%d for prog %d, %s)\\n\",\n\t\t       vers, prog, progp->pg_name);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_MISMATCH);\n\tsvc_putnl(resv, progp->pg_lovers);\n\tsvc_putnl(resv, progp->pg_hivers);\n\tgoto sendit;\n\nerr_bad_proc:\n\tsvc_printk(rqstp, \"unknown procedure (%d)\\n\", proc);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROC_UNAVAIL);\n\tgoto sendit;\n\nerr_garbage:\n\tsvc_printk(rqstp, \"failed to decode args\\n\");\n\n\trpc_stat = rpc_garbage_args;\nerr_bad:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, ntohl(rpc_stat));\n\tgoto sendit;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,8 @@\n \tclear_bit(RQ_DROPME, &rqstp->rq_flags);\n \n \t/* Setup reply header */\n-\trqstp->rq_xprt->xpt_ops->xpo_prep_reply_hdr(rqstp);\n+\tif (rqstp->rq_prot == IPPROTO_TCP)\n+\t\tsvc_tcp_prep_reply_hdr(rqstp);\n \n \tsvc_putu32(resv, rqstp->rq_xid);\n \n@@ -95,7 +96,7 @@\n \t * for lower versions. RPC_PROG_MISMATCH seems to be the closest\n \t * fit.\n \t */\n-\tif (versp->vs_need_cong_ctrl &&\n+\tif (versp->vs_need_cong_ctrl && rqstp->rq_xprt &&\n \t    !test_bit(XPT_CONG_CTRL, &rqstp->rq_xprt->xpt_flags))\n \t\tgoto err_bad_vers;\n \n@@ -187,7 +188,7 @@\n \treturn 0;\n \n  close:\n-\tif (test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n+\tif (rqstp->rq_xprt && test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n \t\tsvc_close_xprt(rqstp->rq_xprt);\n \tdprintk(\"svc: svc_process close\\n\");\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tif (rqstp->rq_prot == IPPROTO_TCP)",
                "\t\tsvc_tcp_prep_reply_hdr(rqstp);",
                "\tif (versp->vs_need_cong_ctrl && rqstp->rq_xprt &&",
                "\tif (rqstp->rq_xprt && test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))"
            ],
            "deleted": [
                "\trqstp->rq_xprt->xpt_ops->xpo_prep_reply_hdr(rqstp);",
                "\tif (versp->vs_need_cong_ctrl &&",
                "\tif (test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's NFS41+ subsystem. NFS41+ shares mounted in different network namespaces at the same time can make bc_svc_process() use wrong back-channel IDs and cause a use-after-free vulnerability. Thus a malicious container user can cause a host kernel memory corruption and a system panic. Due to the nature of the flaw, privilege escalation cannot be fully ruled out.",
        "id": 1722
    },
    {
        "cve_id": "CVE-2021-39800",
        "code_before_change": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\tstruct ion_handle *handle;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_buffer *buffer = NULL;\n\tstruct ion_heap *heap;\n\tint ret;\n\n\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n\t\t len, align, heap_id_mask, flags);\n\t/*\n\t * traverse the list of heaps available in this system in priority\n\t * order.  If the heap type is supported by the client, and matches the\n\t * request of the caller allocate from it.  Repeat until allocate has\n\t * succeeded or all heaps have been tried\n\t */\n\tlen = PAGE_ALIGN(len);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&dev->lock);\n\tplist_for_each_entry(heap, &dev->heaps, node) {\n\t\t/* if the caller didn't specify this heap id */\n\t\tif (!((1 << heap->id) & heap_id_mask))\n\t\t\tcontinue;\n\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n\t\tif (!IS_ERR(buffer))\n\t\t\tbreak;\n\t}\n\tup_read(&dev->lock);\n\n\tif (buffer == NULL)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (IS_ERR(buffer))\n\t\treturn ERR_CAST(buffer);\n\n\thandle = ion_handle_create(client, buffer);\n\n\t/*\n\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n\t * and ion_handle_create will take a second reference, drop one here\n\t */\n\tion_buffer_put(buffer);\n\n\tif (IS_ERR(handle))\n\t\treturn handle;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\n\treturn handle;\n}",
        "code_after_change": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,60 +2,5 @@\n \t\t\t     size_t align, unsigned int heap_id_mask,\n \t\t\t     unsigned int flags)\n {\n-\tstruct ion_handle *handle;\n-\tstruct ion_device *dev = client->dev;\n-\tstruct ion_buffer *buffer = NULL;\n-\tstruct ion_heap *heap;\n-\tint ret;\n-\n-\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n-\t\t len, align, heap_id_mask, flags);\n-\t/*\n-\t * traverse the list of heaps available in this system in priority\n-\t * order.  If the heap type is supported by the client, and matches the\n-\t * request of the caller allocate from it.  Repeat until allocate has\n-\t * succeeded or all heaps have been tried\n-\t */\n-\tlen = PAGE_ALIGN(len);\n-\n-\tif (!len)\n-\t\treturn ERR_PTR(-EINVAL);\n-\n-\tdown_read(&dev->lock);\n-\tplist_for_each_entry(heap, &dev->heaps, node) {\n-\t\t/* if the caller didn't specify this heap id */\n-\t\tif (!((1 << heap->id) & heap_id_mask))\n-\t\t\tcontinue;\n-\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n-\t\tif (!IS_ERR(buffer))\n-\t\t\tbreak;\n-\t}\n-\tup_read(&dev->lock);\n-\n-\tif (buffer == NULL)\n-\t\treturn ERR_PTR(-ENODEV);\n-\n-\tif (IS_ERR(buffer))\n-\t\treturn ERR_CAST(buffer);\n-\n-\thandle = ion_handle_create(client, buffer);\n-\n-\t/*\n-\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n-\t * and ion_handle_create will take a second reference, drop one here\n-\t */\n-\tion_buffer_put(buffer);\n-\n-\tif (IS_ERR(handle))\n-\t\treturn handle;\n-\n-\tmutex_lock(&client->lock);\n-\tret = ion_handle_add(client, handle);\n-\tmutex_unlock(&client->lock);\n-\tif (ret) {\n-\t\tion_handle_put(handle);\n-\t\thandle = ERR_PTR(ret);\n-\t}\n-\n-\treturn handle;\n+\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);"
            ],
            "deleted": [
                "\tstruct ion_handle *handle;",
                "\tstruct ion_device *dev = client->dev;",
                "\tstruct ion_buffer *buffer = NULL;",
                "\tstruct ion_heap *heap;",
                "\tint ret;",
                "",
                "\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,",
                "\t\t len, align, heap_id_mask, flags);",
                "\t/*",
                "\t * traverse the list of heaps available in this system in priority",
                "\t * order.  If the heap type is supported by the client, and matches the",
                "\t * request of the caller allocate from it.  Repeat until allocate has",
                "\t * succeeded or all heaps have been tried",
                "\t */",
                "\tlen = PAGE_ALIGN(len);",
                "",
                "\tif (!len)",
                "\t\treturn ERR_PTR(-EINVAL);",
                "",
                "\tdown_read(&dev->lock);",
                "\tplist_for_each_entry(heap, &dev->heaps, node) {",
                "\t\t/* if the caller didn't specify this heap id */",
                "\t\tif (!((1 << heap->id) & heap_id_mask))",
                "\t\t\tcontinue;",
                "\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);",
                "\t\tif (!IS_ERR(buffer))",
                "\t\t\tbreak;",
                "\t}",
                "\tup_read(&dev->lock);",
                "",
                "\tif (buffer == NULL)",
                "\t\treturn ERR_PTR(-ENODEV);",
                "",
                "\tif (IS_ERR(buffer))",
                "\t\treturn ERR_CAST(buffer);",
                "",
                "\thandle = ion_handle_create(client, buffer);",
                "",
                "\t/*",
                "\t * ion_buffer_create will create a buffer with a ref_cnt of 1,",
                "\t * and ion_handle_create will take a second reference, drop one here",
                "\t */",
                "\tion_buffer_put(buffer);",
                "",
                "\tif (IS_ERR(handle))",
                "\t\treturn handle;",
                "",
                "\tmutex_lock(&client->lock);",
                "\tret = ion_handle_add(client, handle);",
                "\tmutex_unlock(&client->lock);",
                "\tif (ret) {",
                "\t\tion_handle_put(handle);",
                "\t\thandle = ERR_PTR(ret);",
                "\t}",
                "",
                "\treturn handle;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In ion_ioctl of ion-ioctl.c, there is a possible way to leak kernel head data due to a use after free. This could lead to local information disclosure with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-208277166References: Upstream kernel",
        "id": 3109
    },
    {
        "cve_id": "CVE-2022-3977",
        "code_before_change": "static int mctp_ioctl_alloctag(struct mctp_sock *msk, unsigned long arg)\n{\n\tstruct net *net = sock_net(&msk->sk);\n\tstruct mctp_sk_key *key = NULL;\n\tstruct mctp_ioc_tag_ctl ctl;\n\tunsigned long flags;\n\tu8 tag;\n\n\tif (copy_from_user(&ctl, (void __user *)arg, sizeof(ctl)))\n\t\treturn -EFAULT;\n\n\tif (ctl.tag)\n\t\treturn -EINVAL;\n\n\tif (ctl.flags)\n\t\treturn -EINVAL;\n\n\tkey = mctp_alloc_local_tag(msk, ctl.peer_addr, MCTP_ADDR_ANY,\n\t\t\t\t   true, &tag);\n\tif (IS_ERR(key))\n\t\treturn PTR_ERR(key);\n\n\tctl.tag = tag | MCTP_TAG_OWNER | MCTP_TAG_PREALLOC;\n\tif (copy_to_user((void __user *)arg, &ctl, sizeof(ctl))) {\n\t\tspin_lock_irqsave(&key->lock, flags);\n\t\t__mctp_key_remove(key, net, flags, MCTP_TRACE_KEY_DROPPED);\n\t\tmctp_key_unref(key);\n\t\treturn -EFAULT;\n\t}\n\n\tmctp_key_unref(key);\n\treturn 0;\n}",
        "code_after_change": "static int mctp_ioctl_alloctag(struct mctp_sock *msk, unsigned long arg)\n{\n\tstruct net *net = sock_net(&msk->sk);\n\tstruct mctp_sk_key *key = NULL;\n\tstruct mctp_ioc_tag_ctl ctl;\n\tunsigned long flags;\n\tu8 tag;\n\n\tif (copy_from_user(&ctl, (void __user *)arg, sizeof(ctl)))\n\t\treturn -EFAULT;\n\n\tif (ctl.tag)\n\t\treturn -EINVAL;\n\n\tif (ctl.flags)\n\t\treturn -EINVAL;\n\n\tkey = mctp_alloc_local_tag(msk, ctl.peer_addr, MCTP_ADDR_ANY,\n\t\t\t\t   true, &tag);\n\tif (IS_ERR(key))\n\t\treturn PTR_ERR(key);\n\n\tctl.tag = tag | MCTP_TAG_OWNER | MCTP_TAG_PREALLOC;\n\tif (copy_to_user((void __user *)arg, &ctl, sizeof(ctl))) {\n\t\tunsigned long fl2;\n\t\t/* Unwind our key allocation: the keys list lock needs to be\n\t\t * taken before the individual key locks, and we need a valid\n\t\t * flags value (fl2) to pass to __mctp_key_remove, hence the\n\t\t * second spin_lock_irqsave() rather than a plain spin_lock().\n\t\t */\n\t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);\n\t\tspin_lock_irqsave(&key->lock, fl2);\n\t\t__mctp_key_remove(key, net, fl2, MCTP_TRACE_KEY_DROPPED);\n\t\tmctp_key_unref(key);\n\t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);\n\t\treturn -EFAULT;\n\t}\n\n\tmctp_key_unref(key);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,9 +22,17 @@\n \n \tctl.tag = tag | MCTP_TAG_OWNER | MCTP_TAG_PREALLOC;\n \tif (copy_to_user((void __user *)arg, &ctl, sizeof(ctl))) {\n-\t\tspin_lock_irqsave(&key->lock, flags);\n-\t\t__mctp_key_remove(key, net, flags, MCTP_TRACE_KEY_DROPPED);\n+\t\tunsigned long fl2;\n+\t\t/* Unwind our key allocation: the keys list lock needs to be\n+\t\t * taken before the individual key locks, and we need a valid\n+\t\t * flags value (fl2) to pass to __mctp_key_remove, hence the\n+\t\t * second spin_lock_irqsave() rather than a plain spin_lock().\n+\t\t */\n+\t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);\n+\t\tspin_lock_irqsave(&key->lock, fl2);\n+\t\t__mctp_key_remove(key, net, fl2, MCTP_TRACE_KEY_DROPPED);\n \t\tmctp_key_unref(key);\n+\t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);\n \t\treturn -EFAULT;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tunsigned long fl2;",
                "\t\t/* Unwind our key allocation: the keys list lock needs to be",
                "\t\t * taken before the individual key locks, and we need a valid",
                "\t\t * flags value (fl2) to pass to __mctp_key_remove, hence the",
                "\t\t * second spin_lock_irqsave() rather than a plain spin_lock().",
                "\t\t */",
                "\t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);",
                "\t\tspin_lock_irqsave(&key->lock, fl2);",
                "\t\t__mctp_key_remove(key, net, fl2, MCTP_TRACE_KEY_DROPPED);",
                "\t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);"
            ],
            "deleted": [
                "\t\tspin_lock_irqsave(&key->lock, flags);",
                "\t\t__mctp_key_remove(key, net, flags, MCTP_TRACE_KEY_DROPPED);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel MCTP (Management Component Transport Protocol) functionality. This issue occurs when a user simultaneously calls DROPTAG ioctl and socket close happens, which could allow a local user to crash the system or potentially escalate their privileges on the system.",
        "id": 3699
    },
    {
        "cve_id": "CVE-2020-27786",
        "code_before_change": "static int resize_runtime_buffer(struct snd_rawmidi_runtime *runtime,\n\t\t\t\t struct snd_rawmidi_params *params,\n\t\t\t\t bool is_input)\n{\n\tchar *newbuf, *oldbuf;\n\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L)\n\t\treturn -EINVAL;\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size)\n\t\treturn -EINVAL;\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = kvzalloc(params->buffer_size, GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&runtime->lock);\n\t\toldbuf = runtime->buffer;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\t__reset_runtime_ptrs(runtime, is_input);\n\t\tspin_unlock_irq(&runtime->lock);\n\t\tkvfree(oldbuf);\n\t}\n\truntime->avail_min = params->avail_min;\n\treturn 0;\n}",
        "code_after_change": "static int resize_runtime_buffer(struct snd_rawmidi_runtime *runtime,\n\t\t\t\t struct snd_rawmidi_params *params,\n\t\t\t\t bool is_input)\n{\n\tchar *newbuf, *oldbuf;\n\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L)\n\t\treturn -EINVAL;\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size)\n\t\treturn -EINVAL;\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = kvzalloc(params->buffer_size, GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&runtime->lock);\n\t\tif (runtime->buffer_ref) {\n\t\t\tspin_unlock_irq(&runtime->lock);\n\t\t\tkvfree(newbuf);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\toldbuf = runtime->buffer;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\t__reset_runtime_ptrs(runtime, is_input);\n\t\tspin_unlock_irq(&runtime->lock);\n\t\tkvfree(oldbuf);\n\t}\n\truntime->avail_min = params->avail_min;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,11 @@\n \t\tif (!newbuf)\n \t\t\treturn -ENOMEM;\n \t\tspin_lock_irq(&runtime->lock);\n+\t\tif (runtime->buffer_ref) {\n+\t\t\tspin_unlock_irq(&runtime->lock);\n+\t\t\tkvfree(newbuf);\n+\t\t\treturn -EBUSY;\n+\t\t}\n \t\toldbuf = runtime->buffer;\n \t\truntime->buffer = newbuf;\n \t\truntime->buffer_size = params->buffer_size;",
        "function_modified_lines": {
            "added": [
                "\t\tif (runtime->buffer_ref) {",
                "\t\t\tspin_unlock_irq(&runtime->lock);",
                "\t\t\tkvfree(newbuf);",
                "\t\t\treturn -EBUSY;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel\u2019s implementation of MIDI, where an attacker with a local account and the permissions to issue ioctl commands to midi devices could trigger a use-after-free issue. A write to this specific memory while freed and before use causes the flow of execution to change and possibly allow for memory corruption or privilege escalation. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 2635
    },
    {
        "cve_id": "CVE-2023-45898",
        "code_before_change": "void ext4_es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len)\n{\n\text4_lblk_t end;\n\tint err = 0;\n\tint reserved = 0;\n\tstruct extent_status *es = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_remove_extent(inode, lblk, len);\n\tes_debug(\"remove [%u/%u) from extent status tree of inode %lu\\n\",\n\t\t lblk, len, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tend = lblk + len - 1;\n\tBUG_ON(end < lblk);\n\nretry:\n\tif (err && !es)\n\t\tes = __es_alloc_extent(true);\n\t/*\n\t * ext4_clear_inode() depends on us taking i_es_lock unconditionally\n\t * so that we are sure __es_shrink() is done with the inode before it\n\t * is reclaimed.\n\t */\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n\tif (es && !es->es_len)\n\t\t__es_free_extent(es);\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_da_release_space(inode, reserved);\n\treturn;\n}",
        "code_after_change": "void ext4_es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len)\n{\n\text4_lblk_t end;\n\tint err = 0;\n\tint reserved = 0;\n\tstruct extent_status *es = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_remove_extent(inode, lblk, len);\n\tes_debug(\"remove [%u/%u) from extent status tree of inode %lu\\n\",\n\t\t lblk, len, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tend = lblk + len - 1;\n\tBUG_ON(end < lblk);\n\nretry:\n\tif (err && !es)\n\t\tes = __es_alloc_extent(true);\n\t/*\n\t * ext4_clear_inode() depends on us taking i_es_lock unconditionally\n\t * so that we are sure __es_shrink() is done with the inode before it\n\t * is reclaimed.\n\t */\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es) {\n\t\tif (!es->es_len)\n\t\t\t__es_free_extent(es);\n\t\tes = NULL;\n\t}\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_da_release_space(inode, reserved);\n\treturn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,8 +29,12 @@\n \t */\n \twrite_lock(&EXT4_I(inode)->i_es_lock);\n \terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n-\tif (es && !es->es_len)\n-\t\t__es_free_extent(es);\n+\t/* Free preallocated extent if it didn't get used. */\n+\tif (es) {\n+\t\tif (!es->es_len)\n+\t\t\t__es_free_extent(es);\n+\t\tes = NULL;\n+\t}\n \twrite_unlock(&EXT4_I(inode)->i_es_lock);\n \tif (err)\n \t\tgoto retry;",
        "function_modified_lines": {
            "added": [
                "\t/* Free preallocated extent if it didn't get used. */",
                "\tif (es) {",
                "\t\tif (!es->es_len)",
                "\t\t\t__es_free_extent(es);",
                "\t\tes = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tif (es && !es->es_len)",
                "\t\t__es_free_extent(es);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 6.5.4 has an es1 use-after-free in fs/ext4/extents_status.c, related to ext4_es_insert_extent.",
        "id": 4228
    },
    {
        "cve_id": "CVE-2020-0433",
        "code_before_change": "void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,\n\t\tvoid *priv)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tstruct blk_mq_tags *tags = hctx->tags;\n\n\t\t/*\n\t\t * If not software queues are currently mapped to this\n\t\t * hardware queue, there's nothing to check\n\t\t */\n\t\tif (!blk_mq_hw_queue_mapped(hctx))\n\t\t\tcontinue;\n\n\t\tif (tags->nr_reserved_tags)\n\t\t\tbt_for_each(hctx, &tags->breserved_tags, fn, priv, true);\n\t\tbt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);\n\t}\n\n}",
        "code_after_change": "void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,\n\t\tvoid *priv)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\t/*\n\t * __blk_mq_update_nr_hw_queues will update the nr_hw_queues and\n\t * queue_hw_ctx after freeze the queue. So we could use q_usage_counter\n\t * to avoid race with it. __blk_mq_update_nr_hw_queues will users\n\t * synchronize_rcu to ensure all of the users go out of the critical\n\t * section below and see zeroed q_usage_counter.\n\t */\n\trcu_read_lock();\n\tif (percpu_ref_is_zero(&q->q_usage_counter)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tstruct blk_mq_tags *tags = hctx->tags;\n\n\t\t/*\n\t\t * If not software queues are currently mapped to this\n\t\t * hardware queue, there's nothing to check\n\t\t */\n\t\tif (!blk_mq_hw_queue_mapped(hctx))\n\t\t\tcontinue;\n\n\t\tif (tags->nr_reserved_tags)\n\t\t\tbt_for_each(hctx, &tags->breserved_tags, fn, priv, true);\n\t\tbt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);\n\t}\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,18 @@\n \tstruct blk_mq_hw_ctx *hctx;\n \tint i;\n \n+\t/*\n+\t * __blk_mq_update_nr_hw_queues will update the nr_hw_queues and\n+\t * queue_hw_ctx after freeze the queue. So we could use q_usage_counter\n+\t * to avoid race with it. __blk_mq_update_nr_hw_queues will users\n+\t * synchronize_rcu to ensure all of the users go out of the critical\n+\t * section below and see zeroed q_usage_counter.\n+\t */\n+\trcu_read_lock();\n+\tif (percpu_ref_is_zero(&q->q_usage_counter)) {\n+\t\trcu_read_unlock();\n+\t\treturn;\n+\t}\n \n \tqueue_for_each_hw_ctx(q, hctx, i) {\n \t\tstruct blk_mq_tags *tags = hctx->tags;\n@@ -19,5 +31,5 @@\n \t\t\tbt_for_each(hctx, &tags->breserved_tags, fn, priv, true);\n \t\tbt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);\n \t}\n-\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * __blk_mq_update_nr_hw_queues will update the nr_hw_queues and",
                "\t * queue_hw_ctx after freeze the queue. So we could use q_usage_counter",
                "\t * to avoid race with it. __blk_mq_update_nr_hw_queues will users",
                "\t * synchronize_rcu to ensure all of the users go out of the critical",
                "\t * section below and see zeroed q_usage_counter.",
                "\t */",
                "\trcu_read_lock();",
                "\tif (percpu_ref_is_zero(&q->q_usage_counter)) {",
                "\t\trcu_read_unlock();",
                "\t\treturn;",
                "\t}",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In blk_mq_queue_tag_busy_iter of blk-mq-tag.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-151939299",
        "id": 2387
    },
    {
        "cve_id": "CVE-2023-1249",
        "code_before_change": "static bool dump_vma_snapshot(struct coredump_params *cprm)\n{\n\tstruct vm_area_struct *vma, *gate_vma;\n\tstruct mm_struct *mm = current->mm;\n\tint i;\n\n\t/*\n\t * Once the stack expansion code is fixed to not change VMA bounds\n\t * under mmap_lock in read mode, this can be changed to take the\n\t * mmap_lock in read mode.\n\t */\n\tif (mmap_write_lock_killable(mm))\n\t\treturn false;\n\n\tcprm->vma_data_size = 0;\n\tgate_vma = get_gate_vma(mm);\n\tcprm->vma_count = mm->map_count + (gate_vma ? 1 : 0);\n\n\tcprm->vma_meta = kvmalloc_array(cprm->vma_count, sizeof(*cprm->vma_meta), GFP_KERNEL);\n\tif (!cprm->vma_meta) {\n\t\tmmap_write_unlock(mm);\n\t\treturn false;\n\t}\n\n\tfor (i = 0, vma = first_vma(current, gate_vma); vma != NULL;\n\t\t\tvma = next_vma(vma, gate_vma), i++) {\n\t\tstruct core_vma_metadata *m = cprm->vma_meta + i;\n\n\t\tm->start = vma->vm_start;\n\t\tm->end = vma->vm_end;\n\t\tm->flags = vma->vm_flags;\n\t\tm->dump_size = vma_dump_size(vma, cprm->mm_flags);\n\t}\n\n\tmmap_write_unlock(mm);\n\n\tfor (i = 0; i < cprm->vma_count; i++) {\n\t\tstruct core_vma_metadata *m = cprm->vma_meta + i;\n\n\t\tif (m->dump_size == DUMP_SIZE_MAYBE_ELFHDR_PLACEHOLDER) {\n\t\t\tchar elfmag[SELFMAG];\n\n\t\t\tif (copy_from_user(elfmag, (void __user *)m->start, SELFMAG) ||\n\t\t\t\t\tmemcmp(elfmag, ELFMAG, SELFMAG) != 0) {\n\t\t\t\tm->dump_size = 0;\n\t\t\t} else {\n\t\t\t\tm->dump_size = PAGE_SIZE;\n\t\t\t}\n\t\t}\n\n\t\tcprm->vma_data_size += m->dump_size;\n\t}\n\n\treturn true;\n}",
        "code_after_change": "static bool dump_vma_snapshot(struct coredump_params *cprm)\n{\n\tstruct vm_area_struct *vma, *gate_vma;\n\tstruct mm_struct *mm = current->mm;\n\tint i;\n\n\t/*\n\t * Once the stack expansion code is fixed to not change VMA bounds\n\t * under mmap_lock in read mode, this can be changed to take the\n\t * mmap_lock in read mode.\n\t */\n\tif (mmap_write_lock_killable(mm))\n\t\treturn false;\n\n\tcprm->vma_data_size = 0;\n\tgate_vma = get_gate_vma(mm);\n\tcprm->vma_count = mm->map_count + (gate_vma ? 1 : 0);\n\n\tcprm->vma_meta = kvmalloc_array(cprm->vma_count, sizeof(*cprm->vma_meta), GFP_KERNEL);\n\tif (!cprm->vma_meta) {\n\t\tmmap_write_unlock(mm);\n\t\treturn false;\n\t}\n\n\tfor (i = 0, vma = first_vma(current, gate_vma); vma != NULL;\n\t\t\tvma = next_vma(vma, gate_vma), i++) {\n\t\tstruct core_vma_metadata *m = cprm->vma_meta + i;\n\n\t\tm->start = vma->vm_start;\n\t\tm->end = vma->vm_end;\n\t\tm->flags = vma->vm_flags;\n\t\tm->dump_size = vma_dump_size(vma, cprm->mm_flags);\n\t\tm->pgoff = vma->vm_pgoff;\n\n\t\tm->file = vma->vm_file;\n\t\tif (m->file)\n\t\t\tget_file(m->file);\n\t}\n\n\tmmap_write_unlock(mm);\n\n\tfor (i = 0; i < cprm->vma_count; i++) {\n\t\tstruct core_vma_metadata *m = cprm->vma_meta + i;\n\n\t\tif (m->dump_size == DUMP_SIZE_MAYBE_ELFHDR_PLACEHOLDER) {\n\t\t\tchar elfmag[SELFMAG];\n\n\t\t\tif (copy_from_user(elfmag, (void __user *)m->start, SELFMAG) ||\n\t\t\t\t\tmemcmp(elfmag, ELFMAG, SELFMAG) != 0) {\n\t\t\t\tm->dump_size = 0;\n\t\t\t} else {\n\t\t\t\tm->dump_size = PAGE_SIZE;\n\t\t\t}\n\t\t}\n\n\t\tcprm->vma_data_size += m->dump_size;\n\t}\n\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,6 +30,11 @@\n \t\tm->end = vma->vm_end;\n \t\tm->flags = vma->vm_flags;\n \t\tm->dump_size = vma_dump_size(vma, cprm->mm_flags);\n+\t\tm->pgoff = vma->vm_pgoff;\n+\n+\t\tm->file = vma->vm_file;\n+\t\tif (m->file)\n+\t\t\tget_file(m->file);\n \t}\n \n \tmmap_write_unlock(mm);",
        "function_modified_lines": {
            "added": [
                "\t\tm->pgoff = vma->vm_pgoff;",
                "",
                "\t\tm->file = vma->vm_file;",
                "\t\tif (m->file)",
                "\t\t\tget_file(m->file);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s core dump subsystem. This flaw allows a local user to crash the system. Only if patch 390031c94211 (\"coredump: Use the vma snapshot in fill_files_note\") not applied yet, then kernel could be affected.",
        "id": 3860
    },
    {
        "cve_id": "CVE-2017-18218",
        "code_before_change": "int hns_nic_net_xmit_hw(struct net_device *ndev,\n\t\t\tstruct sk_buff *skb,\n\t\t\tstruct hns_nic_ring_data *ring_data)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct device *dev = ring_to_dev(ring);\n\tstruct netdev_queue *dev_queue;\n\tstruct skb_frag_struct *frag;\n\tint buf_num;\n\tint seg_num;\n\tdma_addr_t dma;\n\tint size, next_to_use;\n\tint i;\n\n\tswitch (priv->ops.maybe_stop_tx(&skb, &buf_num, ring)) {\n\tcase -EBUSY:\n\t\tring->stats.tx_busy++;\n\t\tgoto out_net_tx_busy;\n\tcase -ENOMEM:\n\t\tring->stats.sw_err_cnt++;\n\t\tnetdev_err(ndev, \"no memory to xmit!\\n\");\n\t\tgoto out_err_tx_ok;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* no. of segments (plus a header) */\n\tseg_num = skb_shinfo(skb)->nr_frags + 1;\n\tnext_to_use = ring->next_to_use;\n\n\t/* fill the first part */\n\tsize = skb_headlen(skb);\n\tdma = dma_map_single(dev, skb->data, size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma)) {\n\t\tnetdev_err(ndev, \"TX head DMA map failed\\n\");\n\t\tring->stats.sw_err_cnt++;\n\t\tgoto out_err_tx_ok;\n\t}\n\tpriv->ops.fill_desc(ring, skb, size, dma, seg_num == 1 ? 1 : 0,\n\t\t\t    buf_num, DESC_TYPE_SKB, ndev->mtu);\n\n\t/* fill the fragments */\n\tfor (i = 1; i < seg_num; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i - 1];\n\t\tsize = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(dev, frag, 0, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma)) {\n\t\t\tnetdev_err(ndev, \"TX frag(%d) DMA map failed\\n\", i);\n\t\t\tring->stats.sw_err_cnt++;\n\t\t\tgoto out_map_frag_fail;\n\t\t}\n\t\tpriv->ops.fill_desc(ring, skb_frag_page(frag), size, dma,\n\t\t\t\t    seg_num - 1 == i ? 1 : 0, buf_num,\n\t\t\t\t    DESC_TYPE_PAGE, ndev->mtu);\n\t}\n\n\t/*complete translate all packets*/\n\tdev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);\n\tnetdev_tx_sent_queue(dev_queue, skb->len);\n\n\twmb(); /* commit all data before submit */\n\tassert(skb->queue_mapping < priv->ae_handle->q_num);\n\thnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);\n\tring->stats.tx_pkts++;\n\tring->stats.tx_bytes += skb->len;\n\n\treturn NETDEV_TX_OK;\n\nout_map_frag_fail:\n\n\twhile (ring->next_to_use != next_to_use) {\n\t\tunfill_desc(ring);\n\t\tif (ring->next_to_use != next_to_use)\n\t\t\tdma_unmap_page(dev,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].dma,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].length,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(dev,\n\t\t\t\t\t ring->desc_cb[next_to_use].dma,\n\t\t\t\t\t ring->desc_cb[next_to_use].length,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\n\nout_err_tx_ok:\n\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n\nout_net_tx_busy:\n\n\tnetif_stop_subqueue(ndev, skb->queue_mapping);\n\n\t/* Herbert's original patch had:\n\t *  smp_mb__after_netif_stop_queue();\n\t * but since that doesn't exist yet, just open code it.\n\t */\n\tsmp_mb();\n\treturn NETDEV_TX_BUSY;\n}",
        "code_after_change": "netdev_tx_t hns_nic_net_xmit_hw(struct net_device *ndev,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct hns_nic_ring_data *ring_data)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct device *dev = ring_to_dev(ring);\n\tstruct netdev_queue *dev_queue;\n\tstruct skb_frag_struct *frag;\n\tint buf_num;\n\tint seg_num;\n\tdma_addr_t dma;\n\tint size, next_to_use;\n\tint i;\n\n\tswitch (priv->ops.maybe_stop_tx(&skb, &buf_num, ring)) {\n\tcase -EBUSY:\n\t\tring->stats.tx_busy++;\n\t\tgoto out_net_tx_busy;\n\tcase -ENOMEM:\n\t\tring->stats.sw_err_cnt++;\n\t\tnetdev_err(ndev, \"no memory to xmit!\\n\");\n\t\tgoto out_err_tx_ok;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* no. of segments (plus a header) */\n\tseg_num = skb_shinfo(skb)->nr_frags + 1;\n\tnext_to_use = ring->next_to_use;\n\n\t/* fill the first part */\n\tsize = skb_headlen(skb);\n\tdma = dma_map_single(dev, skb->data, size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma)) {\n\t\tnetdev_err(ndev, \"TX head DMA map failed\\n\");\n\t\tring->stats.sw_err_cnt++;\n\t\tgoto out_err_tx_ok;\n\t}\n\tpriv->ops.fill_desc(ring, skb, size, dma, seg_num == 1 ? 1 : 0,\n\t\t\t    buf_num, DESC_TYPE_SKB, ndev->mtu);\n\n\t/* fill the fragments */\n\tfor (i = 1; i < seg_num; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i - 1];\n\t\tsize = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(dev, frag, 0, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma)) {\n\t\t\tnetdev_err(ndev, \"TX frag(%d) DMA map failed\\n\", i);\n\t\t\tring->stats.sw_err_cnt++;\n\t\t\tgoto out_map_frag_fail;\n\t\t}\n\t\tpriv->ops.fill_desc(ring, skb_frag_page(frag), size, dma,\n\t\t\t\t    seg_num - 1 == i ? 1 : 0, buf_num,\n\t\t\t\t    DESC_TYPE_PAGE, ndev->mtu);\n\t}\n\n\t/*complete translate all packets*/\n\tdev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);\n\tnetdev_tx_sent_queue(dev_queue, skb->len);\n\n\tnetif_trans_update(ndev);\n\tndev->stats.tx_bytes += skb->len;\n\tndev->stats.tx_packets++;\n\n\twmb(); /* commit all data before submit */\n\tassert(skb->queue_mapping < priv->ae_handle->q_num);\n\thnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);\n\tring->stats.tx_pkts++;\n\tring->stats.tx_bytes += skb->len;\n\n\treturn NETDEV_TX_OK;\n\nout_map_frag_fail:\n\n\twhile (ring->next_to_use != next_to_use) {\n\t\tunfill_desc(ring);\n\t\tif (ring->next_to_use != next_to_use)\n\t\t\tdma_unmap_page(dev,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].dma,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].length,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(dev,\n\t\t\t\t\t ring->desc_cb[next_to_use].dma,\n\t\t\t\t\t ring->desc_cb[next_to_use].length,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\n\nout_err_tx_ok:\n\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n\nout_net_tx_busy:\n\n\tnetif_stop_subqueue(ndev, skb->queue_mapping);\n\n\t/* Herbert's original patch had:\n\t *  smp_mb__after_netif_stop_queue();\n\t * but since that doesn't exist yet, just open code it.\n\t */\n\tsmp_mb();\n\treturn NETDEV_TX_BUSY;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n-int hns_nic_net_xmit_hw(struct net_device *ndev,\n-\t\t\tstruct sk_buff *skb,\n-\t\t\tstruct hns_nic_ring_data *ring_data)\n+netdev_tx_t hns_nic_net_xmit_hw(struct net_device *ndev,\n+\t\t\t\tstruct sk_buff *skb,\n+\t\t\t\tstruct hns_nic_ring_data *ring_data)\n {\n \tstruct hns_nic_priv *priv = netdev_priv(ndev);\n \tstruct hnae_ring *ring = ring_data->ring;\n@@ -59,6 +59,10 @@\n \tdev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);\n \tnetdev_tx_sent_queue(dev_queue, skb->len);\n \n+\tnetif_trans_update(ndev);\n+\tndev->stats.tx_bytes += skb->len;\n+\tndev->stats.tx_packets++;\n+\n \twmb(); /* commit all data before submit */\n \tassert(skb->queue_mapping < priv->ae_handle->q_num);\n \thnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);",
        "function_modified_lines": {
            "added": [
                "netdev_tx_t hns_nic_net_xmit_hw(struct net_device *ndev,",
                "\t\t\t\tstruct sk_buff *skb,",
                "\t\t\t\tstruct hns_nic_ring_data *ring_data)",
                "\tnetif_trans_update(ndev);",
                "\tndev->stats.tx_bytes += skb->len;",
                "\tndev->stats.tx_packets++;",
                ""
            ],
            "deleted": [
                "int hns_nic_net_xmit_hw(struct net_device *ndev,",
                "\t\t\tstruct sk_buff *skb,",
                "\t\t\tstruct hns_nic_ring_data *ring_data)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In drivers/net/ethernet/hisilicon/hns/hns_enet.c in the Linux kernel before 4.13, local users can cause a denial of service (use-after-free and BUG) or possibly have unspecified other impact by leveraging differences in skb handling between hns_nic_net_xmit_hw and hns_nic_net_xmit.",
        "id": 1404
    },
    {
        "cve_id": "CVE-2023-1872",
        "code_before_change": "static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req);\n\tif (ret > 0)\n\t\treturn;\n\n\tif (!ret) {\n\t\treq->result = mangle_poll(req->result & req->poll.events);\n\t} else {\n\t\treq->result = ret;\n\t\treq_set_fail(req);\n\t}\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\t__io_req_complete_post(req, req->result, 0);\n\tio_commit_cqring(ctx);\n\tspin_unlock(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n}",
        "code_after_change": "static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req, *locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tif (!ret) {\n\t\treq->result = mangle_poll(req->result & req->poll.events);\n\t} else {\n\t\treq->result = ret;\n\t\treq_set_fail(req);\n\t}\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\t__io_req_complete_post(req, req->result, 0);\n\tio_commit_cqring(ctx);\n\tspin_unlock(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tstruct io_ring_ctx *ctx = req->ctx;\n \tint ret;\n \n-\tret = io_poll_check_events(req);\n+\tret = io_poll_check_events(req, *locked);\n \tif (ret > 0)\n \t\treturn;\n ",
        "function_modified_lines": {
            "added": [
                "\tret = io_poll_check_events(req, *locked);"
            ],
            "deleted": [
                "\tret = io_poll_check_events(req);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring system can be exploited to achieve local privilege escalation.\n\nThe io_file_get_fixed function lacks the presence of ctx->uring_lock which can lead to a Use-After-Free vulnerability due a race condition with fixed files getting unregistered.\n\nWe recommend upgrading past commit da24142b1ef9fd5d36b76e36bab328a5b27523e8.\n\n",
        "id": 3884
    },
    {
        "cve_id": "CVE-2018-10902",
        "code_before_change": "int snd_rawmidi_output_params(struct snd_rawmidi_substream *substream,\n\t\t\t      struct snd_rawmidi_params * params)\n{\n\tchar *newbuf;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\t\n\tif (substream->append && substream->use_count > 1)\n\t\treturn -EBUSY;\n\tsnd_rawmidi_drain_output(substream);\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\truntime->avail = runtime->buffer_size;\n\t}\n\truntime->avail_min = params->avail_min;\n\tsubstream->active_sensing = !params->no_active_sensing;\n\treturn 0;\n}",
        "code_after_change": "int snd_rawmidi_output_params(struct snd_rawmidi_substream *substream,\n\t\t\t      struct snd_rawmidi_params * params)\n{\n\tchar *newbuf, *oldbuf;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\t\n\tif (substream->append && substream->use_count > 1)\n\t\treturn -EBUSY;\n\tsnd_rawmidi_drain_output(substream);\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&runtime->lock);\n\t\toldbuf = runtime->buffer;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\truntime->avail = runtime->buffer_size;\n\t\truntime->appl_ptr = runtime->hw_ptr = 0;\n\t\tspin_unlock_irq(&runtime->lock);\n\t\tkfree(oldbuf);\n\t}\n\truntime->avail_min = params->avail_min;\n\tsubstream->active_sensing = !params->no_active_sensing;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n int snd_rawmidi_output_params(struct snd_rawmidi_substream *substream,\n \t\t\t      struct snd_rawmidi_params * params)\n {\n-\tchar *newbuf;\n+\tchar *newbuf, *oldbuf;\n \tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n \t\n \tif (substream->append && substream->use_count > 1)\n@@ -14,13 +14,17 @@\n \t\treturn -EINVAL;\n \t}\n \tif (params->buffer_size != runtime->buffer_size) {\n-\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,\n-\t\t\t\t  GFP_KERNEL);\n+\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);\n \t\tif (!newbuf)\n \t\t\treturn -ENOMEM;\n+\t\tspin_lock_irq(&runtime->lock);\n+\t\toldbuf = runtime->buffer;\n \t\truntime->buffer = newbuf;\n \t\truntime->buffer_size = params->buffer_size;\n \t\truntime->avail = runtime->buffer_size;\n+\t\truntime->appl_ptr = runtime->hw_ptr = 0;\n+\t\tspin_unlock_irq(&runtime->lock);\n+\t\tkfree(oldbuf);\n \t}\n \truntime->avail_min = params->avail_min;\n \tsubstream->active_sensing = !params->no_active_sensing;",
        "function_modified_lines": {
            "added": [
                "\tchar *newbuf, *oldbuf;",
                "\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);",
                "\t\tspin_lock_irq(&runtime->lock);",
                "\t\toldbuf = runtime->buffer;",
                "\t\truntime->appl_ptr = runtime->hw_ptr = 0;",
                "\t\tspin_unlock_irq(&runtime->lock);",
                "\t\tkfree(oldbuf);"
            ],
            "deleted": [
                "\tchar *newbuf;",
                "\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,",
                "\t\t\t\t  GFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "It was found that the raw midi kernel driver does not protect against concurrent access which leads to a double realloc (double free) in snd_rawmidi_input_params() and snd_rawmidi_output_status() which are part of snd_rawmidi_ioctl() handler in rawmidi.c file. A malicious local attacker could possibly use this for privilege escalation.",
        "id": 1622
    },
    {
        "cve_id": "CVE-2021-0941",
        "code_before_change": "static inline int __bpf_skb_change_tail(struct sk_buff *skb, u32 new_len,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = __bpf_skb_max_len(skb);\n\tu32 min_len = __bpf_skb_min_len(skb);\n\tint ret;\n\n\tif (unlikely(flags || new_len > max_len || new_len < min_len))\n\t\treturn -EINVAL;\n\tif (skb->encapsulation)\n\t\treturn -ENOTSUPP;\n\n\t/* The basic idea of this helper is that it's performing the\n\t * needed work to either grow or trim an skb, and eBPF program\n\t * rewrites the rest via helpers like bpf_skb_store_bytes(),\n\t * bpf_lX_csum_replace() and others rather than passing a raw\n\t * buffer here. This one is a slow path helper and intended\n\t * for replies with control messages.\n\t *\n\t * Like in bpf_skb_change_proto(), we want to keep this rather\n\t * minimal and without protocol specifics so that we are able\n\t * to separate concerns as in bpf_skb_store_bytes() should only\n\t * be the one responsible for writing buffers.\n\t *\n\t * It's really expected to be a slow path operation here for\n\t * control message replies, so we're implicitly linearizing,\n\t * uncloning and drop offloads from the skb by this.\n\t */\n\tret = __bpf_try_make_writable(skb, skb->len);\n\tif (!ret) {\n\t\tif (new_len > skb->len)\n\t\t\tret = bpf_skb_grow_rcsum(skb, new_len);\n\t\telse if (new_len < skb->len)\n\t\t\tret = bpf_skb_trim_rcsum(skb, new_len);\n\t\tif (!ret && skb_is_gso(skb))\n\t\t\tskb_gso_reset(skb);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static inline int __bpf_skb_change_tail(struct sk_buff *skb, u32 new_len,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = BPF_SKB_MAX_LEN;\n\tu32 min_len = __bpf_skb_min_len(skb);\n\tint ret;\n\n\tif (unlikely(flags || new_len > max_len || new_len < min_len))\n\t\treturn -EINVAL;\n\tif (skb->encapsulation)\n\t\treturn -ENOTSUPP;\n\n\t/* The basic idea of this helper is that it's performing the\n\t * needed work to either grow or trim an skb, and eBPF program\n\t * rewrites the rest via helpers like bpf_skb_store_bytes(),\n\t * bpf_lX_csum_replace() and others rather than passing a raw\n\t * buffer here. This one is a slow path helper and intended\n\t * for replies with control messages.\n\t *\n\t * Like in bpf_skb_change_proto(), we want to keep this rather\n\t * minimal and without protocol specifics so that we are able\n\t * to separate concerns as in bpf_skb_store_bytes() should only\n\t * be the one responsible for writing buffers.\n\t *\n\t * It's really expected to be a slow path operation here for\n\t * control message replies, so we're implicitly linearizing,\n\t * uncloning and drop offloads from the skb by this.\n\t */\n\tret = __bpf_try_make_writable(skb, skb->len);\n\tif (!ret) {\n\t\tif (new_len > skb->len)\n\t\t\tret = bpf_skb_grow_rcsum(skb, new_len);\n\t\telse if (new_len < skb->len)\n\t\t\tret = bpf_skb_trim_rcsum(skb, new_len);\n\t\tif (!ret && skb_is_gso(skb))\n\t\t\tskb_gso_reset(skb);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n static inline int __bpf_skb_change_tail(struct sk_buff *skb, u32 new_len,\n \t\t\t\t\tu64 flags)\n {\n-\tu32 max_len = __bpf_skb_max_len(skb);\n+\tu32 max_len = BPF_SKB_MAX_LEN;\n \tu32 min_len = __bpf_skb_min_len(skb);\n \tint ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tu32 max_len = BPF_SKB_MAX_LEN;"
            ],
            "deleted": [
                "\tu32 max_len = __bpf_skb_max_len(skb);"
            ]
        },
        "cwe": [
            "CWE-125",
            "CWE-416"
        ],
        "cve_description": "In bpf_skb_change_head of filter.c, there is a possible out of bounds read due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-154177719References: Upstream kernel",
        "id": 2840
    },
    {
        "cve_id": "CVE-2017-6874",
        "code_before_change": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 0);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n\t\tucounts = NULL;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "code_after_change": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tnew->count = 0;\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (ucounts->count == INT_MAX)\n\t\tucounts = NULL;\n\telse\n\t\tucounts->count += 1;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,7 @@\n \n \t\tnew->ns = ns;\n \t\tnew->uid = uid;\n-\t\tatomic_set(&new->count, 0);\n+\t\tnew->count = 0;\n \n \t\tspin_lock_irq(&ucounts_lock);\n \t\tucounts = find_ucounts(ns, uid, hashent);\n@@ -25,8 +25,10 @@\n \t\t\tucounts = new;\n \t\t}\n \t}\n-\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n+\tif (ucounts->count == INT_MAX)\n \t\tucounts = NULL;\n+\telse\n+\t\tucounts->count += 1;\n \tspin_unlock_irq(&ucounts_lock);\n \treturn ucounts;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tnew->count = 0;",
                "\tif (ucounts->count == INT_MAX)",
                "\telse",
                "\t\tucounts->count += 1;"
            ],
            "deleted": [
                "\t\tatomic_set(&new->count, 0);",
                "\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/ucount.c in the Linux kernel through 4.10.2 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls that leverage certain decrement behavior that causes incorrect interaction between put_ucounts and get_ucounts.",
        "id": 1488
    },
    {
        "cve_id": "CVE-2017-7374",
        "code_before_change": "static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n{\n\tstruct dentry *dir;\n\tstruct fscrypt_info *ci;\n\tint dir_has_key, cached_with_key;\n\n\tif (flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\tdir = dget_parent(dentry);\n\tif (!d_inode(dir)->i_sb->s_cop->is_encrypted(d_inode(dir))) {\n\t\tdput(dir);\n\t\treturn 0;\n\t}\n\n\tci = d_inode(dir)->i_crypt_info;\n\tif (ci && ci->ci_keyring_key &&\n\t    (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n\t\t\t\t\t  (1 << KEY_FLAG_REVOKED) |\n\t\t\t\t\t  (1 << KEY_FLAG_DEAD))))\n\t\tci = NULL;\n\n\t/* this should eventually be an flag in d_flags */\n\tspin_lock(&dentry->d_lock);\n\tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n\tspin_unlock(&dentry->d_lock);\n\tdir_has_key = (ci != NULL);\n\tdput(dir);\n\n\t/*\n\t * If the dentry was cached without the key, and it is a\n\t * negative dentry, it might be a valid name.  We can't check\n\t * if the key has since been made available due to locking\n\t * reasons, so we fail the validation so ext4_lookup() can do\n\t * this check.\n\t *\n\t * We also fail the validation if the dentry was created with\n\t * the key present, but we no longer have the key, or vice versa.\n\t */\n\tif ((!cached_with_key && d_is_negative(dentry)) ||\n\t\t\t(!cached_with_key && dir_has_key) ||\n\t\t\t(cached_with_key && !dir_has_key))\n\t\treturn 0;\n\treturn 1;\n}",
        "code_after_change": "static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n{\n\tstruct dentry *dir;\n\tint dir_has_key, cached_with_key;\n\n\tif (flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\tdir = dget_parent(dentry);\n\tif (!d_inode(dir)->i_sb->s_cop->is_encrypted(d_inode(dir))) {\n\t\tdput(dir);\n\t\treturn 0;\n\t}\n\n\t/* this should eventually be an flag in d_flags */\n\tspin_lock(&dentry->d_lock);\n\tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n\tspin_unlock(&dentry->d_lock);\n\tdir_has_key = (d_inode(dir)->i_crypt_info != NULL);\n\tdput(dir);\n\n\t/*\n\t * If the dentry was cached without the key, and it is a\n\t * negative dentry, it might be a valid name.  We can't check\n\t * if the key has since been made available due to locking\n\t * reasons, so we fail the validation so ext4_lookup() can do\n\t * this check.\n\t *\n\t * We also fail the validation if the dentry was created with\n\t * the key present, but we no longer have the key, or vice versa.\n\t */\n\tif ((!cached_with_key && d_is_negative(dentry)) ||\n\t\t\t(!cached_with_key && dir_has_key) ||\n\t\t\t(cached_with_key && !dir_has_key))\n\t\treturn 0;\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n {\n \tstruct dentry *dir;\n-\tstruct fscrypt_info *ci;\n \tint dir_has_key, cached_with_key;\n \n \tif (flags & LOOKUP_RCU)\n@@ -13,18 +12,11 @@\n \t\treturn 0;\n \t}\n \n-\tci = d_inode(dir)->i_crypt_info;\n-\tif (ci && ci->ci_keyring_key &&\n-\t    (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n-\t\t\t\t\t  (1 << KEY_FLAG_REVOKED) |\n-\t\t\t\t\t  (1 << KEY_FLAG_DEAD))))\n-\t\tci = NULL;\n-\n \t/* this should eventually be an flag in d_flags */\n \tspin_lock(&dentry->d_lock);\n \tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n \tspin_unlock(&dentry->d_lock);\n-\tdir_has_key = (ci != NULL);\n+\tdir_has_key = (d_inode(dir)->i_crypt_info != NULL);\n \tdput(dir);\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\tdir_has_key = (d_inode(dir)->i_crypt_info != NULL);"
            ],
            "deleted": [
                "\tstruct fscrypt_info *ci;",
                "\tci = d_inode(dir)->i_crypt_info;",
                "\tif (ci && ci->ci_keyring_key &&",
                "\t    (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |",
                "\t\t\t\t\t  (1 << KEY_FLAG_REVOKED) |",
                "\t\t\t\t\t  (1 << KEY_FLAG_DEAD))))",
                "\t\tci = NULL;",
                "",
                "\tdir_has_key = (ci != NULL);"
            ]
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/crypto/ in the Linux kernel before 4.10.7 allows local users to cause a denial of service (NULL pointer dereference) or possibly gain privileges by revoking keyring keys being used for ext4, f2fs, or ubifs encryption, causing cryptographic transform objects to be freed prematurely.",
        "id": 1497
    },
    {
        "cve_id": "CVE-2023-0240",
        "code_before_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe,\n\t\t       struct io_submit_state *state)\n{\n\tunsigned int sqe_flags;\n\tint id, ret;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->flags = 0;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm(ctx, req)))\n\t\treturn -EFAULT;\n\n\tsqe_flags = READ_ONCE(sqe->flags);\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tio_req_init_async(req);\n\t\treq->work.identity->creds = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!req->work.identity->creds))\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->work.identity->creds);\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags |= sqe_flags;\n\n\tif (!io_op_defs[req->opcode].needs_file)\n\t\treturn 0;\n\n\tret = io_req_set_file(state, req, READ_ONCE(sqe->fd));\n\tstate->ios_left--;\n\treturn ret;\n}",
        "code_after_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe,\n\t\t       struct io_submit_state *state)\n{\n\tunsigned int sqe_flags;\n\tint id, ret;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->flags = 0;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm(ctx, req)))\n\t\treturn -EFAULT;\n\n\tsqe_flags = READ_ONCE(sqe->flags);\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tstruct io_identity *iod;\n\n\t\tio_req_init_async(req);\n\t\tiod = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!iod))\n\t\t\treturn -EINVAL;\n\t\trefcount_inc(&iod->count);\n\t\tio_put_identity(req);\n\t\tget_cred(iod->creds);\n\t\treq->work.identity = iod;\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags |= sqe_flags;\n\n\tif (!io_op_defs[req->opcode].needs_file)\n\t\treturn 0;\n\n\tret = io_req_set_file(state, req, READ_ONCE(sqe->fd));\n\tstate->ios_left--;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,11 +36,16 @@\n \n \tid = READ_ONCE(sqe->personality);\n \tif (id) {\n+\t\tstruct io_identity *iod;\n+\n \t\tio_req_init_async(req);\n-\t\treq->work.identity->creds = idr_find(&ctx->personality_idr, id);\n-\t\tif (unlikely(!req->work.identity->creds))\n+\t\tiod = idr_find(&ctx->personality_idr, id);\n+\t\tif (unlikely(!iod))\n \t\t\treturn -EINVAL;\n-\t\tget_cred(req->work.identity->creds);\n+\t\trefcount_inc(&iod->count);\n+\t\tio_put_identity(req);\n+\t\tget_cred(iod->creds);\n+\t\treq->work.identity = iod;\n \t\treq->work.flags |= IO_WQ_WORK_CREDS;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tstruct io_identity *iod;",
                "",
                "\t\tiod = idr_find(&ctx->personality_idr, id);",
                "\t\tif (unlikely(!iod))",
                "\t\trefcount_inc(&iod->count);",
                "\t\tio_put_identity(req);",
                "\t\tget_cred(iod->creds);",
                "\t\treq->work.identity = iod;"
            ],
            "deleted": [
                "\t\treq->work.identity->creds = idr_find(&ctx->personality_idr, id);",
                "\t\tif (unlikely(!req->work.identity->creds))",
                "\t\tget_cred(req->work.identity->creds);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "id": 3819
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static void __io_req_task_submit(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/* ctx stays valid until unlock, even if we drop all ours ctx->refs */\n\tmutex_lock(&ctx->uring_lock);\n\tif (!ctx->sqo_dead && !(current->flags & PF_EXITING) && !current->in_execve)\n\t\t__io_queue_sqe(req);\n\telse\n\t\t__io_req_task_cancel(req, -EFAULT);\n\tmutex_unlock(&ctx->uring_lock);\n}",
        "code_after_change": "static void __io_req_task_submit(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/* ctx stays valid until unlock, even if we drop all ours ctx->refs */\n\tmutex_lock(&ctx->uring_lock);\n\tif (!(current->flags & PF_EXITING) && !current->in_execve)\n\t\t__io_queue_sqe(req);\n\telse\n\t\t__io_req_task_cancel(req, -EFAULT);\n\tmutex_unlock(&ctx->uring_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \n \t/* ctx stays valid until unlock, even if we drop all ours ctx->refs */\n \tmutex_lock(&ctx->uring_lock);\n-\tif (!ctx->sqo_dead && !(current->flags & PF_EXITING) && !current->in_execve)\n+\tif (!(current->flags & PF_EXITING) && !current->in_execve)\n \t\t__io_queue_sqe(req);\n \telse\n \t\t__io_req_task_cancel(req, -EFAULT);",
        "function_modified_lines": {
            "added": [
                "\tif (!(current->flags & PF_EXITING) && !current->in_execve)"
            ],
            "deleted": [
                "\tif (!ctx->sqo_dead && !(current->flags & PF_EXITING) && !current->in_execve)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3786
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static void io_uring_cancel_sqpoll(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\tstruct io_uring_task *tctx;\n\ts64 inflight;\n\tDEFINE_WAIT(wait);\n\n\tif (!sqd)\n\t\treturn;\n\tio_disable_sqo_submit(ctx);\n\tif (!io_sq_thread_park(sqd))\n\t\treturn;\n\ttctx = ctx->sq_data->thread->io_uring;\n\t/* can happen on fork/alloc failure, just ignore that state */\n\tif (!tctx) {\n\t\tio_sq_thread_unpark(sqd);\n\t\treturn;\n\t}\n\n\tatomic_inc(&tctx->in_idle);\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\tio_uring_cancel_task_requests(ctx, NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\tatomic_dec(&tctx->in_idle);\n\tio_sq_thread_unpark(sqd);\n}",
        "code_after_change": "static void io_uring_cancel_sqpoll(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\tstruct io_uring_task *tctx;\n\ts64 inflight;\n\tDEFINE_WAIT(wait);\n\n\tif (!sqd)\n\t\treturn;\n\tif (!io_sq_thread_park(sqd))\n\t\treturn;\n\ttctx = ctx->sq_data->thread->io_uring;\n\t/* can happen on fork/alloc failure, just ignore that state */\n\tif (!tctx) {\n\t\tio_sq_thread_unpark(sqd);\n\t\treturn;\n\t}\n\n\tatomic_inc(&tctx->in_idle);\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\tio_uring_cancel_task_requests(ctx, NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\tatomic_dec(&tctx->in_idle);\n\tio_sq_thread_unpark(sqd);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,6 @@\n \n \tif (!sqd)\n \t\treturn;\n-\tio_disable_sqo_submit(ctx);\n \tif (!io_sq_thread_park(sqd))\n \t\treturn;\n \ttctx = ctx->sq_data->thread->io_uring;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tio_disable_sqo_submit(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3783
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static int io_sq_thread_fork(struct io_sq_data *sqd, struct io_ring_ctx *ctx)\n{\n\tint ret;\n\n\tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\treinit_completion(&sqd->completion);\n\tctx->sqo_dead = ctx->sqo_exec = 0;\n\tsqd->task_pid = current->pid;\n\tcurrent->flags |= PF_IO_WORKER;\n\tret = io_wq_fork_thread(io_sq_thread, sqd);\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (ret < 0) {\n\t\tsqd->thread = NULL;\n\t\treturn ret;\n\t}\n\twait_for_completion(&sqd->completion);\n\treturn io_uring_alloc_task_context(sqd->thread, ctx);\n}",
        "code_after_change": "static int io_sq_thread_fork(struct io_sq_data *sqd, struct io_ring_ctx *ctx)\n{\n\tint ret;\n\n\tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\treinit_completion(&sqd->completion);\n\tctx->sqo_exec = 0;\n\tsqd->task_pid = current->pid;\n\tcurrent->flags |= PF_IO_WORKER;\n\tret = io_wq_fork_thread(io_sq_thread, sqd);\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (ret < 0) {\n\t\tsqd->thread = NULL;\n\t\treturn ret;\n\t}\n\twait_for_completion(&sqd->completion);\n\treturn io_uring_alloc_task_context(sqd->thread, ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \n \tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n \treinit_completion(&sqd->completion);\n-\tctx->sqo_dead = ctx->sqo_exec = 0;\n+\tctx->sqo_exec = 0;\n \tsqd->task_pid = current->pid;\n \tcurrent->flags |= PF_IO_WORKER;\n \tret = io_wq_fork_thread(io_sq_thread, sqd);",
        "function_modified_lines": {
            "added": [
                "\tctx->sqo_exec = 0;"
            ],
            "deleted": [
                "\tctx->sqo_dead = ctx->sqo_exec = 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3781
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static int io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tint ret = 0;\n\tDEFINE_WAIT(wait);\n\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (unlikely(ctx->sqo_dead)) {\n\t\t\tret = -EOWNERDEAD;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\n\t\tschedule();\n\t} while (!signal_pending(current));\n\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\nout:\n\treturn ret;\n}",
        "code_after_change": "static int io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tint ret = 0;\n\tDEFINE_WAIT(wait);\n\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tschedule();\n\t} while (!signal_pending(current));\n\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,21 +6,13 @@\n \tdo {\n \t\tif (!io_sqring_full(ctx))\n \t\t\tbreak;\n-\n \t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n-\n-\t\tif (unlikely(ctx->sqo_dead)) {\n-\t\t\tret = -EOWNERDEAD;\n-\t\t\tgoto out;\n-\t\t}\n \n \t\tif (!io_sqring_full(ctx))\n \t\t\tbreak;\n-\n \t\tschedule();\n \t} while (!signal_pending(current));\n \n \tfinish_wait(&ctx->sqo_sq_wait, &wait);\n-out:\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "",
                "\t\tif (unlikely(ctx->sqo_dead)) {",
                "\t\t\tret = -EOWNERDEAD;",
                "\t\t\tgoto out;",
                "\t\t}",
                "",
                "out:"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3778
    },
    {
        "cve_id": "CVE-2022-1652",
        "code_before_change": "static int set_next_request(void)\n{\n\tcurrent_req = list_first_entry_or_null(&floppy_reqs, struct request,\n\t\t\t\t\t       queuelist);\n\tif (current_req) {\n\t\tcurrent_req->error_count = 0;\n\t\tlist_del_init(&current_req->queuelist);\n\t}\n\treturn current_req != NULL;\n}",
        "code_after_change": "static int set_next_request(void)\n{\n\tcurrent_req = list_first_entry_or_null(&floppy_reqs, struct request,\n\t\t\t\t\t       queuelist);\n\tif (current_req) {\n\t\tfloppy_errors = 0;\n\t\tlist_del_init(&current_req->queuelist);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,8 +3,9 @@\n \tcurrent_req = list_first_entry_or_null(&floppy_reqs, struct request,\n \t\t\t\t\t       queuelist);\n \tif (current_req) {\n-\t\tcurrent_req->error_count = 0;\n+\t\tfloppy_errors = 0;\n \t\tlist_del_init(&current_req->queuelist);\n+\t\treturn 1;\n \t}\n-\treturn current_req != NULL;\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tfloppy_errors = 0;",
                "\t\treturn 1;",
                "\treturn 0;"
            ],
            "deleted": [
                "\t\tcurrent_req->error_count = 0;",
                "\treturn current_req != NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Linux Kernel could allow a local attacker to execute arbitrary code on the system, caused by a concurrency use-after-free flaw in the bad_flp_intr function. By executing a specially-crafted program, an attacker could exploit this vulnerability to execute arbitrary code or cause a denial of service condition on the system.",
        "id": 3269
    },
    {
        "cve_id": "CVE-2022-1652",
        "code_before_change": "static void bad_flp_intr(void)\n{\n\tint err_count;\n\n\tif (probing) {\n\t\tdrive_state[current_drive].probed_format++;\n\t\tif (!next_valid_format(current_drive))\n\t\t\treturn;\n\t}\n\terr_count = ++(*errors);\n\tINFBOUND(write_errors[current_drive].badness, err_count);\n\tif (err_count > drive_params[current_drive].max_errors.abort)\n\t\tcont->done(0);\n\tif (err_count > drive_params[current_drive].max_errors.reset)\n\t\tfdc_state[current_fdc].reset = 1;\n\telse if (err_count > drive_params[current_drive].max_errors.recal)\n\t\tdrive_state[current_drive].track = NEED_2_RECAL;\n}",
        "code_after_change": "static void bad_flp_intr(void)\n{\n\tint err_count;\n\n\tif (probing) {\n\t\tdrive_state[current_drive].probed_format++;\n\t\tif (!next_valid_format(current_drive))\n\t\t\treturn;\n\t}\n\terr_count = ++floppy_errors;\n\tINFBOUND(write_errors[current_drive].badness, err_count);\n\tif (err_count > drive_params[current_drive].max_errors.abort)\n\t\tcont->done(0);\n\tif (err_count > drive_params[current_drive].max_errors.reset)\n\t\tfdc_state[current_fdc].reset = 1;\n\telse if (err_count > drive_params[current_drive].max_errors.recal)\n\t\tdrive_state[current_drive].track = NEED_2_RECAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \t\tif (!next_valid_format(current_drive))\n \t\t\treturn;\n \t}\n-\terr_count = ++(*errors);\n+\terr_count = ++floppy_errors;\n \tINFBOUND(write_errors[current_drive].badness, err_count);\n \tif (err_count > drive_params[current_drive].max_errors.abort)\n \t\tcont->done(0);",
        "function_modified_lines": {
            "added": [
                "\terr_count = ++floppy_errors;"
            ],
            "deleted": [
                "\terr_count = ++(*errors);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Linux Kernel could allow a local attacker to execute arbitrary code on the system, caused by a concurrency use-after-free flaw in the bad_flp_intr function. By executing a specially-crafted program, an attacker could exploit this vulnerability to execute arbitrary code or cause a denial of service condition on the system.",
        "id": 3266
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tvmw_user_bo_unref(vmw_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo, *tmp_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\ttmp_bo = vmw_bo;\n\tvmw_user_bo_unref(&tmp_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t\t   SVGAGuestPtr *ptr,\n \t\t\t\t   struct vmw_bo **vmw_bo_p)\n {\n-\tstruct vmw_bo *vmw_bo;\n+\tstruct vmw_bo *vmw_bo, *tmp_bo;\n \tuint32_t handle = ptr->gmrId;\n \tstruct vmw_relocation *reloc;\n \tint ret;\n@@ -17,7 +17,8 @@\n \tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n \t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n-\tvmw_user_bo_unref(vmw_bo);\n+\ttmp_bo = vmw_bo;\n+\tvmw_user_bo_unref(&tmp_bo);\n \tif (unlikely(ret != 0))\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct vmw_bo *vmw_bo, *tmp_bo;",
                "\ttmp_bo = vmw_bo;",
                "\tvmw_user_bo_unref(&tmp_bo);"
            ],
            "deleted": [
                "\tstruct vmw_bo *vmw_bo;",
                "\tvmw_user_bo_unref(vmw_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4272
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_shader_define(struct drm_device *dev, struct drm_file *file_priv,\n\t\t\t     enum drm_vmw_shader_type shader_type_drm,\n\t\t\t     u32 buffer_handle, size_t size, size_t offset,\n\t\t\t     uint8_t num_input_sig, uint8_t num_output_sig,\n\t\t\t     uint32_t *shader_handle)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_bo *buffer = NULL;\n\tSVGA3dShaderType shader_type;\n\tint ret;\n\n\tif (buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_bo_lookup(file_priv, buffer_handle, &buffer);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tVMW_DEBUG_USER(\"Couldn't find buffer for shader creation.\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tif ((u64)buffer->tbo.base.size < (u64)size + (u64)offset) {\n\t\t\tVMW_DEBUG_USER(\"Illegal buffer- or shader size.\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_bad_arg;\n\t\t}\n\t}\n\n\tswitch (shader_type_drm) {\n\tcase drm_vmw_shader_type_vs:\n\t\tshader_type = SVGA3D_SHADERTYPE_VS;\n\t\tbreak;\n\tcase drm_vmw_shader_type_ps:\n\t\tshader_type = SVGA3D_SHADERTYPE_PS;\n\t\tbreak;\n\tdefault:\n\t\tVMW_DEBUG_USER(\"Illegal shader type.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_bad_arg;\n\t}\n\n\tret = vmw_user_shader_alloc(dev_priv, buffer, size, offset,\n\t\t\t\t    shader_type, num_input_sig,\n\t\t\t\t    num_output_sig, tfile, shader_handle);\nout_bad_arg:\n\tvmw_user_bo_unref(buffer);\n\treturn ret;\n}",
        "code_after_change": "static int vmw_shader_define(struct drm_device *dev, struct drm_file *file_priv,\n\t\t\t     enum drm_vmw_shader_type shader_type_drm,\n\t\t\t     u32 buffer_handle, size_t size, size_t offset,\n\t\t\t     uint8_t num_input_sig, uint8_t num_output_sig,\n\t\t\t     uint32_t *shader_handle)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_bo *buffer = NULL;\n\tSVGA3dShaderType shader_type;\n\tint ret;\n\n\tif (buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_bo_lookup(file_priv, buffer_handle, &buffer);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tVMW_DEBUG_USER(\"Couldn't find buffer for shader creation.\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tif ((u64)buffer->tbo.base.size < (u64)size + (u64)offset) {\n\t\t\tVMW_DEBUG_USER(\"Illegal buffer- or shader size.\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_bad_arg;\n\t\t}\n\t}\n\n\tswitch (shader_type_drm) {\n\tcase drm_vmw_shader_type_vs:\n\t\tshader_type = SVGA3D_SHADERTYPE_VS;\n\t\tbreak;\n\tcase drm_vmw_shader_type_ps:\n\t\tshader_type = SVGA3D_SHADERTYPE_PS;\n\t\tbreak;\n\tdefault:\n\t\tVMW_DEBUG_USER(\"Illegal shader type.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_bad_arg;\n\t}\n\n\tret = vmw_user_shader_alloc(dev_priv, buffer, size, offset,\n\t\t\t\t    shader_type, num_input_sig,\n\t\t\t\t    num_output_sig, tfile, shader_handle);\nout_bad_arg:\n\tvmw_user_bo_unref(&buffer);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -41,6 +41,6 @@\n \t\t\t\t    shader_type, num_input_sig,\n \t\t\t\t    num_output_sig, tfile, shader_handle);\n out_bad_arg:\n-\tvmw_user_bo_unref(buffer);\n+\tvmw_user_bo_unref(&buffer);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tvmw_user_bo_unref(&buffer);"
            ],
            "deleted": [
                "\tvmw_user_bo_unref(buffer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4283
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int\nvmw_resource_check_buffer(struct ww_acquire_ctx *ticket,\n\t\t\t  struct vmw_resource *res,\n\t\t\t  bool interruptible,\n\t\t\t  struct ttm_validate_buffer *val_buf)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct list_head val_list;\n\tbool guest_memory_dirty = false;\n\tint ret;\n\n\tif (unlikely(!res->guest_memory_bo)) {\n\t\tret = vmw_resource_buf_alloc(res, interruptible);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t}\n\n\tINIT_LIST_HEAD(&val_list);\n\tttm_bo_get(&res->guest_memory_bo->tbo);\n\tval_buf->bo = &res->guest_memory_bo->tbo;\n\tval_buf->num_shared = 0;\n\tlist_add_tail(&val_buf->head, &val_list);\n\tret = ttm_eu_reserve_buffers(ticket, &val_list, interruptible, NULL);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_reserve;\n\n\tif (res->func->needs_guest_memory && !vmw_resource_mob_attached(res))\n\t\treturn 0;\n\n\tguest_memory_dirty = res->guest_memory_dirty;\n\tvmw_bo_placement_set(res->guest_memory_bo, res->func->domain,\n\t\t\t     res->func->busy_domain);\n\tret = ttm_bo_validate(&res->guest_memory_bo->tbo,\n\t\t\t      &res->guest_memory_bo->placement,\n\t\t\t      &ctx);\n\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_validate;\n\n\treturn 0;\n\nout_no_validate:\n\tttm_eu_backoff_reservation(ticket, &val_list);\nout_no_reserve:\n\tttm_bo_put(val_buf->bo);\n\tval_buf->bo = NULL;\n\tif (guest_memory_dirty)\n\t\tvmw_bo_unreference(&res->guest_memory_bo);\n\n\treturn ret;\n}",
        "code_after_change": "static int\nvmw_resource_check_buffer(struct ww_acquire_ctx *ticket,\n\t\t\t  struct vmw_resource *res,\n\t\t\t  bool interruptible,\n\t\t\t  struct ttm_validate_buffer *val_buf)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct list_head val_list;\n\tbool guest_memory_dirty = false;\n\tint ret;\n\n\tif (unlikely(!res->guest_memory_bo)) {\n\t\tret = vmw_resource_buf_alloc(res, interruptible);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t}\n\n\tINIT_LIST_HEAD(&val_list);\n\tttm_bo_get(&res->guest_memory_bo->tbo);\n\tval_buf->bo = &res->guest_memory_bo->tbo;\n\tval_buf->num_shared = 0;\n\tlist_add_tail(&val_buf->head, &val_list);\n\tret = ttm_eu_reserve_buffers(ticket, &val_list, interruptible, NULL);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_reserve;\n\n\tif (res->func->needs_guest_memory && !vmw_resource_mob_attached(res))\n\t\treturn 0;\n\n\tguest_memory_dirty = res->guest_memory_dirty;\n\tvmw_bo_placement_set(res->guest_memory_bo, res->func->domain,\n\t\t\t     res->func->busy_domain);\n\tret = ttm_bo_validate(&res->guest_memory_bo->tbo,\n\t\t\t      &res->guest_memory_bo->placement,\n\t\t\t      &ctx);\n\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_validate;\n\n\treturn 0;\n\nout_no_validate:\n\tttm_eu_backoff_reservation(ticket, &val_list);\nout_no_reserve:\n\tttm_bo_put(val_buf->bo);\n\tval_buf->bo = NULL;\n\tif (guest_memory_dirty)\n\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,7 +45,7 @@\n \tttm_bo_put(val_buf->bo);\n \tval_buf->bo = NULL;\n \tif (guest_memory_dirty)\n-\t\tvmw_bo_unreference(&res->guest_memory_bo);\n+\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n \n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tvmw_user_bo_unref(&res->guest_memory_bo);"
            ],
            "deleted": [
                "\t\tvmw_bo_unreference(&res->guest_memory_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4280
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "int vmw_user_bo_lookup(struct drm_file *filp,\n\t\t       u32 handle,\n\t\t       struct vmw_bo **out)\n{\n\tstruct drm_gem_object *gobj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj) {\n\t\tDRM_ERROR(\"Invalid buffer object handle 0x%08lx.\\n\",\n\t\t\t  (unsigned long)handle);\n\t\treturn -ESRCH;\n\t}\n\n\t*out = to_vmw_bo(gobj);\n\tttm_bo_get(&(*out)->tbo);\n\n\treturn 0;\n}",
        "code_after_change": "int vmw_user_bo_lookup(struct drm_file *filp,\n\t\t       u32 handle,\n\t\t       struct vmw_bo **out)\n{\n\tstruct drm_gem_object *gobj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj) {\n\t\tDRM_ERROR(\"Invalid buffer object handle 0x%08lx.\\n\",\n\t\t\t  (unsigned long)handle);\n\t\treturn -ESRCH;\n\t}\n\n\t*out = to_vmw_bo(gobj);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,6 @@\n \t}\n \n \t*out = to_vmw_bo(gobj);\n-\tttm_bo_get(&(*out)->tbo);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tttm_bo_get(&(*out)->tbo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4269
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_cotable_resize(struct vmw_resource *res, size_t new_size)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct vmw_cotable *vcotbl = vmw_cotable(res);\n\tstruct vmw_bo *buf, *old_buf = res->guest_memory_bo;\n\tstruct ttm_buffer_object *bo, *old_bo = &res->guest_memory_bo->tbo;\n\tsize_t old_size = res->guest_memory_size;\n\tsize_t old_size_read_back = vcotbl->size_read_back;\n\tsize_t cur_size_read_back;\n\tstruct ttm_bo_kmap_obj old_map, new_map;\n\tint ret;\n\tsize_t i;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = VMW_BO_DOMAIN_MOB,\n\t\t.busy_domain = VMW_BO_DOMAIN_MOB,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = new_size,\n\t\t.pin = true\n\t};\n\n\tMKS_STAT_TIME_DECL(MKSSTAT_KERN_COTABLE_RESIZE);\n\tMKS_STAT_TIME_PUSH(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\tret = vmw_cotable_readback(res);\n\tif (ret)\n\t\tgoto out_done;\n\n\tcur_size_read_back = vcotbl->size_read_back;\n\tvcotbl->size_read_back = old_size_read_back;\n\n\t/*\n\t * While device is processing, Allocate and reserve a buffer object\n\t * for the new COTable. Initially pin the buffer object to make sure\n\t * we can use tryreserve without failure.\n\t */\n\tret = vmw_bo_create(dev_priv, &bo_params, &buf);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed initializing new cotable MOB.\\n\");\n\t\tgoto out_done;\n\t}\n\n\tbo = &buf->tbo;\n\tWARN_ON_ONCE(ttm_bo_reserve(bo, false, true, NULL));\n\n\tret = ttm_bo_wait(old_bo, false, false);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed waiting for cotable unbind.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\t/*\n\t * Do a page by page copy of COTables. This eliminates slow vmap()s.\n\t * This should really be a TTM utility.\n\t */\n\tfor (i = 0; i < PFN_UP(old_bo->resource->size); ++i) {\n\t\tbool dummy;\n\n\t\tret = ttm_bo_kmap(old_bo, i, 1, &old_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping old COTable on resize.\\n\");\n\t\t\tgoto out_wait;\n\t\t}\n\t\tret = ttm_bo_kmap(bo, i, 1, &new_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping new COTable on resize.\\n\");\n\t\t\tgoto out_map_new;\n\t\t}\n\t\tmemcpy(ttm_kmap_obj_virtual(&new_map, &dummy),\n\t\t       ttm_kmap_obj_virtual(&old_map, &dummy),\n\t\t       PAGE_SIZE);\n\t\tttm_bo_kunmap(&new_map);\n\t\tttm_bo_kunmap(&old_map);\n\t}\n\n\t/* Unpin new buffer, and switch backup buffers. */\n\tvmw_bo_placement_set(buf,\n\t\t\t     VMW_BO_DOMAIN_MOB,\n\t\t\t     VMW_BO_DOMAIN_MOB);\n\tret = ttm_bo_validate(bo, &buf->placement, &ctx);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed validating new COTable backup buffer.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_detach(res);\n\tres->guest_memory_bo = buf;\n\tres->guest_memory_size = new_size;\n\tvcotbl->size_read_back = cur_size_read_back;\n\n\t/*\n\t * Now tell the device to switch. If this fails, then we need to\n\t * revert the full resize.\n\t */\n\tret = vmw_cotable_unscrub(res);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed switching COTable backup buffer.\\n\");\n\t\tres->guest_memory_bo = old_buf;\n\t\tres->guest_memory_size = old_size;\n\t\tvcotbl->size_read_back = old_size_read_back;\n\t\tvmw_resource_mob_attach(res);\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_attach(res);\n\t/* Let go of the old mob. */\n\tvmw_bo_unreference(&old_buf);\n\tres->id = vcotbl->type;\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tif (unlikely(ret))\n\t\tgoto out_wait;\n\n\t/* Release the pin acquired in vmw_bo_create */\n\tttm_bo_unpin(bo);\n\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn 0;\n\nout_map_new:\n\tttm_bo_kunmap(&old_map);\nout_wait:\n\tttm_bo_unpin(bo);\n\tttm_bo_unreserve(bo);\n\tvmw_bo_unreference(&buf);\n\nout_done:\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn ret;\n}",
        "code_after_change": "static int vmw_cotable_resize(struct vmw_resource *res, size_t new_size)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct vmw_cotable *vcotbl = vmw_cotable(res);\n\tstruct vmw_bo *buf, *old_buf = res->guest_memory_bo;\n\tstruct ttm_buffer_object *bo, *old_bo = &res->guest_memory_bo->tbo;\n\tsize_t old_size = res->guest_memory_size;\n\tsize_t old_size_read_back = vcotbl->size_read_back;\n\tsize_t cur_size_read_back;\n\tstruct ttm_bo_kmap_obj old_map, new_map;\n\tint ret;\n\tsize_t i;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = VMW_BO_DOMAIN_MOB,\n\t\t.busy_domain = VMW_BO_DOMAIN_MOB,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = new_size,\n\t\t.pin = true\n\t};\n\n\tMKS_STAT_TIME_DECL(MKSSTAT_KERN_COTABLE_RESIZE);\n\tMKS_STAT_TIME_PUSH(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\tret = vmw_cotable_readback(res);\n\tif (ret)\n\t\tgoto out_done;\n\n\tcur_size_read_back = vcotbl->size_read_back;\n\tvcotbl->size_read_back = old_size_read_back;\n\n\t/*\n\t * While device is processing, Allocate and reserve a buffer object\n\t * for the new COTable. Initially pin the buffer object to make sure\n\t * we can use tryreserve without failure.\n\t */\n\tret = vmw_gem_object_create(dev_priv, &bo_params, &buf);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed initializing new cotable MOB.\\n\");\n\t\tgoto out_done;\n\t}\n\n\tbo = &buf->tbo;\n\tWARN_ON_ONCE(ttm_bo_reserve(bo, false, true, NULL));\n\n\tret = ttm_bo_wait(old_bo, false, false);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed waiting for cotable unbind.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\t/*\n\t * Do a page by page copy of COTables. This eliminates slow vmap()s.\n\t * This should really be a TTM utility.\n\t */\n\tfor (i = 0; i < PFN_UP(old_bo->resource->size); ++i) {\n\t\tbool dummy;\n\n\t\tret = ttm_bo_kmap(old_bo, i, 1, &old_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping old COTable on resize.\\n\");\n\t\t\tgoto out_wait;\n\t\t}\n\t\tret = ttm_bo_kmap(bo, i, 1, &new_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping new COTable on resize.\\n\");\n\t\t\tgoto out_map_new;\n\t\t}\n\t\tmemcpy(ttm_kmap_obj_virtual(&new_map, &dummy),\n\t\t       ttm_kmap_obj_virtual(&old_map, &dummy),\n\t\t       PAGE_SIZE);\n\t\tttm_bo_kunmap(&new_map);\n\t\tttm_bo_kunmap(&old_map);\n\t}\n\n\t/* Unpin new buffer, and switch backup buffers. */\n\tvmw_bo_placement_set(buf,\n\t\t\t     VMW_BO_DOMAIN_MOB,\n\t\t\t     VMW_BO_DOMAIN_MOB);\n\tret = ttm_bo_validate(bo, &buf->placement, &ctx);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed validating new COTable backup buffer.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_detach(res);\n\tres->guest_memory_bo = buf;\n\tres->guest_memory_size = new_size;\n\tvcotbl->size_read_back = cur_size_read_back;\n\n\t/*\n\t * Now tell the device to switch. If this fails, then we need to\n\t * revert the full resize.\n\t */\n\tret = vmw_cotable_unscrub(res);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed switching COTable backup buffer.\\n\");\n\t\tres->guest_memory_bo = old_buf;\n\t\tres->guest_memory_size = old_size;\n\t\tvcotbl->size_read_back = old_size_read_back;\n\t\tvmw_resource_mob_attach(res);\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_attach(res);\n\t/* Let go of the old mob. */\n\tvmw_user_bo_unref(&old_buf);\n\tres->id = vcotbl->type;\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tif (unlikely(ret))\n\t\tgoto out_wait;\n\n\t/* Release the pin acquired in vmw_bo_create */\n\tttm_bo_unpin(bo);\n\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn 0;\n\nout_map_new:\n\tttm_bo_kunmap(&old_map);\nout_wait:\n\tttm_bo_unpin(bo);\n\tttm_bo_unreserve(bo);\n\tvmw_user_bo_unref(&buf);\n\nout_done:\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,7 +34,7 @@\n \t * for the new COTable. Initially pin the buffer object to make sure\n \t * we can use tryreserve without failure.\n \t */\n-\tret = vmw_bo_create(dev_priv, &bo_params, &buf);\n+\tret = vmw_gem_object_create(dev_priv, &bo_params, &buf);\n \tif (ret) {\n \t\tDRM_ERROR(\"Failed initializing new cotable MOB.\\n\");\n \t\tgoto out_done;\n@@ -104,7 +104,7 @@\n \n \tvmw_resource_mob_attach(res);\n \t/* Let go of the old mob. */\n-\tvmw_bo_unreference(&old_buf);\n+\tvmw_user_bo_unref(&old_buf);\n \tres->id = vcotbl->type;\n \n \tret = dma_resv_reserve_fences(bo->base.resv, 1);\n@@ -123,7 +123,7 @@\n out_wait:\n \tttm_bo_unpin(bo);\n \tttm_bo_unreserve(bo);\n-\tvmw_bo_unreference(&buf);\n+\tvmw_user_bo_unref(&buf);\n \n out_done:\n \tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);",
        "function_modified_lines": {
            "added": [
                "\tret = vmw_gem_object_create(dev_priv, &bo_params, &buf);",
                "\tvmw_user_bo_unref(&old_buf);",
                "\tvmw_user_bo_unref(&buf);"
            ],
            "deleted": [
                "\tret = vmw_bo_create(dev_priv, &bo_params, &buf);",
                "\tvmw_bo_unreference(&old_buf);",
                "\tvmw_bo_unreference(&buf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4271
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tvmw_user_bo_unref(vmw_bo);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tvmw_user_bo_unref(&vmw_bo);\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n \t\t\tatomic_dec(&vmw_bo->cpu_writers);\n \t\t}\n-\t\tvmw_user_bo_unref(vmw_bo);\n+\t\tvmw_user_bo_unref(&vmw_bo);\n \t}\n \n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "\t\tvmw_user_bo_unref(&vmw_bo);"
            ],
            "deleted": [
                "\t\tvmw_user_bo_unref(vmw_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4270
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tvmw_user_bo_unref(vmw_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo, *tmp_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\ttmp_bo = vmw_bo;\n\tvmw_user_bo_unref(&tmp_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t\t SVGAMobId *id,\n \t\t\t\t struct vmw_bo **vmw_bo_p)\n {\n-\tstruct vmw_bo *vmw_bo;\n+\tstruct vmw_bo *vmw_bo, *tmp_bo;\n \tuint32_t handle = *id;\n \tstruct vmw_relocation *reloc;\n \tint ret;\n@@ -16,7 +16,8 @@\n \t}\n \tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n-\tvmw_user_bo_unref(vmw_bo);\n+\ttmp_bo = vmw_bo;\n+\tvmw_user_bo_unref(&tmp_bo);\n \tif (unlikely(ret != 0))\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct vmw_bo *vmw_bo, *tmp_bo;",
                "\ttmp_bo = vmw_bo;",
                "\tvmw_user_bo_unref(&tmp_bo);"
            ],
            "deleted": [
                "\tstruct vmw_bo *vmw_bo;",
                "\tvmw_user_bo_unref(vmw_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4273
    },
    {
        "cve_id": "CVE-2019-25044",
        "code_before_change": "static void blk_mq_sched_tags_teardown(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_sched_free_tags(set, hctx, i);\n}",
        "code_after_change": "static void blk_mq_sched_tags_teardown(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (hctx->sched_tags) {\n\t\t\tblk_mq_free_rq_map(hctx->sched_tags);\n\t\t\thctx->sched_tags = NULL;\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,12 @@\n static void blk_mq_sched_tags_teardown(struct request_queue *q)\n {\n-\tstruct blk_mq_tag_set *set = q->tag_set;\n \tstruct blk_mq_hw_ctx *hctx;\n \tint i;\n \n-\tqueue_for_each_hw_ctx(q, hctx, i)\n-\t\tblk_mq_sched_free_tags(set, hctx, i);\n+\tqueue_for_each_hw_ctx(q, hctx, i) {\n+\t\tif (hctx->sched_tags) {\n+\t\t\tblk_mq_free_rq_map(hctx->sched_tags);\n+\t\t\thctx->sched_tags = NULL;\n+\t\t}\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tqueue_for_each_hw_ctx(q, hctx, i) {",
                "\t\tif (hctx->sched_tags) {",
                "\t\t\tblk_mq_free_rq_map(hctx->sched_tags);",
                "\t\t\thctx->sched_tags = NULL;",
                "\t\t}",
                "\t}"
            ],
            "deleted": [
                "\tstruct blk_mq_tag_set *set = q->tag_set;",
                "\tqueue_for_each_hw_ctx(q, hctx, i)",
                "\t\tblk_mq_sched_free_tags(set, hctx, i);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The block subsystem in the Linux kernel before 5.2 has a use-after-free that can lead to arbitrary code execution in the kernel context and privilege escalation, aka CID-c3e2219216c9. This is related to blk_mq_free_rqs and blk_cleanup_queue.",
        "id": 2299
    },
    {
        "cve_id": "CVE-2020-36387",
        "code_before_change": "static bool io_rw_reissue(struct io_kiocb *req, long res)\n{\n#ifdef CONFIG_BLOCK\n\tint ret;\n\n\tif ((res != -EAGAIN && res != -EOPNOTSUPP) || io_wq_current_is_worker())\n\t\treturn false;\n\n\tinit_task_work(&req->task_work, io_rw_resubmit);\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (!ret)\n\t\treturn true;\n#endif\n\treturn false;\n}",
        "code_after_change": "static bool io_rw_reissue(struct io_kiocb *req, long res)\n{\n#ifdef CONFIG_BLOCK\n\tint ret;\n\n\tif ((res != -EAGAIN && res != -EOPNOTSUPP) || io_wq_current_is_worker())\n\t\treturn false;\n\n\tinit_task_work(&req->task_work, io_rw_resubmit);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (!ret)\n\t\treturn true;\n#endif\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,8 @@\n \t\treturn false;\n \n \tinit_task_work(&req->task_work, io_rw_resubmit);\n+\tpercpu_ref_get(&req->ctx->refs);\n+\n \tret = io_req_task_work_add(req, &req->task_work);\n \tif (!ret)\n \t\treturn true;",
        "function_modified_lines": {
            "added": [
                "\tpercpu_ref_get(&req->ctx->refs);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.8.2. fs/io_uring.c has a use-after-free related to io_async_task_func and ctx reference holding, aka CID-6d816e088c35.",
        "id": 2760
    },
    {
        "cve_id": "CVE-2022-3424",
        "code_before_change": "int gru_handle_user_call_os(unsigned long cb)\n{\n\tstruct gru_tlb_fault_handle *tfh;\n\tstruct gru_thread_state *gts;\n\tvoid *cbk;\n\tint ucbnum, cbrnum, ret = -EINVAL;\n\n\tSTAT(call_os);\n\n\t/* sanity check the cb pointer */\n\tucbnum = get_cb_number((void *)cb);\n\tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n\t\treturn -EINVAL;\n\n\tgts = gru_find_lock_gts(cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n\t\tgoto exit;\n\n\tgru_check_context_placement(gts);\n\n\t/*\n\t * CCH may contain stale data if ts_force_cch_reload is set.\n\t */\n\tif (gts->ts_gru && gts->ts_force_cch_reload) {\n\t\tgts->ts_force_cch_reload = 0;\n\t\tgru_update_cch(gts);\n\t}\n\n\tret = -EAGAIN;\n\tcbrnum = thread_cbr_number(gts, ucbnum);\n\tif (gts->ts_gru) {\n\t\ttfh = get_tfh_by_index(gts->ts_gru, cbrnum);\n\t\tcbk = get_gseg_base_address_cb(gts->ts_gru->gs_gru_base_vaddr,\n\t\t\t\tgts->ts_ctxnum, ucbnum);\n\t\tret = gru_user_dropin(gts, tfh, cbk);\n\t}\nexit:\n\tgru_unlock_gts(gts);\n\treturn ret;\n}",
        "code_after_change": "int gru_handle_user_call_os(unsigned long cb)\n{\n\tstruct gru_tlb_fault_handle *tfh;\n\tstruct gru_thread_state *gts;\n\tvoid *cbk;\n\tint ucbnum, cbrnum, ret = -EINVAL;\n\n\tSTAT(call_os);\n\n\t/* sanity check the cb pointer */\n\tucbnum = get_cb_number((void *)cb);\n\tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n\t\treturn -EINVAL;\n\nagain:\n\tgts = gru_find_lock_gts(cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n\t\tgoto exit;\n\n\tif (gru_check_context_placement(gts)) {\n\t\tgru_unlock_gts(gts);\n\t\tgru_unload_context(gts, 1);\n\t\tgoto again;\n\t}\n\n\t/*\n\t * CCH may contain stale data if ts_force_cch_reload is set.\n\t */\n\tif (gts->ts_gru && gts->ts_force_cch_reload) {\n\t\tgts->ts_force_cch_reload = 0;\n\t\tgru_update_cch(gts);\n\t}\n\n\tret = -EAGAIN;\n\tcbrnum = thread_cbr_number(gts, ucbnum);\n\tif (gts->ts_gru) {\n\t\ttfh = get_tfh_by_index(gts->ts_gru, cbrnum);\n\t\tcbk = get_gseg_base_address_cb(gts->ts_gru->gs_gru_base_vaddr,\n\t\t\t\tgts->ts_ctxnum, ucbnum);\n\t\tret = gru_user_dropin(gts, tfh, cbk);\n\t}\nexit:\n\tgru_unlock_gts(gts);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,7 @@\n \tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n \t\treturn -EINVAL;\n \n+again:\n \tgts = gru_find_lock_gts(cb);\n \tif (!gts)\n \t\treturn -EINVAL;\n@@ -20,7 +21,11 @@\n \tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n \t\tgoto exit;\n \n-\tgru_check_context_placement(gts);\n+\tif (gru_check_context_placement(gts)) {\n+\t\tgru_unlock_gts(gts);\n+\t\tgru_unload_context(gts, 1);\n+\t\tgoto again;\n+\t}\n \n \t/*\n \t * CCH may contain stale data if ts_force_cch_reload is set.",
        "function_modified_lines": {
            "added": [
                "again:",
                "\tif (gru_check_context_placement(gts)) {",
                "\t\tgru_unlock_gts(gts);",
                "\t\tgru_unload_context(gts, 1);",
                "\t\tgoto again;",
                "\t}"
            ],
            "deleted": [
                "\tgru_check_context_placement(gts);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s SGI GRU driver in the way the first gru_file_unlocked_ioctl function is called by the user, where a fail pass occurs in the gru_check_chiplet_assignment function. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3595
    },
    {
        "cve_id": "CVE-2022-3424",
        "code_before_change": "int gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t/* Select blade/chiplet for GRU context */\n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tgru_check_context_placement(gts);\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t/* Register the current task as the GSEG owner */\n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t/* Set the CCH slice option */\n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}",
        "code_after_change": "int gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t/* Select blade/chiplet for GRU context */\n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tif (gru_check_context_placement(gts)) {\n\t\t\t\tgru_unlock_gts(gts);\n\t\t\t\tgru_unload_context(gts, 1);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t/* Register the current task as the GSEG owner */\n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t/* Set the CCH slice option */\n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,7 +26,11 @@\n \t\t} else {\n \t\t\tgts->ts_user_blade_id = req.val1;\n \t\t\tgts->ts_user_chiplet_id = req.val0;\n-\t\t\tgru_check_context_placement(gts);\n+\t\t\tif (gru_check_context_placement(gts)) {\n+\t\t\t\tgru_unlock_gts(gts);\n+\t\t\t\tgru_unload_context(gts, 1);\n+\t\t\t\treturn ret;\n+\t\t\t}\n \t\t}\n \t\tbreak;\n \tcase sco_gseg_owner:",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (gru_check_context_placement(gts)) {",
                "\t\t\t\tgru_unlock_gts(gts);",
                "\t\t\t\tgru_unload_context(gts, 1);",
                "\t\t\t\treturn ret;",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tgru_check_context_placement(gts);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s SGI GRU driver in the way the first gru_file_unlocked_ioctl function is called by the user, where a fail pass occurs in the gru_check_chiplet_assignment function. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3596
    },
    {
        "cve_id": "CVE-2021-43057",
        "code_before_change": "static int smack_getprocattr(struct task_struct *p, char *name, char **value)\n{\n\tstruct smack_known *skp = smk_of_task_struct_subj(p);\n\tchar *cp;\n\tint slen;\n\n\tif (strcmp(name, \"current\") != 0)\n\t\treturn -EINVAL;\n\n\tcp = kstrdup(skp->smk_known, GFP_KERNEL);\n\tif (cp == NULL)\n\t\treturn -ENOMEM;\n\n\tslen = strlen(cp);\n\t*value = cp;\n\treturn slen;\n}",
        "code_after_change": "static int smack_getprocattr(struct task_struct *p, char *name, char **value)\n{\n\tstruct smack_known *skp = smk_of_task_struct_obj(p);\n\tchar *cp;\n\tint slen;\n\n\tif (strcmp(name, \"current\") != 0)\n\t\treturn -EINVAL;\n\n\tcp = kstrdup(skp->smk_known, GFP_KERNEL);\n\tif (cp == NULL)\n\t\treturn -ENOMEM;\n\n\tslen = strlen(cp);\n\t*value = cp;\n\treturn slen;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static int smack_getprocattr(struct task_struct *p, char *name, char **value)\n {\n-\tstruct smack_known *skp = smk_of_task_struct_subj(p);\n+\tstruct smack_known *skp = smk_of_task_struct_obj(p);\n \tchar *cp;\n \tint slen;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct smack_known *skp = smk_of_task_struct_obj(p);"
            ],
            "deleted": [
                "\tstruct smack_known *skp = smk_of_task_struct_subj(p);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.14.8. A use-after-free in selinux_ptrace_traceme (aka the SELinux handler for PTRACE_TRACEME) could be used by local attackers to cause memory corruption and escalate privileges, aka CID-a3727a8bac0a. This occurs because of an attempt to access the subjective credentials of another task.",
        "id": 3161
    },
    {
        "cve_id": "CVE-2021-43057",
        "code_before_change": "static int selinux_ptrace_traceme(struct task_struct *parent)\n{\n\treturn avc_has_perm(&selinux_state,\n\t\t\t    task_sid_subj(parent), task_sid_obj(current),\n\t\t\t    SECCLASS_PROCESS, PROCESS__PTRACE, NULL);\n}",
        "code_after_change": "static int selinux_ptrace_traceme(struct task_struct *parent)\n{\n\treturn avc_has_perm(&selinux_state,\n\t\t\t    task_sid_obj(parent), task_sid_obj(current),\n\t\t\t    SECCLASS_PROCESS, PROCESS__PTRACE, NULL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static int selinux_ptrace_traceme(struct task_struct *parent)\n {\n \treturn avc_has_perm(&selinux_state,\n-\t\t\t    task_sid_subj(parent), task_sid_obj(current),\n+\t\t\t    task_sid_obj(parent), task_sid_obj(current),\n \t\t\t    SECCLASS_PROCESS, PROCESS__PTRACE, NULL);\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\t    task_sid_obj(parent), task_sid_obj(current),"
            ],
            "deleted": [
                "\t\t\t    task_sid_subj(parent), task_sid_obj(current),"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.14.8. A use-after-free in selinux_ptrace_traceme (aka the SELinux handler for PTRACE_TRACEME) could be used by local attackers to cause memory corruption and escalate privileges, aka CID-a3727a8bac0a. This occurs because of an attempt to access the subjective credentials of another task.",
        "id": 3159
    },
    {
        "cve_id": "CVE-2021-38204",
        "code_before_change": "static int\nmax3421_select_and_start_urb(struct usb_hcd *hcd)\n{\n\tstruct spi_device *spi = to_spi_device(hcd->self.controller);\n\tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n\tstruct urb *urb, *curr_urb = NULL;\n\tstruct max3421_ep *max3421_ep;\n\tint epnum, force_toggles = 0;\n\tstruct usb_host_endpoint *ep;\n\tstruct list_head *pos;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n\n\tfor (;\n\t     max3421_hcd->sched_pass < SCHED_PASS_DONE;\n\t     ++max3421_hcd->sched_pass)\n\t\tlist_for_each(pos, &max3421_hcd->ep_list) {\n\t\t\turb = NULL;\n\t\t\tmax3421_ep = container_of(pos, struct max3421_ep,\n\t\t\t\t\t\t  ep_list);\n\t\t\tep = max3421_ep->ep;\n\n\t\t\tswitch (usb_endpoint_type(&ep->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (max3421_hcd->sched_pass !=\n\t\t\t\t    SCHED_PASS_PERIODIC)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tif (max3421_hcd->sched_pass !=\n\t\t\t\t    SCHED_PASS_NON_PERIODIC)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (list_empty(&ep->urb_list))\n\t\t\t\tcontinue;\t/* nothing to do */\n\t\t\turb = list_first_entry(&ep->urb_list, struct urb,\n\t\t\t\t\t       urb_list);\n\t\t\tif (urb->unlinked) {\n\t\t\t\tdev_dbg(&spi->dev, \"%s: URB %p unlinked=%d\",\n\t\t\t\t\t__func__, urb, urb->unlinked);\n\t\t\t\tmax3421_hcd->curr_urb = urb;\n\t\t\t\tmax3421_hcd->urb_done = 1;\n\t\t\t\tspin_unlock_irqrestore(&max3421_hcd->lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tswitch (usb_endpoint_type(&ep->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\t\t\t/*\n\t\t\t\t * Allow one control transaction per\n\t\t\t\t * frame per endpoint:\n\t\t\t\t */\n\t\t\t\tif (frame_diff(max3421_ep->last_active,\n\t\t\t\t\t       max3421_hcd->frame_number) == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tif (max3421_ep->retransmit\n\t\t\t\t    && (frame_diff(max3421_ep->last_active,\n\t\t\t\t\t\t   max3421_hcd->frame_number)\n\t\t\t\t\t== 0))\n\t\t\t\t\t/*\n\t\t\t\t\t * We already tried this EP\n\t\t\t\t\t * during this frame and got a\n\t\t\t\t\t * NAK or error; wait for next frame\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (frame_diff(max3421_hcd->frame_number,\n\t\t\t\t\t       max3421_ep->last_active)\n\t\t\t\t    < urb->interval)\n\t\t\t\t\t/*\n\t\t\t\t\t * We already processed this\n\t\t\t\t\t * end-point in the current\n\t\t\t\t\t * frame\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* move current ep to tail: */\n\t\t\tlist_move_tail(pos, &max3421_hcd->ep_list);\n\t\t\tcurr_urb = urb;\n\t\t\tgoto done;\n\t\t}\ndone:\n\tif (!curr_urb) {\n\t\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\t\treturn 0;\n\t}\n\n\turb = max3421_hcd->curr_urb = curr_urb;\n\tepnum = usb_endpoint_num(&urb->ep->desc);\n\tif (max3421_ep->retransmit)\n\t\t/* restart (part of) a USB transaction: */\n\t\tmax3421_ep->retransmit = 0;\n\telse {\n\t\t/* start USB transaction: */\n\t\tif (usb_endpoint_xfer_control(&ep->desc)) {\n\t\t\t/*\n\t\t\t * See USB 2.0 spec section 8.6.1\n\t\t\t * Initialization via SETUP Token:\n\t\t\t */\n\t\t\tusb_settoggle(urb->dev, epnum, 0, 1);\n\t\t\tusb_settoggle(urb->dev, epnum, 1, 1);\n\t\t\tmax3421_ep->pkt_state = PKT_STATE_SETUP;\n\t\t\tforce_toggles = 1;\n\t\t} else\n\t\t\tmax3421_ep->pkt_state = PKT_STATE_TRANSFER;\n\t}\n\n\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\n\tmax3421_ep->last_active = max3421_hcd->frame_number;\n\tmax3421_set_address(hcd, urb->dev, epnum, force_toggles);\n\tmax3421_set_speed(hcd, urb->dev);\n\tmax3421_next_transfer(hcd, 0);\n\treturn 1;\n}",
        "code_after_change": "static int\nmax3421_select_and_start_urb(struct usb_hcd *hcd)\n{\n\tstruct spi_device *spi = to_spi_device(hcd->self.controller);\n\tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n\tstruct urb *urb, *curr_urb = NULL;\n\tstruct max3421_ep *max3421_ep;\n\tint epnum;\n\tstruct usb_host_endpoint *ep;\n\tstruct list_head *pos;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n\n\tfor (;\n\t     max3421_hcd->sched_pass < SCHED_PASS_DONE;\n\t     ++max3421_hcd->sched_pass)\n\t\tlist_for_each(pos, &max3421_hcd->ep_list) {\n\t\t\turb = NULL;\n\t\t\tmax3421_ep = container_of(pos, struct max3421_ep,\n\t\t\t\t\t\t  ep_list);\n\t\t\tep = max3421_ep->ep;\n\n\t\t\tswitch (usb_endpoint_type(&ep->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (max3421_hcd->sched_pass !=\n\t\t\t\t    SCHED_PASS_PERIODIC)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tif (max3421_hcd->sched_pass !=\n\t\t\t\t    SCHED_PASS_NON_PERIODIC)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (list_empty(&ep->urb_list))\n\t\t\t\tcontinue;\t/* nothing to do */\n\t\t\turb = list_first_entry(&ep->urb_list, struct urb,\n\t\t\t\t\t       urb_list);\n\t\t\tif (urb->unlinked) {\n\t\t\t\tdev_dbg(&spi->dev, \"%s: URB %p unlinked=%d\",\n\t\t\t\t\t__func__, urb, urb->unlinked);\n\t\t\t\tmax3421_hcd->curr_urb = urb;\n\t\t\t\tmax3421_hcd->urb_done = 1;\n\t\t\t\tspin_unlock_irqrestore(&max3421_hcd->lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tswitch (usb_endpoint_type(&ep->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\t\t\t/*\n\t\t\t\t * Allow one control transaction per\n\t\t\t\t * frame per endpoint:\n\t\t\t\t */\n\t\t\t\tif (frame_diff(max3421_ep->last_active,\n\t\t\t\t\t       max3421_hcd->frame_number) == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tif (max3421_ep->retransmit\n\t\t\t\t    && (frame_diff(max3421_ep->last_active,\n\t\t\t\t\t\t   max3421_hcd->frame_number)\n\t\t\t\t\t== 0))\n\t\t\t\t\t/*\n\t\t\t\t\t * We already tried this EP\n\t\t\t\t\t * during this frame and got a\n\t\t\t\t\t * NAK or error; wait for next frame\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (frame_diff(max3421_hcd->frame_number,\n\t\t\t\t\t       max3421_ep->last_active)\n\t\t\t\t    < urb->interval)\n\t\t\t\t\t/*\n\t\t\t\t\t * We already processed this\n\t\t\t\t\t * end-point in the current\n\t\t\t\t\t * frame\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* move current ep to tail: */\n\t\t\tlist_move_tail(pos, &max3421_hcd->ep_list);\n\t\t\tcurr_urb = urb;\n\t\t\tgoto done;\n\t\t}\ndone:\n\tif (!curr_urb) {\n\t\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\t\treturn 0;\n\t}\n\n\turb = max3421_hcd->curr_urb = curr_urb;\n\tepnum = usb_endpoint_num(&urb->ep->desc);\n\tif (max3421_ep->retransmit)\n\t\t/* restart (part of) a USB transaction: */\n\t\tmax3421_ep->retransmit = 0;\n\telse {\n\t\t/* start USB transaction: */\n\t\tif (usb_endpoint_xfer_control(&ep->desc)) {\n\t\t\t/*\n\t\t\t * See USB 2.0 spec section 8.6.1\n\t\t\t * Initialization via SETUP Token:\n\t\t\t */\n\t\t\tusb_settoggle(urb->dev, epnum, 0, 1);\n\t\t\tusb_settoggle(urb->dev, epnum, 1, 1);\n\t\t\tmax3421_ep->pkt_state = PKT_STATE_SETUP;\n\t\t} else\n\t\t\tmax3421_ep->pkt_state = PKT_STATE_TRANSFER;\n\t}\n\n\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\n\tmax3421_ep->last_active = max3421_hcd->frame_number;\n\tmax3421_set_address(hcd, urb->dev, epnum);\n\tmax3421_set_speed(hcd, urb->dev);\n\tmax3421_next_transfer(hcd, 0);\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n \tstruct urb *urb, *curr_urb = NULL;\n \tstruct max3421_ep *max3421_ep;\n-\tint epnum, force_toggles = 0;\n+\tint epnum;\n \tstruct usb_host_endpoint *ep;\n \tstruct list_head *pos;\n \tunsigned long flags;\n@@ -115,7 +115,6 @@\n \t\t\tusb_settoggle(urb->dev, epnum, 0, 1);\n \t\t\tusb_settoggle(urb->dev, epnum, 1, 1);\n \t\t\tmax3421_ep->pkt_state = PKT_STATE_SETUP;\n-\t\t\tforce_toggles = 1;\n \t\t} else\n \t\t\tmax3421_ep->pkt_state = PKT_STATE_TRANSFER;\n \t}\n@@ -123,7 +122,7 @@\n \tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n \n \tmax3421_ep->last_active = max3421_hcd->frame_number;\n-\tmax3421_set_address(hcd, urb->dev, epnum, force_toggles);\n+\tmax3421_set_address(hcd, urb->dev, epnum);\n \tmax3421_set_speed(hcd, urb->dev);\n \tmax3421_next_transfer(hcd, 0);\n \treturn 1;",
        "function_modified_lines": {
            "added": [
                "\tint epnum;",
                "\tmax3421_set_address(hcd, urb->dev, epnum);"
            ],
            "deleted": [
                "\tint epnum, force_toggles = 0;",
                "\t\t\tforce_toggles = 1;",
                "\tmax3421_set_address(hcd, urb->dev, epnum, force_toggles);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "drivers/usb/host/max3421-hcd.c in the Linux kernel before 5.13.6 allows physically proximate attackers to cause a denial of service (use-after-free and panic) by removing a MAX-3421 USB device in certain situations.",
        "id": 3079
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static vm_fault_t kvmppc_uvmem_migrate_to_ram(struct vm_fault *vmf)\n{\n\tstruct kvmppc_uvmem_page_pvt *pvt = vmf->page->zone_device_data;\n\n\tif (kvmppc_svm_page_out(vmf->vma, vmf->address,\n\t\t\t\tvmf->address + PAGE_SIZE, PAGE_SHIFT,\n\t\t\t\tpvt->kvm, pvt->gpa))\n\t\treturn VM_FAULT_SIGBUS;\n\telse\n\t\treturn 0;\n}",
        "code_after_change": "static vm_fault_t kvmppc_uvmem_migrate_to_ram(struct vm_fault *vmf)\n{\n\tstruct kvmppc_uvmem_page_pvt *pvt = vmf->page->zone_device_data;\n\n\tif (kvmppc_svm_page_out(vmf->vma, vmf->address,\n\t\t\t\tvmf->address + PAGE_SIZE, PAGE_SHIFT,\n\t\t\t\tpvt->kvm, pvt->gpa, vmf->page))\n\t\treturn VM_FAULT_SIGBUS;\n\telse\n\t\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \n \tif (kvmppc_svm_page_out(vmf->vma, vmf->address,\n \t\t\t\tvmf->address + PAGE_SIZE, PAGE_SHIFT,\n-\t\t\t\tpvt->kvm, pvt->gpa))\n+\t\t\t\tpvt->kvm, pvt->gpa, vmf->page))\n \t\treturn VM_FAULT_SIGBUS;\n \telse\n \t\treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tpvt->kvm, pvt->gpa, vmf->page))"
            ],
            "deleted": [
                "\t\t\t\tpvt->kvm, pvt->gpa))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3607
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "int\nsvm_range_restore_pages(struct amdgpu_device *adev, unsigned int pasid,\n\t\t\tuint64_t addr, bool write_fault)\n{\n\tstruct mm_struct *mm = NULL;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tktime_t timestamp = ktime_get_boottime();\n\tint32_t best_loc;\n\tint32_t gpuidx = MAX_GPU_INSTANCE;\n\tbool write_locked = false;\n\tstruct vm_area_struct *vma;\n\tbool migration = false;\n\tint r = 0;\n\n\tif (!KFD_IS_SVM_API_SUPPORTED(adev->kfd.dev)) {\n\t\tpr_debug(\"device does not support SVM\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tp = kfd_lookup_process_by_pasid(pasid);\n\tif (!p) {\n\t\tpr_debug(\"kfd process not founded pasid 0x%x\\n\", pasid);\n\t\treturn 0;\n\t}\n\tsvms = &p->svms;\n\n\tpr_debug(\"restoring svms 0x%p fault address 0x%llx\\n\", svms, addr);\n\n\tif (atomic_read(&svms->drain_pagefaults)) {\n\t\tpr_debug(\"draining retry fault, drop fault 0x%llx\\n\", addr);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!p->xnack_enabled) {\n\t\tpr_debug(\"XNACK not enabled for pasid 0x%x\\n\", pasid);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* p->lead_thread is available as kfd_process_wq_release flush the work\n\t * before releasing task ref.\n\t */\n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_debug(\"svms 0x%p failed to get mm\\n\", svms);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tmmap_read_lock(mm);\nretry_write_locked:\n\tmutex_lock(&svms->lock);\n\tprange = svm_range_from_addr(svms, addr, NULL);\n\tif (!prange) {\n\t\tpr_debug(\"failed to find prange svms 0x%p address [0x%llx]\\n\",\n\t\t\t svms, addr);\n\t\tif (!write_locked) {\n\t\t\t/* Need the write lock to create new range with MMU notifier.\n\t\t\t * Also flush pending deferred work to make sure the interval\n\t\t\t * tree is up to date before we add a new range\n\t\t\t */\n\t\t\tmutex_unlock(&svms->lock);\n\t\t\tmmap_read_unlock(mm);\n\t\t\tmmap_write_lock(mm);\n\t\t\twrite_locked = true;\n\t\t\tgoto retry_write_locked;\n\t\t}\n\t\tprange = svm_range_create_unregistered_range(adev, p, mm, addr);\n\t\tif (!prange) {\n\t\t\tpr_debug(\"failed to create unregistered range svms 0x%p address [0x%llx]\\n\",\n\t\t\t\t svms, addr);\n\t\t\tmmap_write_downgrade(mm);\n\t\t\tr = -EFAULT;\n\t\t\tgoto out_unlock_svms;\n\t\t}\n\t}\n\tif (write_locked)\n\t\tmmap_write_downgrade(mm);\n\n\tmutex_lock(&prange->migrate_mutex);\n\n\tif (svm_range_skip_recover(prange)) {\n\t\tamdgpu_gmc_filter_faults_remove(adev, addr, pasid);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t/* skip duplicate vm fault on different pages of same range */\n\tif (ktime_before(timestamp, ktime_add_ns(prange->validate_timestamp,\n\t\t\t\tAMDGPU_SVM_RANGE_RETRY_FAULT_PENDING))) {\n\t\tpr_debug(\"svms 0x%p [0x%lx %lx] already restored\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t/* __do_munmap removed VMA, return success as we are handling stale\n\t * retry fault.\n\t */\n\tvma = find_vma(mm, addr << PAGE_SHIFT);\n\tif (!vma || (addr << PAGE_SHIFT) < vma->vm_start) {\n\t\tpr_debug(\"address 0x%llx VMA is removed\\n\", addr);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\tif (!svm_fault_allowed(vma, write_fault)) {\n\t\tpr_debug(\"fault addr 0x%llx no %s permission\\n\", addr,\n\t\t\twrite_fault ? \"write\" : \"read\");\n\t\tr = -EPERM;\n\t\tgoto out_unlock_range;\n\t}\n\n\tbest_loc = svm_range_best_restore_location(prange, adev, &gpuidx);\n\tif (best_loc == -1) {\n\t\tpr_debug(\"svms %p failed get best restore loc [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = -EACCES;\n\t\tgoto out_unlock_range;\n\t}\n\n\tpr_debug(\"svms %p [0x%lx 0x%lx] best restore 0x%x, actual loc 0x%x\\n\",\n\t\t svms, prange->start, prange->last, best_loc,\n\t\t prange->actual_loc);\n\n\tkfd_smi_event_page_fault_start(adev->kfd.dev, p->lead_thread->pid, addr,\n\t\t\t\t       write_fault, timestamp);\n\n\tif (prange->actual_loc != best_loc) {\n\t\tmigration = true;\n\t\tif (best_loc) {\n\t\t\tr = svm_migrate_to_vram(prange, best_loc, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t\tif (r) {\n\t\t\t\tpr_debug(\"svm_migrate_to_vram failed (%d) at %llx, falling back to system memory\\n\",\n\t\t\t\t\t r, addr);\n\t\t\t\t/* Fallback to system memory if migration to\n\t\t\t\t * VRAM failed\n\t\t\t\t */\n\t\t\t\tif (prange->actual_loc)\n\t\t\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t\t\telse\n\t\t\t\t\tr = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t}\n\t\tif (r) {\n\t\t\tpr_debug(\"failed %d to migrate svms %p [0x%lx 0x%lx]\\n\",\n\t\t\t\t r, svms, prange->start, prange->last);\n\t\t\tgoto out_unlock_range;\n\t\t}\n\t}\n\n\tr = svm_range_validate_and_map(mm, prange, gpuidx, false, false, false);\n\tif (r)\n\t\tpr_debug(\"failed %d to map svms 0x%p [0x%lx 0x%lx] to gpus\\n\",\n\t\t\t r, svms, prange->start, prange->last);\n\n\tkfd_smi_event_page_fault_end(adev->kfd.dev, p->lead_thread->pid, addr,\n\t\t\t\t     migration);\n\nout_unlock_range:\n\tmutex_unlock(&prange->migrate_mutex);\nout_unlock_svms:\n\tmutex_unlock(&svms->lock);\n\tmmap_read_unlock(mm);\n\n\tsvm_range_count_fault(adev, p, gpuidx);\n\n\tmmput(mm);\nout:\n\tkfd_unref_process(p);\n\n\tif (r == -EAGAIN) {\n\t\tpr_debug(\"recover vm fault later\\n\");\n\t\tamdgpu_gmc_filter_faults_remove(adev, addr, pasid);\n\t\tr = 0;\n\t}\n\treturn r;\n}",
        "code_after_change": "int\nsvm_range_restore_pages(struct amdgpu_device *adev, unsigned int pasid,\n\t\t\tuint64_t addr, bool write_fault)\n{\n\tstruct mm_struct *mm = NULL;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tktime_t timestamp = ktime_get_boottime();\n\tint32_t best_loc;\n\tint32_t gpuidx = MAX_GPU_INSTANCE;\n\tbool write_locked = false;\n\tstruct vm_area_struct *vma;\n\tbool migration = false;\n\tint r = 0;\n\n\tif (!KFD_IS_SVM_API_SUPPORTED(adev->kfd.dev)) {\n\t\tpr_debug(\"device does not support SVM\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tp = kfd_lookup_process_by_pasid(pasid);\n\tif (!p) {\n\t\tpr_debug(\"kfd process not founded pasid 0x%x\\n\", pasid);\n\t\treturn 0;\n\t}\n\tsvms = &p->svms;\n\n\tpr_debug(\"restoring svms 0x%p fault address 0x%llx\\n\", svms, addr);\n\n\tif (atomic_read(&svms->drain_pagefaults)) {\n\t\tpr_debug(\"draining retry fault, drop fault 0x%llx\\n\", addr);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!p->xnack_enabled) {\n\t\tpr_debug(\"XNACK not enabled for pasid 0x%x\\n\", pasid);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* p->lead_thread is available as kfd_process_wq_release flush the work\n\t * before releasing task ref.\n\t */\n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_debug(\"svms 0x%p failed to get mm\\n\", svms);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tmmap_read_lock(mm);\nretry_write_locked:\n\tmutex_lock(&svms->lock);\n\tprange = svm_range_from_addr(svms, addr, NULL);\n\tif (!prange) {\n\t\tpr_debug(\"failed to find prange svms 0x%p address [0x%llx]\\n\",\n\t\t\t svms, addr);\n\t\tif (!write_locked) {\n\t\t\t/* Need the write lock to create new range with MMU notifier.\n\t\t\t * Also flush pending deferred work to make sure the interval\n\t\t\t * tree is up to date before we add a new range\n\t\t\t */\n\t\t\tmutex_unlock(&svms->lock);\n\t\t\tmmap_read_unlock(mm);\n\t\t\tmmap_write_lock(mm);\n\t\t\twrite_locked = true;\n\t\t\tgoto retry_write_locked;\n\t\t}\n\t\tprange = svm_range_create_unregistered_range(adev, p, mm, addr);\n\t\tif (!prange) {\n\t\t\tpr_debug(\"failed to create unregistered range svms 0x%p address [0x%llx]\\n\",\n\t\t\t\t svms, addr);\n\t\t\tmmap_write_downgrade(mm);\n\t\t\tr = -EFAULT;\n\t\t\tgoto out_unlock_svms;\n\t\t}\n\t}\n\tif (write_locked)\n\t\tmmap_write_downgrade(mm);\n\n\tmutex_lock(&prange->migrate_mutex);\n\n\tif (svm_range_skip_recover(prange)) {\n\t\tamdgpu_gmc_filter_faults_remove(adev, addr, pasid);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t/* skip duplicate vm fault on different pages of same range */\n\tif (ktime_before(timestamp, ktime_add_ns(prange->validate_timestamp,\n\t\t\t\tAMDGPU_SVM_RANGE_RETRY_FAULT_PENDING))) {\n\t\tpr_debug(\"svms 0x%p [0x%lx %lx] already restored\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t/* __do_munmap removed VMA, return success as we are handling stale\n\t * retry fault.\n\t */\n\tvma = find_vma(mm, addr << PAGE_SHIFT);\n\tif (!vma || (addr << PAGE_SHIFT) < vma->vm_start) {\n\t\tpr_debug(\"address 0x%llx VMA is removed\\n\", addr);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\tif (!svm_fault_allowed(vma, write_fault)) {\n\t\tpr_debug(\"fault addr 0x%llx no %s permission\\n\", addr,\n\t\t\twrite_fault ? \"write\" : \"read\");\n\t\tr = -EPERM;\n\t\tgoto out_unlock_range;\n\t}\n\n\tbest_loc = svm_range_best_restore_location(prange, adev, &gpuidx);\n\tif (best_loc == -1) {\n\t\tpr_debug(\"svms %p failed get best restore loc [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = -EACCES;\n\t\tgoto out_unlock_range;\n\t}\n\n\tpr_debug(\"svms %p [0x%lx 0x%lx] best restore 0x%x, actual loc 0x%x\\n\",\n\t\t svms, prange->start, prange->last, best_loc,\n\t\t prange->actual_loc);\n\n\tkfd_smi_event_page_fault_start(adev->kfd.dev, p->lead_thread->pid, addr,\n\t\t\t\t       write_fault, timestamp);\n\n\tif (prange->actual_loc != best_loc) {\n\t\tmigration = true;\n\t\tif (best_loc) {\n\t\t\tr = svm_migrate_to_vram(prange, best_loc, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t\tif (r) {\n\t\t\t\tpr_debug(\"svm_migrate_to_vram failed (%d) at %llx, falling back to system memory\\n\",\n\t\t\t\t\t r, addr);\n\t\t\t\t/* Fallback to system memory if migration to\n\t\t\t\t * VRAM failed\n\t\t\t\t */\n\t\t\t\tif (prange->actual_loc)\n\t\t\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n\t\t\t\t\t   NULL);\n\t\t\t\telse\n\t\t\t\t\tr = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n\t\t\t\t\tNULL);\n\t\t}\n\t\tif (r) {\n\t\t\tpr_debug(\"failed %d to migrate svms %p [0x%lx 0x%lx]\\n\",\n\t\t\t\t r, svms, prange->start, prange->last);\n\t\t\tgoto out_unlock_range;\n\t\t}\n\t}\n\n\tr = svm_range_validate_and_map(mm, prange, gpuidx, false, false, false);\n\tif (r)\n\t\tpr_debug(\"failed %d to map svms 0x%p [0x%lx 0x%lx] to gpus\\n\",\n\t\t\t r, svms, prange->start, prange->last);\n\n\tkfd_smi_event_page_fault_end(adev->kfd.dev, p->lead_thread->pid, addr,\n\t\t\t\t     migration);\n\nout_unlock_range:\n\tmutex_unlock(&prange->migrate_mutex);\nout_unlock_svms:\n\tmutex_unlock(&svms->lock);\n\tmmap_read_unlock(mm);\n\n\tsvm_range_count_fault(adev, p, gpuidx);\n\n\tmmput(mm);\nout:\n\tkfd_unref_process(p);\n\n\tif (r == -EAGAIN) {\n\t\tpr_debug(\"recover vm fault later\\n\");\n\t\tamdgpu_gmc_filter_faults_remove(adev, addr, pasid);\n\t\tr = 0;\n\t}\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -142,13 +142,15 @@\n \t\t\t\t */\n \t\t\t\tif (prange->actual_loc)\n \t\t\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n-\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n+\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n+\t\t\t\t\t   NULL);\n \t\t\t\telse\n \t\t\t\t\tr = 0;\n \t\t\t}\n \t\t} else {\n \t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n-\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n+\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n+\t\t\t\t\tNULL);\n \t\t}\n \t\tif (r) {\n \t\t\tpr_debug(\"failed %d to migrate svms %p [0x%lx 0x%lx]\\n\",",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,",
                "\t\t\t\t\t   NULL);",
                "\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,",
                "\t\t\t\t\tNULL);"
            ],
            "deleted": [
                "\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);",
                "\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3614
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static vm_fault_t dmirror_devmem_fault(struct vm_fault *vmf)\n{\n\tstruct migrate_vma args;\n\tunsigned long src_pfns = 0;\n\tunsigned long dst_pfns = 0;\n\tstruct page *rpage;\n\tstruct dmirror *dmirror;\n\tvm_fault_t ret;\n\n\t/*\n\t * Normally, a device would use the page->zone_device_data to point to\n\t * the mirror but here we use it to hold the page for the simulated\n\t * device memory and that page holds the pointer to the mirror.\n\t */\n\trpage = vmf->page->zone_device_data;\n\tdmirror = rpage->zone_device_data;\n\n\t/* FIXME demonstrate how we can adjust migrate range */\n\targs.vma = vmf->vma;\n\targs.start = vmf->address;\n\targs.end = args.start + PAGE_SIZE;\n\targs.src = &src_pfns;\n\targs.dst = &dst_pfns;\n\targs.pgmap_owner = dmirror->mdevice;\n\targs.flags = dmirror_select_device(dmirror);\n\n\tif (migrate_vma_setup(&args))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tret = dmirror_devmem_fault_alloc_and_copy(&args, dmirror);\n\tif (ret)\n\t\treturn ret;\n\tmigrate_vma_pages(&args);\n\t/*\n\t * No device finalize step is needed since\n\t * dmirror_devmem_fault_alloc_and_copy() will have already\n\t * invalidated the device page table.\n\t */\n\tmigrate_vma_finalize(&args);\n\treturn 0;\n}",
        "code_after_change": "static vm_fault_t dmirror_devmem_fault(struct vm_fault *vmf)\n{\n\tstruct migrate_vma args = { 0 };\n\tunsigned long src_pfns = 0;\n\tunsigned long dst_pfns = 0;\n\tstruct page *rpage;\n\tstruct dmirror *dmirror;\n\tvm_fault_t ret;\n\n\t/*\n\t * Normally, a device would use the page->zone_device_data to point to\n\t * the mirror but here we use it to hold the page for the simulated\n\t * device memory and that page holds the pointer to the mirror.\n\t */\n\trpage = vmf->page->zone_device_data;\n\tdmirror = rpage->zone_device_data;\n\n\t/* FIXME demonstrate how we can adjust migrate range */\n\targs.vma = vmf->vma;\n\targs.start = vmf->address;\n\targs.end = args.start + PAGE_SIZE;\n\targs.src = &src_pfns;\n\targs.dst = &dst_pfns;\n\targs.pgmap_owner = dmirror->mdevice;\n\targs.flags = dmirror_select_device(dmirror);\n\targs.fault_page = vmf->page;\n\n\tif (migrate_vma_setup(&args))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tret = dmirror_devmem_fault_alloc_and_copy(&args, dmirror);\n\tif (ret)\n\t\treturn ret;\n\tmigrate_vma_pages(&args);\n\t/*\n\t * No device finalize step is needed since\n\t * dmirror_devmem_fault_alloc_and_copy() will have already\n\t * invalidated the device page table.\n\t */\n\tmigrate_vma_finalize(&args);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static vm_fault_t dmirror_devmem_fault(struct vm_fault *vmf)\n {\n-\tstruct migrate_vma args;\n+\tstruct migrate_vma args = { 0 };\n \tunsigned long src_pfns = 0;\n \tunsigned long dst_pfns = 0;\n \tstruct page *rpage;\n@@ -23,6 +23,7 @@\n \targs.dst = &dst_pfns;\n \targs.pgmap_owner = dmirror->mdevice;\n \targs.flags = dmirror_select_device(dmirror);\n+\targs.fault_page = vmf->page;\n \n \tif (migrate_vma_setup(&args))\n \t\treturn VM_FAULT_SIGBUS;",
        "function_modified_lines": {
            "added": [
                "\tstruct migrate_vma args = { 0 };",
                "\targs.fault_page = vmf->page;"
            ],
            "deleted": [
                "\tstruct migrate_vma args;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3617
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "int migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\tstruct folio *src, enum migrate_mode mode)\n{\n\tint rc;\n\n\tBUG_ON(folio_test_writeback(src));\t/* Writeback must be complete */\n\n\trc = folio_migrate_mapping(mapping, dst, src, 0);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\tif (mode != MIGRATE_SYNC_NO_COPY)\n\t\tfolio_migrate_copy(dst, src);\n\telse\n\t\tfolio_migrate_flags(dst, src);\n\treturn MIGRATEPAGE_SUCCESS;\n}",
        "code_after_change": "int migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\tstruct folio *src, enum migrate_mode mode)\n{\n\treturn migrate_folio_extra(mapping, dst, src, mode, 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,18 +1,5 @@\n int migrate_folio(struct address_space *mapping, struct folio *dst,\n \t\tstruct folio *src, enum migrate_mode mode)\n {\n-\tint rc;\n-\n-\tBUG_ON(folio_test_writeback(src));\t/* Writeback must be complete */\n-\n-\trc = folio_migrate_mapping(mapping, dst, src, 0);\n-\n-\tif (rc != MIGRATEPAGE_SUCCESS)\n-\t\treturn rc;\n-\n-\tif (mode != MIGRATE_SYNC_NO_COPY)\n-\t\tfolio_migrate_copy(dst, src);\n-\telse\n-\t\tfolio_migrate_flags(dst, src);\n-\treturn MIGRATEPAGE_SUCCESS;\n+\treturn migrate_folio_extra(mapping, dst, src, mode, 0);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn migrate_folio_extra(mapping, dst, src, mode, 0);"
            ],
            "deleted": [
                "\tint rc;",
                "",
                "\tBUG_ON(folio_test_writeback(src));\t/* Writeback must be complete */",
                "",
                "\trc = folio_migrate_mapping(mapping, dst, src, 0);",
                "",
                "\tif (rc != MIGRATEPAGE_SUCCESS)",
                "\t\treturn rc;",
                "",
                "\tif (mode != MIGRATE_SYNC_NO_COPY)",
                "\t\tfolio_migrate_copy(dst, src);",
                "\telse",
                "\t\tfolio_migrate_flags(dst, src);",
                "\treturn MIGRATEPAGE_SUCCESS;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3619
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static void migrate_vma_unmap(struct migrate_vma *migrate)\n{\n\tconst unsigned long npages = migrate->npages;\n\tunsigned long i, restore = 0;\n\tbool allow_drain = true;\n\n\tlru_add_drain();\n\n\tfor (i = 0; i < npages; i++) {\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct folio *folio;\n\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* ZONE_DEVICE pages are not on LRU */\n\t\tif (!is_zone_device_page(page)) {\n\t\t\tif (!PageLRU(page) && allow_drain) {\n\t\t\t\t/* Drain CPU's pagevec */\n\t\t\t\tlru_add_drain_all();\n\t\t\t\tallow_drain = false;\n\t\t\t}\n\n\t\t\tif (isolate_lru_page(page)) {\n\t\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\t\tmigrate->cpages--;\n\t\t\t\trestore++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Drop the reference we took in collect */\n\t\t\tput_page(page);\n\t\t}\n\n\t\tfolio = page_folio(page);\n\t\tif (folio_mapped(folio))\n\t\t\ttry_to_migrate(folio, 0);\n\n\t\tif (page_mapped(page) || !migrate_vma_check_page(page)) {\n\t\t\tif (!is_zone_device_page(page)) {\n\t\t\t\tget_page(page);\n\t\t\t\tputback_lru_page(page);\n\t\t\t}\n\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\tmigrate->cpages--;\n\t\t\trestore++;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tfor (i = 0; i < npages && restore; i++) {\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct folio *folio;\n\n\t\tif (!page || (migrate->src[i] & MIGRATE_PFN_MIGRATE))\n\t\t\tcontinue;\n\n\t\tfolio = page_folio(page);\n\t\tremove_migration_ptes(folio, folio, false);\n\n\t\tmigrate->src[i] = 0;\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\trestore--;\n\t}\n}",
        "code_after_change": "static void migrate_vma_unmap(struct migrate_vma *migrate)\n{\n\tconst unsigned long npages = migrate->npages;\n\tunsigned long i, restore = 0;\n\tbool allow_drain = true;\n\n\tlru_add_drain();\n\n\tfor (i = 0; i < npages; i++) {\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct folio *folio;\n\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* ZONE_DEVICE pages are not on LRU */\n\t\tif (!is_zone_device_page(page)) {\n\t\t\tif (!PageLRU(page) && allow_drain) {\n\t\t\t\t/* Drain CPU's pagevec */\n\t\t\t\tlru_add_drain_all();\n\t\t\t\tallow_drain = false;\n\t\t\t}\n\n\t\t\tif (isolate_lru_page(page)) {\n\t\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\t\tmigrate->cpages--;\n\t\t\t\trestore++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Drop the reference we took in collect */\n\t\t\tput_page(page);\n\t\t}\n\n\t\tfolio = page_folio(page);\n\t\tif (folio_mapped(folio))\n\t\t\ttry_to_migrate(folio, 0);\n\n\t\tif (page_mapped(page) ||\n\t\t    !migrate_vma_check_page(page, migrate->fault_page)) {\n\t\t\tif (!is_zone_device_page(page)) {\n\t\t\t\tget_page(page);\n\t\t\t\tputback_lru_page(page);\n\t\t\t}\n\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\tmigrate->cpages--;\n\t\t\trestore++;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tfor (i = 0; i < npages && restore; i++) {\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct folio *folio;\n\n\t\tif (!page || (migrate->src[i] & MIGRATE_PFN_MIGRATE))\n\t\t\tcontinue;\n\n\t\tfolio = page_folio(page);\n\t\tremove_migration_ptes(folio, folio, false);\n\n\t\tmigrate->src[i] = 0;\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\trestore--;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,7 +36,8 @@\n \t\tif (folio_mapped(folio))\n \t\t\ttry_to_migrate(folio, 0);\n \n-\t\tif (page_mapped(page) || !migrate_vma_check_page(page)) {\n+\t\tif (page_mapped(page) ||\n+\t\t    !migrate_vma_check_page(page, migrate->fault_page)) {\n \t\t\tif (!is_zone_device_page(page)) {\n \t\t\t\tget_page(page);\n \t\t\t\tputback_lru_page(page);",
        "function_modified_lines": {
            "added": [
                "\t\tif (page_mapped(page) ||",
                "\t\t    !migrate_vma_check_page(page, migrate->fault_page)) {"
            ],
            "deleted": [
                "\t\tif (page_mapped(page) || !migrate_vma_check_page(page)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3622
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "vm_fault_t do_swap_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct folio *swapcache, *folio = NULL;\n\tstruct page *page;\n\tstruct swap_info_struct *si = NULL;\n\trmap_t rmap_flags = RMAP_NONE;\n\tbool exclusive = false;\n\tswp_entry_t entry;\n\tpte_t pte;\n\tint locked;\n\tvm_fault_t ret = 0;\n\tvoid *shadow = NULL;\n\n\tif (!pte_unmap_same(vmf))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(vmf->orig_pte);\n\tif (unlikely(non_swap_entry(entry))) {\n\t\tif (is_migration_entry(entry)) {\n\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,\n\t\t\t\t\t     vmf->address);\n\t\t} else if (is_device_exclusive_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tret = remove_device_exclusive_entry(vmf);\n\t\t} else if (is_device_private_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);\n\t\t} else if (is_hwpoison_entry(entry)) {\n\t\t\tret = VM_FAULT_HWPOISON;\n\t\t} else if (is_swapin_error_entry(entry)) {\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t} else if (is_pte_marker_entry(entry)) {\n\t\t\tret = handle_pte_marker(vmf);\n\t\t} else {\n\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Prevent swapoff from happening to us. */\n\tsi = get_swap_device(entry);\n\tif (unlikely(!si))\n\t\tgoto out;\n\n\tfolio = swap_cache_get_folio(entry, vma, vmf->address);\n\tif (folio)\n\t\tpage = folio_file_page(folio, swp_offset(entry));\n\tswapcache = folio;\n\n\tif (!folio) {\n\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n\t\t    __swap_count(entry) == 1) {\n\t\t\t/* skip swapcache */\n\t\t\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0,\n\t\t\t\t\t\tvma, vmf->address, false);\n\t\t\tpage = &folio->page;\n\t\t\tif (folio) {\n\t\t\t\t__folio_set_locked(folio);\n\t\t\t\t__folio_set_swapbacked(folio);\n\n\t\t\t\tif (mem_cgroup_swapin_charge_folio(folio,\n\t\t\t\t\t\t\tvma->vm_mm, GFP_KERNEL,\n\t\t\t\t\t\t\tentry)) {\n\t\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\t\tgoto out_page;\n\t\t\t\t}\n\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry);\n\n\t\t\t\tshadow = get_shadow_from_swap_cache(entry);\n\t\t\t\tif (shadow)\n\t\t\t\t\tworkingset_refault(folio, shadow);\n\n\t\t\t\tfolio_add_lru(folio);\n\n\t\t\t\t/* To provide entry to swap_readpage() */\n\t\t\t\tfolio_set_swap_entry(folio, entry);\n\t\t\t\tswap_readpage(page, true, NULL);\n\t\t\t\tfolio->private = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n\t\t\t\t\t\tvmf);\n\t\t\tif (page)\n\t\t\t\tfolio = page_folio(page);\n\t\t\tswapcache = folio;\n\t\t}\n\n\t\tif (!folio) {\n\t\t\t/*\n\t\t\t * Back out if somebody else faulted in this pte\n\t\t\t * while we released the pte lock.\n\t\t\t */\n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (likely(pte_same(*vmf->pte, vmf->orig_pte)))\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t/* Had to read the page from swap area: Major fault */\n\t\tret = VM_FAULT_MAJOR;\n\t\tcount_vm_event(PGMAJFAULT);\n\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);\n\t} else if (PageHWPoison(page)) {\n\t\t/*\n\t\t * hwpoisoned dirty swapcache pages are kept for killing\n\t\t * owner processes (which may be unknown at hwpoison time)\n\t\t */\n\t\tret = VM_FAULT_HWPOISON;\n\t\tgoto out_release;\n\t}\n\n\tlocked = folio_lock_or_retry(folio, vma->vm_mm, vmf->flags);\n\n\tif (!locked) {\n\t\tret |= VM_FAULT_RETRY;\n\t\tgoto out_release;\n\t}\n\n\tif (swapcache) {\n\t\t/*\n\t\t * Make sure folio_free_swap() or swapoff did not release the\n\t\t * swapcache from under us.  The page pin, and pte_same test\n\t\t * below, are not enough to exclude that.  Even if it is still\n\t\t * swapcache, we need to check that the page's swap has not\n\t\t * changed.\n\t\t */\n\t\tif (unlikely(!folio_test_swapcache(folio) ||\n\t\t\t     page_private(page) != entry.val))\n\t\t\tgoto out_page;\n\n\t\t/*\n\t\t * KSM sometimes has to copy on read faults, for example, if\n\t\t * page->index of !PageKSM() pages would be nonlinear inside the\n\t\t * anon VMA -- PageKSM() is lost on actual swapout.\n\t\t */\n\t\tpage = ksm_might_need_to_copy(page, vma, vmf->address);\n\t\tif (unlikely(!page)) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_page;\n\t\t}\n\t\tfolio = page_folio(page);\n\n\t\t/*\n\t\t * If we want to map a page that's in the swapcache writable, we\n\t\t * have to detect via the refcount if we're really the exclusive\n\t\t * owner. Try removing the extra reference from the local LRU\n\t\t * pagevecs if required.\n\t\t */\n\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&\n\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))\n\t\t\tlru_add_drain();\n\t}\n\n\tcgroup_throttle_swaprate(page, GFP_KERNEL);\n\n\t/*\n\t * Back out if somebody else already faulted in this pte.\n\t */\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte)))\n\t\tgoto out_nomap;\n\n\tif (unlikely(!folio_test_uptodate(folio))) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_nomap;\n\t}\n\n\t/*\n\t * PG_anon_exclusive reuses PG_mappedtodisk for anon pages. A swap pte\n\t * must never point at an anonymous page in the swapcache that is\n\t * PG_anon_exclusive. Sanity check that this holds and especially, that\n\t * no filesystem set PG_mappedtodisk on a page in the swapcache. Sanity\n\t * check after taking the PT lock and making sure that nobody\n\t * concurrently faulted in this page and set PG_anon_exclusive.\n\t */\n\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));\n\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));\n\n\t/*\n\t * Check under PT lock (to protect against concurrent fork() sharing\n\t * the swap entry concurrently) for certainly exclusive pages.\n\t */\n\tif (!folio_test_ksm(folio)) {\n\t\t/*\n\t\t * Note that pte_swp_exclusive() == false for architectures\n\t\t * without __HAVE_ARCH_PTE_SWP_EXCLUSIVE.\n\t\t */\n\t\texclusive = pte_swp_exclusive(vmf->orig_pte);\n\t\tif (folio != swapcache) {\n\t\t\t/*\n\t\t\t * We have a fresh page that is not exposed to the\n\t\t\t * swapcache -> certainly exclusive.\n\t\t\t */\n\t\t\texclusive = true;\n\t\t} else if (exclusive && folio_test_writeback(folio) &&\n\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {\n\t\t\t/*\n\t\t\t * This is tricky: not all swap backends support\n\t\t\t * concurrent page modifications while under writeback.\n\t\t\t *\n\t\t\t * So if we stumble over such a page in the swapcache\n\t\t\t * we must not set the page exclusive, otherwise we can\n\t\t\t * map it writable without further checks and modify it\n\t\t\t * while still under writeback.\n\t\t\t *\n\t\t\t * For these problematic swap backends, simply drop the\n\t\t\t * exclusive marker: this is perfectly fine as we start\n\t\t\t * writeback only if we fully unmapped the page and\n\t\t\t * there are no unexpected references on the page after\n\t\t\t * unmapping succeeded. After fully unmapped, no\n\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can\n\t\t\t * appear, so dropping the exclusive marker and mapping\n\t\t\t * it only R/O is fine.\n\t\t\t */\n\t\t\texclusive = false;\n\t\t}\n\t}\n\n\t/*\n\t * Remove the swap entry and conditionally try to free up the swapcache.\n\t * We're already holding a reference on the page but haven't mapped it\n\t * yet.\n\t */\n\tswap_free(entry);\n\tif (should_try_to_free_swap(folio, vma, vmf->flags))\n\t\tfolio_free_swap(folio);\n\n\tinc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);\n\tdec_mm_counter_fast(vma->vm_mm, MM_SWAPENTS);\n\tpte = mk_pte(page, vma->vm_page_prot);\n\n\t/*\n\t * Same logic as in do_wp_page(); however, optimize for pages that are\n\t * certainly not shared either because we just allocated them without\n\t * exposing them to the swapcache or because the swap entry indicates\n\t * exclusivity.\n\t */\n\tif (!folio_test_ksm(folio) &&\n\t    (exclusive || folio_ref_count(folio) == 1)) {\n\t\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);\n\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;\n\t\t\tret |= VM_FAULT_WRITE;\n\t\t}\n\t\trmap_flags |= RMAP_EXCLUSIVE;\n\t}\n\tflush_icache_page(vma, page);\n\tif (pte_swp_soft_dirty(vmf->orig_pte))\n\t\tpte = pte_mksoft_dirty(pte);\n\tif (pte_swp_uffd_wp(vmf->orig_pte)) {\n\t\tpte = pte_mkuffd_wp(pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\tvmf->orig_pte = pte;\n\n\t/* ksm created a completely new copy */\n\tif (unlikely(folio != swapcache && swapcache)) {\n\t\tpage_add_new_anon_rmap(page, vma, vmf->address);\n\t\tfolio_add_lru_vma(folio, vma);\n\t} else {\n\t\tpage_add_anon_rmap(page, vma, vmf->address, rmap_flags);\n\t}\n\n\tVM_BUG_ON(!folio_test_anon(folio) ||\n\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);\n\tarch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);\n\n\tfolio_unlock(folio);\n\tif (folio != swapcache && swapcache) {\n\t\t/*\n\t\t * Hold the lock to avoid the swap entry to be reused\n\t\t * until we take the PT lock for the pte_same() check\n\t\t * (to avoid false positives from pte_same). For\n\t\t * further safety release the lock after the swap_free\n\t\t * so that the swap count won't change under a\n\t\t * parallel locked swapcache.\n\t\t */\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\n\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\tret |= do_wp_page(vmf);\n\t\tif (ret & VM_FAULT_ERROR)\n\t\t\tret &= VM_FAULT_ERROR;\n\t\tgoto out;\n\t}\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\nunlock:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout:\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\nout_nomap:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout_page:\n\tfolio_unlock(folio);\nout_release:\n\tfolio_put(folio);\n\tif (folio != swapcache && swapcache) {\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\n}",
        "code_after_change": "vm_fault_t do_swap_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct folio *swapcache, *folio = NULL;\n\tstruct page *page;\n\tstruct swap_info_struct *si = NULL;\n\trmap_t rmap_flags = RMAP_NONE;\n\tbool exclusive = false;\n\tswp_entry_t entry;\n\tpte_t pte;\n\tint locked;\n\tvm_fault_t ret = 0;\n\tvoid *shadow = NULL;\n\n\tif (!pte_unmap_same(vmf))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(vmf->orig_pte);\n\tif (unlikely(non_swap_entry(entry))) {\n\t\tif (is_migration_entry(entry)) {\n\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,\n\t\t\t\t\t     vmf->address);\n\t\t} else if (is_device_exclusive_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tret = remove_device_exclusive_entry(vmf);\n\t\t} else if (is_device_private_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {\n\t\t\t\tspin_unlock(vmf->ptl);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Get a page reference while we know the page can't be\n\t\t\t * freed.\n\t\t\t */\n\t\t\tget_page(vmf->page);\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t\tvmf->page->pgmap->ops->migrate_to_ram(vmf);\n\t\t\tput_page(vmf->page);\n\t\t} else if (is_hwpoison_entry(entry)) {\n\t\t\tret = VM_FAULT_HWPOISON;\n\t\t} else if (is_swapin_error_entry(entry)) {\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t} else if (is_pte_marker_entry(entry)) {\n\t\t\tret = handle_pte_marker(vmf);\n\t\t} else {\n\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Prevent swapoff from happening to us. */\n\tsi = get_swap_device(entry);\n\tif (unlikely(!si))\n\t\tgoto out;\n\n\tfolio = swap_cache_get_folio(entry, vma, vmf->address);\n\tif (folio)\n\t\tpage = folio_file_page(folio, swp_offset(entry));\n\tswapcache = folio;\n\n\tif (!folio) {\n\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n\t\t    __swap_count(entry) == 1) {\n\t\t\t/* skip swapcache */\n\t\t\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0,\n\t\t\t\t\t\tvma, vmf->address, false);\n\t\t\tpage = &folio->page;\n\t\t\tif (folio) {\n\t\t\t\t__folio_set_locked(folio);\n\t\t\t\t__folio_set_swapbacked(folio);\n\n\t\t\t\tif (mem_cgroup_swapin_charge_folio(folio,\n\t\t\t\t\t\t\tvma->vm_mm, GFP_KERNEL,\n\t\t\t\t\t\t\tentry)) {\n\t\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\t\tgoto out_page;\n\t\t\t\t}\n\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry);\n\n\t\t\t\tshadow = get_shadow_from_swap_cache(entry);\n\t\t\t\tif (shadow)\n\t\t\t\t\tworkingset_refault(folio, shadow);\n\n\t\t\t\tfolio_add_lru(folio);\n\n\t\t\t\t/* To provide entry to swap_readpage() */\n\t\t\t\tfolio_set_swap_entry(folio, entry);\n\t\t\t\tswap_readpage(page, true, NULL);\n\t\t\t\tfolio->private = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n\t\t\t\t\t\tvmf);\n\t\t\tif (page)\n\t\t\t\tfolio = page_folio(page);\n\t\t\tswapcache = folio;\n\t\t}\n\n\t\tif (!folio) {\n\t\t\t/*\n\t\t\t * Back out if somebody else faulted in this pte\n\t\t\t * while we released the pte lock.\n\t\t\t */\n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (likely(pte_same(*vmf->pte, vmf->orig_pte)))\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t/* Had to read the page from swap area: Major fault */\n\t\tret = VM_FAULT_MAJOR;\n\t\tcount_vm_event(PGMAJFAULT);\n\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);\n\t} else if (PageHWPoison(page)) {\n\t\t/*\n\t\t * hwpoisoned dirty swapcache pages are kept for killing\n\t\t * owner processes (which may be unknown at hwpoison time)\n\t\t */\n\t\tret = VM_FAULT_HWPOISON;\n\t\tgoto out_release;\n\t}\n\n\tlocked = folio_lock_or_retry(folio, vma->vm_mm, vmf->flags);\n\n\tif (!locked) {\n\t\tret |= VM_FAULT_RETRY;\n\t\tgoto out_release;\n\t}\n\n\tif (swapcache) {\n\t\t/*\n\t\t * Make sure folio_free_swap() or swapoff did not release the\n\t\t * swapcache from under us.  The page pin, and pte_same test\n\t\t * below, are not enough to exclude that.  Even if it is still\n\t\t * swapcache, we need to check that the page's swap has not\n\t\t * changed.\n\t\t */\n\t\tif (unlikely(!folio_test_swapcache(folio) ||\n\t\t\t     page_private(page) != entry.val))\n\t\t\tgoto out_page;\n\n\t\t/*\n\t\t * KSM sometimes has to copy on read faults, for example, if\n\t\t * page->index of !PageKSM() pages would be nonlinear inside the\n\t\t * anon VMA -- PageKSM() is lost on actual swapout.\n\t\t */\n\t\tpage = ksm_might_need_to_copy(page, vma, vmf->address);\n\t\tif (unlikely(!page)) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_page;\n\t\t}\n\t\tfolio = page_folio(page);\n\n\t\t/*\n\t\t * If we want to map a page that's in the swapcache writable, we\n\t\t * have to detect via the refcount if we're really the exclusive\n\t\t * owner. Try removing the extra reference from the local LRU\n\t\t * pagevecs if required.\n\t\t */\n\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&\n\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))\n\t\t\tlru_add_drain();\n\t}\n\n\tcgroup_throttle_swaprate(page, GFP_KERNEL);\n\n\t/*\n\t * Back out if somebody else already faulted in this pte.\n\t */\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte)))\n\t\tgoto out_nomap;\n\n\tif (unlikely(!folio_test_uptodate(folio))) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_nomap;\n\t}\n\n\t/*\n\t * PG_anon_exclusive reuses PG_mappedtodisk for anon pages. A swap pte\n\t * must never point at an anonymous page in the swapcache that is\n\t * PG_anon_exclusive. Sanity check that this holds and especially, that\n\t * no filesystem set PG_mappedtodisk on a page in the swapcache. Sanity\n\t * check after taking the PT lock and making sure that nobody\n\t * concurrently faulted in this page and set PG_anon_exclusive.\n\t */\n\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));\n\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));\n\n\t/*\n\t * Check under PT lock (to protect against concurrent fork() sharing\n\t * the swap entry concurrently) for certainly exclusive pages.\n\t */\n\tif (!folio_test_ksm(folio)) {\n\t\t/*\n\t\t * Note that pte_swp_exclusive() == false for architectures\n\t\t * without __HAVE_ARCH_PTE_SWP_EXCLUSIVE.\n\t\t */\n\t\texclusive = pte_swp_exclusive(vmf->orig_pte);\n\t\tif (folio != swapcache) {\n\t\t\t/*\n\t\t\t * We have a fresh page that is not exposed to the\n\t\t\t * swapcache -> certainly exclusive.\n\t\t\t */\n\t\t\texclusive = true;\n\t\t} else if (exclusive && folio_test_writeback(folio) &&\n\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {\n\t\t\t/*\n\t\t\t * This is tricky: not all swap backends support\n\t\t\t * concurrent page modifications while under writeback.\n\t\t\t *\n\t\t\t * So if we stumble over such a page in the swapcache\n\t\t\t * we must not set the page exclusive, otherwise we can\n\t\t\t * map it writable without further checks and modify it\n\t\t\t * while still under writeback.\n\t\t\t *\n\t\t\t * For these problematic swap backends, simply drop the\n\t\t\t * exclusive marker: this is perfectly fine as we start\n\t\t\t * writeback only if we fully unmapped the page and\n\t\t\t * there are no unexpected references on the page after\n\t\t\t * unmapping succeeded. After fully unmapped, no\n\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can\n\t\t\t * appear, so dropping the exclusive marker and mapping\n\t\t\t * it only R/O is fine.\n\t\t\t */\n\t\t\texclusive = false;\n\t\t}\n\t}\n\n\t/*\n\t * Remove the swap entry and conditionally try to free up the swapcache.\n\t * We're already holding a reference on the page but haven't mapped it\n\t * yet.\n\t */\n\tswap_free(entry);\n\tif (should_try_to_free_swap(folio, vma, vmf->flags))\n\t\tfolio_free_swap(folio);\n\n\tinc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);\n\tdec_mm_counter_fast(vma->vm_mm, MM_SWAPENTS);\n\tpte = mk_pte(page, vma->vm_page_prot);\n\n\t/*\n\t * Same logic as in do_wp_page(); however, optimize for pages that are\n\t * certainly not shared either because we just allocated them without\n\t * exposing them to the swapcache or because the swap entry indicates\n\t * exclusivity.\n\t */\n\tif (!folio_test_ksm(folio) &&\n\t    (exclusive || folio_ref_count(folio) == 1)) {\n\t\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);\n\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;\n\t\t\tret |= VM_FAULT_WRITE;\n\t\t}\n\t\trmap_flags |= RMAP_EXCLUSIVE;\n\t}\n\tflush_icache_page(vma, page);\n\tif (pte_swp_soft_dirty(vmf->orig_pte))\n\t\tpte = pte_mksoft_dirty(pte);\n\tif (pte_swp_uffd_wp(vmf->orig_pte)) {\n\t\tpte = pte_mkuffd_wp(pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\tvmf->orig_pte = pte;\n\n\t/* ksm created a completely new copy */\n\tif (unlikely(folio != swapcache && swapcache)) {\n\t\tpage_add_new_anon_rmap(page, vma, vmf->address);\n\t\tfolio_add_lru_vma(folio, vma);\n\t} else {\n\t\tpage_add_anon_rmap(page, vma, vmf->address, rmap_flags);\n\t}\n\n\tVM_BUG_ON(!folio_test_anon(folio) ||\n\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);\n\tarch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);\n\n\tfolio_unlock(folio);\n\tif (folio != swapcache && swapcache) {\n\t\t/*\n\t\t * Hold the lock to avoid the swap entry to be reused\n\t\t * until we take the PT lock for the pte_same() check\n\t\t * (to avoid false positives from pte_same). For\n\t\t * further safety release the lock after the swap_free\n\t\t * so that the swap count won't change under a\n\t\t * parallel locked swapcache.\n\t\t */\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\n\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\tret |= do_wp_page(vmf);\n\t\tif (ret & VM_FAULT_ERROR)\n\t\t\tret &= VM_FAULT_ERROR;\n\t\tgoto out;\n\t}\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\nunlock:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout:\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\nout_nomap:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout_page:\n\tfolio_unlock(folio);\nout_release:\n\tfolio_put(folio);\n\tif (folio != swapcache && swapcache) {\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,7 +25,21 @@\n \t\t\tret = remove_device_exclusive_entry(vmf);\n \t\t} else if (is_device_private_entry(entry)) {\n \t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n-\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);\n+\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n+\t\t\t\t\tvmf->address, &vmf->ptl);\n+\t\t\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {\n+\t\t\t\tspin_unlock(vmf->ptl);\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\t/*\n+\t\t\t * Get a page reference while we know the page can't be\n+\t\t\t * freed.\n+\t\t\t */\n+\t\t\tget_page(vmf->page);\n+\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\t\t\tvmf->page->pgmap->ops->migrate_to_ram(vmf);\n+\t\t\tput_page(vmf->page);\n \t\t} else if (is_hwpoison_entry(entry)) {\n \t\t\tret = VM_FAULT_HWPOISON;\n \t\t} else if (is_swapin_error_entry(entry)) {",
        "function_modified_lines": {
            "added": [
                "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
                "\t\t\t\t\tvmf->address, &vmf->ptl);",
                "\t\t\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {",
                "\t\t\t\tspin_unlock(vmf->ptl);",
                "\t\t\t\tgoto out;",
                "\t\t\t}",
                "",
                "\t\t\t/*",
                "\t\t\t * Get a page reference while we know the page can't be",
                "\t\t\t * freed.",
                "\t\t\t */",
                "\t\t\tget_page(vmf->page);",
                "\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
                "\t\t\tvmf->page->pgmap->ops->migrate_to_ram(vmf);",
                "\t\t\tput_page(vmf->page);"
            ],
            "deleted": [
                "\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3618
    },
    {
        "cve_id": "CVE-2021-20292",
        "code_before_change": "int ttm_tt_init(struct ttm_tt *ttm, struct ttm_buffer_object *bo,\n\t\tuint32_t page_flags)\n{\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\n\tif (ttm_tt_alloc_page_directory(ttm)) {\n\t\tttm_tt_destroy(ttm);\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ttm_tt_init(struct ttm_tt *ttm, struct ttm_buffer_object *bo,\n\t\tuint32_t page_flags)\n{\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\n\tif (ttm_tt_alloc_page_directory(ttm)) {\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tttm_tt_init_fields(ttm, bo, page_flags);\n \n \tif (ttm_tt_alloc_page_directory(ttm)) {\n-\t\tttm_tt_destroy(ttm);\n \t\tpr_err(\"Failed allocating page table\\n\");\n \t\treturn -ENOMEM;\n \t}",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tttm_tt_destroy(ttm);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a flaw reported in the Linux kernel in versions before 5.9 in drivers/gpu/drm/nouveau/nouveau_sgdma.c in nouveau_sgdma_create_ttm in Nouveau DRM subsystem. The issue results from the lack of validating the existence of an object prior to performing operations on the object. An attacker with a local account with a root privilege, can leverage this vulnerability to escalate privileges and execute code in the context of the kernel.",
        "id": 2870
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static void __io_queue_sqe(struct io_kiocb *req)\n{\n\tstruct io_kiocb *linked_timeout = io_prep_linked_timeout(req);\n\tconst struct cred *old_creds = NULL;\n\tint ret;\n\n\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n\t    req->work.identity->creds != current_cred())\n\t\told_creds = override_creds(req->work.identity->creds);\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\tif (old_creds)\n\t\trevert_creds(old_creds);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\tif (req->flags & REQ_F_COMPLETE_INLINE) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\t\tcs->reqs[cs->nr++] = req;\n\t\t\tif (cs->nr == ARRAY_SIZE(cs->reqs))\n\t\t\t\tio_submit_flush_completions(cs, ctx);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t} else {\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}",
        "code_after_change": "static void __io_queue_sqe(struct io_kiocb *req)\n{\n\tstruct io_kiocb *linked_timeout = io_prep_linked_timeout(req);\n\tconst struct cred *old_creds = NULL;\n\tint ret;\n\n\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&\n\t    req->work.creds != current_cred())\n\t\told_creds = override_creds(req->work.creds);\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\tif (old_creds)\n\t\trevert_creds(old_creds);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\tif (req->flags & REQ_F_COMPLETE_INLINE) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\t\tcs->reqs[cs->nr++] = req;\n\t\t\tif (cs->nr == ARRAY_SIZE(cs->reqs))\n\t\t\t\tio_submit_flush_completions(cs, ctx);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t} else {\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,9 @@\n \tconst struct cred *old_creds = NULL;\n \tint ret;\n \n-\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n-\t    req->work.identity->creds != current_cred())\n-\t\told_creds = override_creds(req->work.identity->creds);\n+\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&\n+\t    req->work.creds != current_cred())\n+\t\told_creds = override_creds(req->work.creds);\n \n \tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n ",
        "function_modified_lines": {
            "added": [
                "\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&",
                "\t    req->work.creds != current_cred())",
                "\t\told_creds = override_creds(req->work.creds);"
            ],
            "deleted": [
                "\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&",
                "\t    req->work.identity->creds != current_cred())",
                "\t\told_creds = override_creds(req->work.identity->creds);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3362
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tstruct io_identity *iod;\n\n\tiod = idr_remove(&ctx->personality_idr, id);\n\tif (iod) {\n\t\tput_cred(iod->creds);\n\t\tif (refcount_dec_and_test(&iod->count))\n\t\t\tkfree(iod);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}",
        "code_after_change": "static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tconst struct cred *creds;\n\n\tcreds = idr_remove(&ctx->personality_idr, id);\n\tif (creds) {\n\t\tput_cred(creds);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,10 @@\n static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n {\n-\tstruct io_identity *iod;\n+\tconst struct cred *creds;\n \n-\tiod = idr_remove(&ctx->personality_idr, id);\n-\tif (iod) {\n-\t\tput_cred(iod->creds);\n-\t\tif (refcount_dec_and_test(&iod->count))\n-\t\t\tkfree(iod);\n+\tcreds = idr_remove(&ctx->personality_idr, id);\n+\tif (creds) {\n+\t\tput_cred(creds);\n \t\treturn 0;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *creds;",
                "\tcreds = idr_remove(&ctx->personality_idr, id);",
                "\tif (creds) {",
                "\t\tput_cred(creds);"
            ],
            "deleted": [
                "\tstruct io_identity *iod;",
                "\tiod = idr_remove(&ctx->personality_idr, id);",
                "\tif (iod) {",
                "\t\tput_cred(iod->creds);",
                "\t\tif (refcount_dec_and_test(&iod->count))",
                "\t\t\tkfree(iod);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3360
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
        "code_after_change": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,11 @@\n \tworker->flags = 0;\n \tpreempt_enable();\n \n+\tif (worker->saved_creds) {\n+\t\trevert_creds(worker->saved_creds);\n+\t\tworker->cur_creds = worker->saved_creds = NULL;\n+\t}\n+\n \traw_spin_lock_irq(&wqe->lock);\n \thlist_nulls_del_rcu(&worker->nulls_node);\n \tlist_del_rcu(&worker->all_list);",
        "function_modified_lines": {
            "added": [
                "\tif (worker->saved_creds) {",
                "\t\trevert_creds(worker->saved_creds);",
                "\t\tworker->cur_creds = worker->saved_creds = NULL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3352
    },
    {
        "cve_id": "CVE-2023-4244",
        "code_before_change": "static int nf_tables_commit(struct net *net, struct sk_buff *skb)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tstruct nft_trans *trans, *next;\n\tunsigned int base_seq, gc_seq;\n\tLIST_HEAD(set_update_list);\n\tstruct nft_trans_elem *te;\n\tstruct nft_chain *chain;\n\tstruct nft_table *table;\n\tLIST_HEAD(adl);\n\tint err;\n\n\tif (list_empty(&nft_net->commit_list)) {\n\t\tmutex_unlock(&nft_net->commit_mutex);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(trans, &nft_net->binding_list, binding_list) {\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (!nft_trans_set_update(trans) &&\n\t\t\t    nft_set_is_anonymous(nft_trans_set(trans)) &&\n\t\t\t    !nft_trans_set_bound(trans)) {\n\t\t\t\tpr_warn_once(\"nftables ruleset with unbound set\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (!nft_trans_chain_update(trans) &&\n\t\t\t    nft_chain_binding(nft_trans_chain(trans)) &&\n\t\t\t    !nft_trans_chain_bound(trans)) {\n\t\t\t\tpr_warn_once(\"nftables ruleset with unbound chain\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* 0. Validate ruleset, otherwise roll back for error reporting. */\n\tif (nf_tables_validate(net) < 0)\n\t\treturn -EAGAIN;\n\n\terr = nft_flow_rule_offload_commit(net);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* 1.  Allocate space for next generation rules_gen_X[] */\n\tlist_for_each_entry_safe(trans, next, &nft_net->commit_list, list) {\n\t\tint ret;\n\n\t\tret = nf_tables_commit_audit_alloc(&adl, trans->ctx.table);\n\t\tif (ret) {\n\t\t\tnf_tables_commit_chain_prepare_cancel(net);\n\t\t\tnf_tables_commit_audit_free(&adl);\n\t\t\treturn ret;\n\t\t}\n\t\tif (trans->msg_type == NFT_MSG_NEWRULE ||\n\t\t    trans->msg_type == NFT_MSG_DELRULE) {\n\t\t\tchain = trans->ctx.chain;\n\n\t\t\tret = nf_tables_commit_chain_prepare(net, chain);\n\t\t\tif (ret < 0) {\n\t\t\t\tnf_tables_commit_chain_prepare_cancel(net);\n\t\t\t\tnf_tables_commit_audit_free(&adl);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* step 2.  Make rules_gen_X visible to packet path */\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tlist_for_each_entry(chain, &table->chains, list)\n\t\t\tnf_tables_commit_chain(net, chain);\n\t}\n\n\t/*\n\t * Bump generation counter, invalidate any dump in progress.\n\t * Cannot fail after this point.\n\t */\n\tbase_seq = READ_ONCE(nft_net->base_seq);\n\twhile (++base_seq == 0)\n\t\t;\n\n\tWRITE_ONCE(nft_net->base_seq, base_seq);\n\n\t/* Bump gc counter, it becomes odd, this is the busy mark. */\n\tgc_seq = READ_ONCE(nft_net->gc_seq);\n\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);\n\n\t/* step 3. Start new generation, rules_gen_X now in use. */\n\tnet->nft.gencursor = nft_gencursor_next(net);\n\n\tlist_for_each_entry_safe(trans, next, &nft_net->commit_list, list) {\n\t\tnf_tables_commit_audit_collect(&adl, trans->ctx.table,\n\t\t\t\t\t       trans->msg_type);\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWTABLE:\n\t\t\tif (nft_trans_table_update(trans)) {\n\t\t\t\tif (!(trans->ctx.table->flags & __NFT_TABLE_F_UPDATE)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (trans->ctx.table->flags & NFT_TABLE_F_DORMANT)\n\t\t\t\t\tnf_tables_table_disable(net, trans->ctx.table);\n\n\t\t\t\ttrans->ctx.table->flags &= ~__NFT_TABLE_F_UPDATE;\n\t\t\t} else {\n\t\t\t\tnft_clear(net, trans->ctx.table);\n\t\t\t}\n\t\t\tnf_tables_table_notify(&trans->ctx, NFT_MSG_NEWTABLE);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELTABLE:\n\t\tcase NFT_MSG_DESTROYTABLE:\n\t\t\tlist_del_rcu(&trans->ctx.table->list);\n\t\t\tnf_tables_table_notify(&trans->ctx, trans->msg_type);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnft_chain_commit_update(trans);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_NEWCHAIN,\n\t\t\t\t\t\t       &nft_trans_chain_hooks(trans));\n\t\t\t\tlist_splice(&nft_trans_chain_hooks(trans),\n\t\t\t\t\t    &nft_trans_basechain(trans)->hook_list);\n\t\t\t\t/* trans destroyed after rcu grace period */\n\t\t\t} else {\n\t\t\t\tnft_chain_commit_drop_policy(trans);\n\t\t\t\tnft_clear(net, trans->ctx.chain);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_NEWCHAIN, NULL);\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELCHAIN:\n\t\tcase NFT_MSG_DESTROYCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_DELCHAIN,\n\t\t\t\t\t\t       &nft_trans_chain_hooks(trans));\n\t\t\t\tnft_netdev_unregister_hooks(net,\n\t\t\t\t\t\t\t    &nft_trans_chain_hooks(trans),\n\t\t\t\t\t\t\t    true);\n\t\t\t} else {\n\t\t\t\tnft_chain_del(trans->ctx.chain);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_DELCHAIN,\n\t\t\t\t\t\t       NULL);\n\t\t\t\tnf_tables_unregister_hook(trans->ctx.net,\n\t\t\t\t\t\t\t  trans->ctx.table,\n\t\t\t\t\t\t\t  trans->ctx.chain);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWRULE:\n\t\t\tnft_clear(trans->ctx.net, nft_trans_rule(trans));\n\t\t\tnf_tables_rule_notify(&trans->ctx,\n\t\t\t\t\t      nft_trans_rule(trans),\n\t\t\t\t\t      NFT_MSG_NEWRULE);\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELRULE:\n\t\tcase NFT_MSG_DESTROYRULE:\n\t\t\tlist_del_rcu(&nft_trans_rule(trans)->list);\n\t\t\tnf_tables_rule_notify(&trans->ctx,\n\t\t\t\t\t      nft_trans_rule(trans),\n\t\t\t\t\t      trans->msg_type);\n\t\t\tnft_rule_expr_deactivate(&trans->ctx,\n\t\t\t\t\t\t nft_trans_rule(trans),\n\t\t\t\t\t\t NFT_TRANS_COMMIT);\n\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (nft_trans_set_update(trans)) {\n\t\t\t\tstruct nft_set *set = nft_trans_set(trans);\n\n\t\t\t\tWRITE_ONCE(set->timeout, nft_trans_set_timeout(trans));\n\t\t\t\tWRITE_ONCE(set->gc_int, nft_trans_set_gc_int(trans));\n\n\t\t\t\tif (nft_trans_set_size(trans))\n\t\t\t\t\tWRITE_ONCE(set->size, nft_trans_set_size(trans));\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_set(trans));\n\t\t\t\t/* This avoids hitting -EBUSY when deleting the table\n\t\t\t\t * from the transaction.\n\t\t\t\t */\n\t\t\t\tif (nft_set_is_anonymous(nft_trans_set(trans)) &&\n\t\t\t\t    !list_empty(&nft_trans_set(trans)->bindings))\n\t\t\t\t\tnft_use_dec(&trans->ctx.table->use);\n\t\t\t}\n\t\t\tnf_tables_set_notify(&trans->ctx, nft_trans_set(trans),\n\t\t\t\t\t     NFT_MSG_NEWSET, GFP_KERNEL);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSET:\n\t\tcase NFT_MSG_DESTROYSET:\n\t\t\tnft_trans_set(trans)->dead = 1;\n\t\t\tlist_del_rcu(&nft_trans_set(trans)->list);\n\t\t\tnf_tables_set_notify(&trans->ctx, nft_trans_set(trans),\n\t\t\t\t\t     trans->msg_type, GFP_KERNEL);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnft_setelem_activate(net, te->set, &te->elem);\n\t\t\tnf_tables_setelem_notify(&trans->ctx, te->set,\n\t\t\t\t\t\t &te->elem,\n\t\t\t\t\t\t NFT_MSG_NEWSETELEM);\n\t\t\tif (te->set->ops->commit &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSETELEM:\n\t\tcase NFT_MSG_DESTROYSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnf_tables_setelem_notify(&trans->ctx, te->set,\n\t\t\t\t\t\t &te->elem,\n\t\t\t\t\t\t trans->msg_type);\n\t\t\tnft_setelem_remove(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem)) {\n\t\t\t\tatomic_dec(&te->set->nelems);\n\t\t\t\tte->set->ndeact--;\n\t\t\t}\n\t\t\tif (te->set->ops->commit &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWOBJ:\n\t\t\tif (nft_trans_obj_update(trans)) {\n\t\t\t\tnft_obj_commit_update(trans);\n\t\t\t\tnf_tables_obj_notify(&trans->ctx,\n\t\t\t\t\t\t     nft_trans_obj(trans),\n\t\t\t\t\t\t     NFT_MSG_NEWOBJ);\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_obj(trans));\n\t\t\t\tnf_tables_obj_notify(&trans->ctx,\n\t\t\t\t\t\t     nft_trans_obj(trans),\n\t\t\t\t\t\t     NFT_MSG_NEWOBJ);\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELOBJ:\n\t\tcase NFT_MSG_DESTROYOBJ:\n\t\t\tnft_obj_del(nft_trans_obj(trans));\n\t\t\tnf_tables_obj_notify(&trans->ctx, nft_trans_obj(trans),\n\t\t\t\t\t     trans->msg_type);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnft_trans_flowtable(trans)->data.flags =\n\t\t\t\t\tnft_trans_flowtable_flags(trans);\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t\t\t   NFT_MSG_NEWFLOWTABLE);\n\t\t\t\tlist_splice(&nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t    &nft_trans_flowtable(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_flowtable(trans));\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   NULL,\n\t\t\t\t\t\t\t   NFT_MSG_NEWFLOWTABLE);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELFLOWTABLE:\n\t\tcase NFT_MSG_DESTROYFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t\t\t   trans->msg_type);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans));\n\t\t\t} else {\n\t\t\t\tlist_del_rcu(&nft_trans_flowtable(trans)->list);\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   NULL,\n\t\t\t\t\t\t\t   trans->msg_type);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t&nft_trans_flowtable(trans)->hook_list);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnft_set_commit_update(&set_update_list);\n\n\tnft_commit_notify(net, NETLINK_CB(skb).portid);\n\tnf_tables_gen_notify(net, skb, NFT_MSG_NEWGEN);\n\tnf_tables_commit_audit_log(&adl, nft_net->base_seq);\n\n\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);\n\tnf_tables_commit_release(net);\n\n\treturn 0;\n}",
        "code_after_change": "static int nf_tables_commit(struct net *net, struct sk_buff *skb)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tstruct nft_trans *trans, *next;\n\tunsigned int base_seq, gc_seq;\n\tLIST_HEAD(set_update_list);\n\tstruct nft_trans_elem *te;\n\tstruct nft_chain *chain;\n\tstruct nft_table *table;\n\tLIST_HEAD(adl);\n\tint err;\n\n\tif (list_empty(&nft_net->commit_list)) {\n\t\tmutex_unlock(&nft_net->commit_mutex);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(trans, &nft_net->binding_list, binding_list) {\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (!nft_trans_set_update(trans) &&\n\t\t\t    nft_set_is_anonymous(nft_trans_set(trans)) &&\n\t\t\t    !nft_trans_set_bound(trans)) {\n\t\t\t\tpr_warn_once(\"nftables ruleset with unbound set\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (!nft_trans_chain_update(trans) &&\n\t\t\t    nft_chain_binding(nft_trans_chain(trans)) &&\n\t\t\t    !nft_trans_chain_bound(trans)) {\n\t\t\t\tpr_warn_once(\"nftables ruleset with unbound chain\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* 0. Validate ruleset, otherwise roll back for error reporting. */\n\tif (nf_tables_validate(net) < 0)\n\t\treturn -EAGAIN;\n\n\terr = nft_flow_rule_offload_commit(net);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* 1.  Allocate space for next generation rules_gen_X[] */\n\tlist_for_each_entry_safe(trans, next, &nft_net->commit_list, list) {\n\t\tint ret;\n\n\t\tret = nf_tables_commit_audit_alloc(&adl, trans->ctx.table);\n\t\tif (ret) {\n\t\t\tnf_tables_commit_chain_prepare_cancel(net);\n\t\t\tnf_tables_commit_audit_free(&adl);\n\t\t\treturn ret;\n\t\t}\n\t\tif (trans->msg_type == NFT_MSG_NEWRULE ||\n\t\t    trans->msg_type == NFT_MSG_DELRULE) {\n\t\t\tchain = trans->ctx.chain;\n\n\t\t\tret = nf_tables_commit_chain_prepare(net, chain);\n\t\t\tif (ret < 0) {\n\t\t\t\tnf_tables_commit_chain_prepare_cancel(net);\n\t\t\t\tnf_tables_commit_audit_free(&adl);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* step 2.  Make rules_gen_X visible to packet path */\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tlist_for_each_entry(chain, &table->chains, list)\n\t\t\tnf_tables_commit_chain(net, chain);\n\t}\n\n\t/*\n\t * Bump generation counter, invalidate any dump in progress.\n\t * Cannot fail after this point.\n\t */\n\tbase_seq = READ_ONCE(nft_net->base_seq);\n\twhile (++base_seq == 0)\n\t\t;\n\n\tWRITE_ONCE(nft_net->base_seq, base_seq);\n\n\tgc_seq = nft_gc_seq_begin(nft_net);\n\n\t/* step 3. Start new generation, rules_gen_X now in use. */\n\tnet->nft.gencursor = nft_gencursor_next(net);\n\n\tlist_for_each_entry_safe(trans, next, &nft_net->commit_list, list) {\n\t\tnf_tables_commit_audit_collect(&adl, trans->ctx.table,\n\t\t\t\t\t       trans->msg_type);\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWTABLE:\n\t\t\tif (nft_trans_table_update(trans)) {\n\t\t\t\tif (!(trans->ctx.table->flags & __NFT_TABLE_F_UPDATE)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (trans->ctx.table->flags & NFT_TABLE_F_DORMANT)\n\t\t\t\t\tnf_tables_table_disable(net, trans->ctx.table);\n\n\t\t\t\ttrans->ctx.table->flags &= ~__NFT_TABLE_F_UPDATE;\n\t\t\t} else {\n\t\t\t\tnft_clear(net, trans->ctx.table);\n\t\t\t}\n\t\t\tnf_tables_table_notify(&trans->ctx, NFT_MSG_NEWTABLE);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELTABLE:\n\t\tcase NFT_MSG_DESTROYTABLE:\n\t\t\tlist_del_rcu(&trans->ctx.table->list);\n\t\t\tnf_tables_table_notify(&trans->ctx, trans->msg_type);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnft_chain_commit_update(trans);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_NEWCHAIN,\n\t\t\t\t\t\t       &nft_trans_chain_hooks(trans));\n\t\t\t\tlist_splice(&nft_trans_chain_hooks(trans),\n\t\t\t\t\t    &nft_trans_basechain(trans)->hook_list);\n\t\t\t\t/* trans destroyed after rcu grace period */\n\t\t\t} else {\n\t\t\t\tnft_chain_commit_drop_policy(trans);\n\t\t\t\tnft_clear(net, trans->ctx.chain);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_NEWCHAIN, NULL);\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELCHAIN:\n\t\tcase NFT_MSG_DESTROYCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_DELCHAIN,\n\t\t\t\t\t\t       &nft_trans_chain_hooks(trans));\n\t\t\t\tnft_netdev_unregister_hooks(net,\n\t\t\t\t\t\t\t    &nft_trans_chain_hooks(trans),\n\t\t\t\t\t\t\t    true);\n\t\t\t} else {\n\t\t\t\tnft_chain_del(trans->ctx.chain);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_DELCHAIN,\n\t\t\t\t\t\t       NULL);\n\t\t\t\tnf_tables_unregister_hook(trans->ctx.net,\n\t\t\t\t\t\t\t  trans->ctx.table,\n\t\t\t\t\t\t\t  trans->ctx.chain);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWRULE:\n\t\t\tnft_clear(trans->ctx.net, nft_trans_rule(trans));\n\t\t\tnf_tables_rule_notify(&trans->ctx,\n\t\t\t\t\t      nft_trans_rule(trans),\n\t\t\t\t\t      NFT_MSG_NEWRULE);\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELRULE:\n\t\tcase NFT_MSG_DESTROYRULE:\n\t\t\tlist_del_rcu(&nft_trans_rule(trans)->list);\n\t\t\tnf_tables_rule_notify(&trans->ctx,\n\t\t\t\t\t      nft_trans_rule(trans),\n\t\t\t\t\t      trans->msg_type);\n\t\t\tnft_rule_expr_deactivate(&trans->ctx,\n\t\t\t\t\t\t nft_trans_rule(trans),\n\t\t\t\t\t\t NFT_TRANS_COMMIT);\n\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (nft_trans_set_update(trans)) {\n\t\t\t\tstruct nft_set *set = nft_trans_set(trans);\n\n\t\t\t\tWRITE_ONCE(set->timeout, nft_trans_set_timeout(trans));\n\t\t\t\tWRITE_ONCE(set->gc_int, nft_trans_set_gc_int(trans));\n\n\t\t\t\tif (nft_trans_set_size(trans))\n\t\t\t\t\tWRITE_ONCE(set->size, nft_trans_set_size(trans));\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_set(trans));\n\t\t\t\t/* This avoids hitting -EBUSY when deleting the table\n\t\t\t\t * from the transaction.\n\t\t\t\t */\n\t\t\t\tif (nft_set_is_anonymous(nft_trans_set(trans)) &&\n\t\t\t\t    !list_empty(&nft_trans_set(trans)->bindings))\n\t\t\t\t\tnft_use_dec(&trans->ctx.table->use);\n\t\t\t}\n\t\t\tnf_tables_set_notify(&trans->ctx, nft_trans_set(trans),\n\t\t\t\t\t     NFT_MSG_NEWSET, GFP_KERNEL);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSET:\n\t\tcase NFT_MSG_DESTROYSET:\n\t\t\tnft_trans_set(trans)->dead = 1;\n\t\t\tlist_del_rcu(&nft_trans_set(trans)->list);\n\t\t\tnf_tables_set_notify(&trans->ctx, nft_trans_set(trans),\n\t\t\t\t\t     trans->msg_type, GFP_KERNEL);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnft_setelem_activate(net, te->set, &te->elem);\n\t\t\tnf_tables_setelem_notify(&trans->ctx, te->set,\n\t\t\t\t\t\t &te->elem,\n\t\t\t\t\t\t NFT_MSG_NEWSETELEM);\n\t\t\tif (te->set->ops->commit &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSETELEM:\n\t\tcase NFT_MSG_DESTROYSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnf_tables_setelem_notify(&trans->ctx, te->set,\n\t\t\t\t\t\t &te->elem,\n\t\t\t\t\t\t trans->msg_type);\n\t\t\tnft_setelem_remove(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem)) {\n\t\t\t\tatomic_dec(&te->set->nelems);\n\t\t\t\tte->set->ndeact--;\n\t\t\t}\n\t\t\tif (te->set->ops->commit &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWOBJ:\n\t\t\tif (nft_trans_obj_update(trans)) {\n\t\t\t\tnft_obj_commit_update(trans);\n\t\t\t\tnf_tables_obj_notify(&trans->ctx,\n\t\t\t\t\t\t     nft_trans_obj(trans),\n\t\t\t\t\t\t     NFT_MSG_NEWOBJ);\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_obj(trans));\n\t\t\t\tnf_tables_obj_notify(&trans->ctx,\n\t\t\t\t\t\t     nft_trans_obj(trans),\n\t\t\t\t\t\t     NFT_MSG_NEWOBJ);\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELOBJ:\n\t\tcase NFT_MSG_DESTROYOBJ:\n\t\t\tnft_obj_del(nft_trans_obj(trans));\n\t\t\tnf_tables_obj_notify(&trans->ctx, nft_trans_obj(trans),\n\t\t\t\t\t     trans->msg_type);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnft_trans_flowtable(trans)->data.flags =\n\t\t\t\t\tnft_trans_flowtable_flags(trans);\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t\t\t   NFT_MSG_NEWFLOWTABLE);\n\t\t\t\tlist_splice(&nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t    &nft_trans_flowtable(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_flowtable(trans));\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   NULL,\n\t\t\t\t\t\t\t   NFT_MSG_NEWFLOWTABLE);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELFLOWTABLE:\n\t\tcase NFT_MSG_DESTROYFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t\t\t   trans->msg_type);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans));\n\t\t\t} else {\n\t\t\t\tlist_del_rcu(&nft_trans_flowtable(trans)->list);\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   NULL,\n\t\t\t\t\t\t\t   trans->msg_type);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t&nft_trans_flowtable(trans)->hook_list);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnft_set_commit_update(&set_update_list);\n\n\tnft_commit_notify(net, NETLINK_CB(skb).portid);\n\tnf_tables_gen_notify(net, skb, NFT_MSG_NEWGEN);\n\tnf_tables_commit_audit_log(&adl, nft_net->base_seq);\n\n\tnft_gc_seq_end(nft_net, gc_seq);\n\tnf_tables_commit_release(net);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -83,9 +83,7 @@\n \n \tWRITE_ONCE(nft_net->base_seq, base_seq);\n \n-\t/* Bump gc counter, it becomes odd, this is the busy mark. */\n-\tgc_seq = READ_ONCE(nft_net->gc_seq);\n-\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);\n+\tgc_seq = nft_gc_seq_begin(nft_net);\n \n \t/* step 3. Start new generation, rules_gen_X now in use. */\n \tnet->nft.gencursor = nft_gencursor_next(net);\n@@ -298,7 +296,7 @@\n \tnf_tables_gen_notify(net, skb, NFT_MSG_NEWGEN);\n \tnf_tables_commit_audit_log(&adl, nft_net->base_seq);\n \n-\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);\n+\tnft_gc_seq_end(nft_net, gc_seq);\n \tnf_tables_commit_release(net);\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tgc_seq = nft_gc_seq_begin(nft_net);",
                "\tnft_gc_seq_end(nft_net, gc_seq);"
            ],
            "deleted": [
                "\t/* Bump gc counter, it becomes odd, this is the busy mark. */",
                "\tgc_seq = READ_ONCE(nft_net->gc_seq);",
                "\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);",
                "\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nDue to a race condition between nf_tables netlink control plane transaction and nft_set element garbage collection, it is possible to underflow the reference counter causing a use-after-free vulnerability.\n\nWe recommend upgrading past commit 3e91b0ebd994635df2346353322ac51ce84ce6d8.\n\n",
        "id": 4201
    },
    {
        "cve_id": "CVE-2023-4244",
        "code_before_change": "static int nft_rcv_nl_event(struct notifier_block *this, unsigned long event,\n\t\t\t    void *ptr)\n{\n\tstruct nft_table *table, *to_delete[8];\n\tstruct nftables_pernet *nft_net;\n\tstruct netlink_notify *n = ptr;\n\tstruct net *net = n->net;\n\tunsigned int deleted;\n\tbool restart = false;\n\n\tif (event != NETLINK_URELEASE || n->protocol != NETLINK_NETFILTER)\n\t\treturn NOTIFY_DONE;\n\n\tnft_net = nft_pernet(net);\n\tdeleted = 0;\n\tmutex_lock(&nft_net->commit_mutex);\n\tif (!list_empty(&nf_tables_destroy_list))\n\t\trcu_barrier();\nagain:\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tif (nft_table_has_owner(table) &&\n\t\t    n->portid == table->nlpid) {\n\t\t\t__nft_release_hook(net, table);\n\t\t\tlist_del_rcu(&table->list);\n\t\t\tto_delete[deleted++] = table;\n\t\t\tif (deleted >= ARRAY_SIZE(to_delete))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (deleted) {\n\t\trestart = deleted >= ARRAY_SIZE(to_delete);\n\t\tsynchronize_rcu();\n\t\twhile (deleted)\n\t\t\t__nft_release_table(net, to_delete[--deleted]);\n\n\t\tif (restart)\n\t\t\tgoto again;\n\t}\n\tmutex_unlock(&nft_net->commit_mutex);\n\n\treturn NOTIFY_DONE;\n}",
        "code_after_change": "static int nft_rcv_nl_event(struct notifier_block *this, unsigned long event,\n\t\t\t    void *ptr)\n{\n\tstruct nft_table *table, *to_delete[8];\n\tstruct nftables_pernet *nft_net;\n\tstruct netlink_notify *n = ptr;\n\tstruct net *net = n->net;\n\tunsigned int deleted;\n\tbool restart = false;\n\tunsigned int gc_seq;\n\n\tif (event != NETLINK_URELEASE || n->protocol != NETLINK_NETFILTER)\n\t\treturn NOTIFY_DONE;\n\n\tnft_net = nft_pernet(net);\n\tdeleted = 0;\n\tmutex_lock(&nft_net->commit_mutex);\n\n\tgc_seq = nft_gc_seq_begin(nft_net);\n\n\tif (!list_empty(&nf_tables_destroy_list))\n\t\trcu_barrier();\nagain:\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tif (nft_table_has_owner(table) &&\n\t\t    n->portid == table->nlpid) {\n\t\t\t__nft_release_hook(net, table);\n\t\t\tlist_del_rcu(&table->list);\n\t\t\tto_delete[deleted++] = table;\n\t\t\tif (deleted >= ARRAY_SIZE(to_delete))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (deleted) {\n\t\trestart = deleted >= ARRAY_SIZE(to_delete);\n\t\tsynchronize_rcu();\n\t\twhile (deleted)\n\t\t\t__nft_release_table(net, to_delete[--deleted]);\n\n\t\tif (restart)\n\t\t\tgoto again;\n\t}\n\tnft_gc_seq_end(nft_net, gc_seq);\n\n\tmutex_unlock(&nft_net->commit_mutex);\n\n\treturn NOTIFY_DONE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct net *net = n->net;\n \tunsigned int deleted;\n \tbool restart = false;\n+\tunsigned int gc_seq;\n \n \tif (event != NETLINK_URELEASE || n->protocol != NETLINK_NETFILTER)\n \t\treturn NOTIFY_DONE;\n@@ -14,6 +15,9 @@\n \tnft_net = nft_pernet(net);\n \tdeleted = 0;\n \tmutex_lock(&nft_net->commit_mutex);\n+\n+\tgc_seq = nft_gc_seq_begin(nft_net);\n+\n \tif (!list_empty(&nf_tables_destroy_list))\n \t\trcu_barrier();\n again:\n@@ -36,6 +40,8 @@\n \t\tif (restart)\n \t\t\tgoto again;\n \t}\n+\tnft_gc_seq_end(nft_net, gc_seq);\n+\n \tmutex_unlock(&nft_net->commit_mutex);\n \n \treturn NOTIFY_DONE;",
        "function_modified_lines": {
            "added": [
                "\tunsigned int gc_seq;",
                "",
                "\tgc_seq = nft_gc_seq_begin(nft_net);",
                "",
                "\tnft_gc_seq_end(nft_net, gc_seq);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nDue to a race condition between nf_tables netlink control plane transaction and nft_set element garbage collection, it is possible to underflow the reference counter causing a use-after-free vulnerability.\n\nWe recommend upgrading past commit 3e91b0ebd994635df2346353322ac51ce84ce6d8.\n\n",
        "id": 4202
    },
    {
        "cve_id": "CVE-2022-2938",
        "code_before_change": "__poll_t psi_trigger_poll(void **trigger_ptr,\n\t\t\t\tstruct file *file, poll_table *wait)\n{\n\t__poll_t ret = DEFAULT_POLLMASK;\n\tstruct psi_trigger *t;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\n\trcu_read_lock();\n\n\tt = rcu_dereference(*(void __rcu __force **)trigger_ptr);\n\tif (!t) {\n\t\trcu_read_unlock();\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\t}\n\tkref_get(&t->refcount);\n\n\trcu_read_unlock();\n\n\tpoll_wait(file, &t->event_wait, wait);\n\n\tif (cmpxchg(&t->event, 1, 0) == 1)\n\t\tret |= EPOLLPRI;\n\n\tkref_put(&t->refcount, psi_trigger_destroy);\n\n\treturn ret;\n}",
        "code_after_change": "__poll_t psi_trigger_poll(void **trigger_ptr,\n\t\t\t\tstruct file *file, poll_table *wait)\n{\n\t__poll_t ret = DEFAULT_POLLMASK;\n\tstruct psi_trigger *t;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\n\tt = smp_load_acquire(trigger_ptr);\n\tif (!t)\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\n\tpoll_wait(file, &t->event_wait, wait);\n\n\tif (cmpxchg(&t->event, 1, 0) == 1)\n\t\tret |= EPOLLPRI;\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,23 +7,14 @@\n \tif (static_branch_likely(&psi_disabled))\n \t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n \n-\trcu_read_lock();\n-\n-\tt = rcu_dereference(*(void __rcu __force **)trigger_ptr);\n-\tif (!t) {\n-\t\trcu_read_unlock();\n+\tt = smp_load_acquire(trigger_ptr);\n+\tif (!t)\n \t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n-\t}\n-\tkref_get(&t->refcount);\n-\n-\trcu_read_unlock();\n \n \tpoll_wait(file, &t->event_wait, wait);\n \n \tif (cmpxchg(&t->event, 1, 0) == 1)\n \t\tret |= EPOLLPRI;\n \n-\tkref_put(&t->refcount, psi_trigger_destroy);\n-\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tt = smp_load_acquire(trigger_ptr);",
                "\tif (!t)"
            ],
            "deleted": [
                "\trcu_read_lock();",
                "",
                "\tt = rcu_dereference(*(void __rcu __force **)trigger_ptr);",
                "\tif (!t) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "\tkref_get(&t->refcount);",
                "",
                "\trcu_read_unlock();",
                "\tkref_put(&t->refcount, psi_trigger_destroy);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's implementation of Pressure Stall Information. While the feature is disabled by default, it could allow an attacker to crash the system or have other memory-corruption side effects.",
        "id": 3519
    },
    {
        "cve_id": "CVE-2022-1048",
        "code_before_change": "static int snd_pcm_hw_params(struct snd_pcm_substream *substream,\n\t\t\t     struct snd_pcm_hw_params *params)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint err, usecs;\n\tunsigned int bits;\n\tsnd_pcm_uframes_t frames;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_OPEN:\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tbreak;\n\tdefault:\n\t\tsnd_pcm_stream_unlock_irq(substream);\n\t\treturn -EBADFD;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n#if IS_ENABLED(CONFIG_SND_PCM_OSS)\n\tif (!substream->oss.oss)\n#endif\n\t\tif (atomic_read(&substream->mmap_count))\n\t\t\treturn -EBADFD;\n\n\tsnd_pcm_sync_stop(substream, true);\n\n\tparams->rmask = ~0U;\n\terr = snd_pcm_hw_refine(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = snd_pcm_hw_params_choose(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = fixup_unreferenced_params(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\tif (substream->managed_buffer_alloc) {\n\t\terr = snd_pcm_lib_malloc_pages(substream,\n\t\t\t\t\t       params_buffer_bytes(params));\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t\truntime->buffer_changed = err > 0;\n\t}\n\n\tif (substream->ops->hw_params != NULL) {\n\t\terr = substream->ops->hw_params(substream, params);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t}\n\n\truntime->access = params_access(params);\n\truntime->format = params_format(params);\n\truntime->subformat = params_subformat(params);\n\truntime->channels = params_channels(params);\n\truntime->rate = params_rate(params);\n\truntime->period_size = params_period_size(params);\n\truntime->periods = params_periods(params);\n\truntime->buffer_size = params_buffer_size(params);\n\truntime->info = params->info;\n\truntime->rate_num = params->rate_num;\n\truntime->rate_den = params->rate_den;\n\truntime->no_period_wakeup =\n\t\t\t(params->info & SNDRV_PCM_INFO_NO_PERIOD_WAKEUP) &&\n\t\t\t(params->flags & SNDRV_PCM_HW_PARAMS_NO_PERIOD_WAKEUP);\n\n\tbits = snd_pcm_format_physical_width(runtime->format);\n\truntime->sample_bits = bits;\n\tbits *= runtime->channels;\n\truntime->frame_bits = bits;\n\tframes = 1;\n\twhile (bits % 8 != 0) {\n\t\tbits *= 2;\n\t\tframes *= 2;\n\t}\n\truntime->byte_align = bits / 8;\n\truntime->min_align = frames;\n\n\t/* Default sw params */\n\truntime->tstamp_mode = SNDRV_PCM_TSTAMP_NONE;\n\truntime->period_step = 1;\n\truntime->control->avail_min = runtime->period_size;\n\truntime->start_threshold = 1;\n\truntime->stop_threshold = runtime->buffer_size;\n\truntime->silence_threshold = 0;\n\truntime->silence_size = 0;\n\truntime->boundary = runtime->buffer_size;\n\twhile (runtime->boundary * 2 <= LONG_MAX - runtime->buffer_size)\n\t\truntime->boundary *= 2;\n\n\t/* clear the buffer for avoiding possible kernel info leaks */\n\tif (runtime->dma_area && !substream->ops->copy_user) {\n\t\tsize_t size = runtime->dma_bytes;\n\n\t\tif (runtime->info & SNDRV_PCM_INFO_MMAP)\n\t\t\tsize = PAGE_ALIGN(size);\n\t\tmemset(runtime->dma_area, 0, size);\n\t}\n\n\tsnd_pcm_timer_resolution_change(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_SETUP);\n\n\tif (cpu_latency_qos_request_active(&substream->latency_pm_qos_req))\n\t\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n\tusecs = period_to_usecs(runtime);\n\tif (usecs >= 0)\n\t\tcpu_latency_qos_add_request(&substream->latency_pm_qos_req,\n\t\t\t\t\t    usecs);\n\treturn 0;\n _error:\n\t/* hardware might be unusable from this time,\n\t   so we force application to retry to set\n\t   the correct hardware parameter settings */\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\tif (substream->ops->hw_free != NULL)\n\t\tsubstream->ops->hw_free(substream);\n\tif (substream->managed_buffer_alloc)\n\t\tsnd_pcm_lib_free_pages(substream);\n\treturn err;\n}",
        "code_after_change": "static int snd_pcm_hw_params(struct snd_pcm_substream *substream,\n\t\t\t     struct snd_pcm_hw_params *params)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint err = 0, usecs;\n\tunsigned int bits;\n\tsnd_pcm_uframes_t frames;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tmutex_lock(&runtime->buffer_mutex);\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_OPEN:\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tif (!is_oss_stream(substream) &&\n\t\t    atomic_read(&substream->mmap_count))\n\t\t\terr = -EBADFD;\n\t\tbreak;\n\tdefault:\n\t\terr = -EBADFD;\n\t\tbreak;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n\tif (err)\n\t\tgoto unlock;\n\n\tsnd_pcm_sync_stop(substream, true);\n\n\tparams->rmask = ~0U;\n\terr = snd_pcm_hw_refine(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = snd_pcm_hw_params_choose(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = fixup_unreferenced_params(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\tif (substream->managed_buffer_alloc) {\n\t\terr = snd_pcm_lib_malloc_pages(substream,\n\t\t\t\t\t       params_buffer_bytes(params));\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t\truntime->buffer_changed = err > 0;\n\t}\n\n\tif (substream->ops->hw_params != NULL) {\n\t\terr = substream->ops->hw_params(substream, params);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t}\n\n\truntime->access = params_access(params);\n\truntime->format = params_format(params);\n\truntime->subformat = params_subformat(params);\n\truntime->channels = params_channels(params);\n\truntime->rate = params_rate(params);\n\truntime->period_size = params_period_size(params);\n\truntime->periods = params_periods(params);\n\truntime->buffer_size = params_buffer_size(params);\n\truntime->info = params->info;\n\truntime->rate_num = params->rate_num;\n\truntime->rate_den = params->rate_den;\n\truntime->no_period_wakeup =\n\t\t\t(params->info & SNDRV_PCM_INFO_NO_PERIOD_WAKEUP) &&\n\t\t\t(params->flags & SNDRV_PCM_HW_PARAMS_NO_PERIOD_WAKEUP);\n\n\tbits = snd_pcm_format_physical_width(runtime->format);\n\truntime->sample_bits = bits;\n\tbits *= runtime->channels;\n\truntime->frame_bits = bits;\n\tframes = 1;\n\twhile (bits % 8 != 0) {\n\t\tbits *= 2;\n\t\tframes *= 2;\n\t}\n\truntime->byte_align = bits / 8;\n\truntime->min_align = frames;\n\n\t/* Default sw params */\n\truntime->tstamp_mode = SNDRV_PCM_TSTAMP_NONE;\n\truntime->period_step = 1;\n\truntime->control->avail_min = runtime->period_size;\n\truntime->start_threshold = 1;\n\truntime->stop_threshold = runtime->buffer_size;\n\truntime->silence_threshold = 0;\n\truntime->silence_size = 0;\n\truntime->boundary = runtime->buffer_size;\n\twhile (runtime->boundary * 2 <= LONG_MAX - runtime->buffer_size)\n\t\truntime->boundary *= 2;\n\n\t/* clear the buffer for avoiding possible kernel info leaks */\n\tif (runtime->dma_area && !substream->ops->copy_user) {\n\t\tsize_t size = runtime->dma_bytes;\n\n\t\tif (runtime->info & SNDRV_PCM_INFO_MMAP)\n\t\t\tsize = PAGE_ALIGN(size);\n\t\tmemset(runtime->dma_area, 0, size);\n\t}\n\n\tsnd_pcm_timer_resolution_change(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_SETUP);\n\n\tif (cpu_latency_qos_request_active(&substream->latency_pm_qos_req))\n\t\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n\tusecs = period_to_usecs(runtime);\n\tif (usecs >= 0)\n\t\tcpu_latency_qos_add_request(&substream->latency_pm_qos_req,\n\t\t\t\t\t    usecs);\n\terr = 0;\n _error:\n\tif (err) {\n\t\t/* hardware might be unusable from this time,\n\t\t * so we force application to retry to set\n\t\t * the correct hardware parameter settings\n\t\t */\n\t\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\t\tif (substream->ops->hw_free != NULL)\n\t\t\tsubstream->ops->hw_free(substream);\n\t\tif (substream->managed_buffer_alloc)\n\t\t\tsnd_pcm_lib_free_pages(substream);\n\t}\n unlock:\n\tmutex_unlock(&runtime->buffer_mutex);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,29 +2,30 @@\n \t\t\t     struct snd_pcm_hw_params *params)\n {\n \tstruct snd_pcm_runtime *runtime;\n-\tint err, usecs;\n+\tint err = 0, usecs;\n \tunsigned int bits;\n \tsnd_pcm_uframes_t frames;\n \n \tif (PCM_RUNTIME_CHECK(substream))\n \t\treturn -ENXIO;\n \truntime = substream->runtime;\n+\tmutex_lock(&runtime->buffer_mutex);\n \tsnd_pcm_stream_lock_irq(substream);\n \tswitch (runtime->status->state) {\n \tcase SNDRV_PCM_STATE_OPEN:\n \tcase SNDRV_PCM_STATE_SETUP:\n \tcase SNDRV_PCM_STATE_PREPARED:\n+\t\tif (!is_oss_stream(substream) &&\n+\t\t    atomic_read(&substream->mmap_count))\n+\t\t\terr = -EBADFD;\n \t\tbreak;\n \tdefault:\n-\t\tsnd_pcm_stream_unlock_irq(substream);\n-\t\treturn -EBADFD;\n+\t\terr = -EBADFD;\n+\t\tbreak;\n \t}\n \tsnd_pcm_stream_unlock_irq(substream);\n-#if IS_ENABLED(CONFIG_SND_PCM_OSS)\n-\tif (!substream->oss.oss)\n-#endif\n-\t\tif (atomic_read(&substream->mmap_count))\n-\t\t\treturn -EBADFD;\n+\tif (err)\n+\t\tgoto unlock;\n \n \tsnd_pcm_sync_stop(substream, true);\n \n@@ -112,15 +113,20 @@\n \tif (usecs >= 0)\n \t\tcpu_latency_qos_add_request(&substream->latency_pm_qos_req,\n \t\t\t\t\t    usecs);\n-\treturn 0;\n+\terr = 0;\n  _error:\n-\t/* hardware might be unusable from this time,\n-\t   so we force application to retry to set\n-\t   the correct hardware parameter settings */\n-\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n-\tif (substream->ops->hw_free != NULL)\n-\t\tsubstream->ops->hw_free(substream);\n-\tif (substream->managed_buffer_alloc)\n-\t\tsnd_pcm_lib_free_pages(substream);\n+\tif (err) {\n+\t\t/* hardware might be unusable from this time,\n+\t\t * so we force application to retry to set\n+\t\t * the correct hardware parameter settings\n+\t\t */\n+\t\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n+\t\tif (substream->ops->hw_free != NULL)\n+\t\t\tsubstream->ops->hw_free(substream);\n+\t\tif (substream->managed_buffer_alloc)\n+\t\t\tsnd_pcm_lib_free_pages(substream);\n+\t}\n+ unlock:\n+\tmutex_unlock(&runtime->buffer_mutex);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tint err = 0, usecs;",
                "\tmutex_lock(&runtime->buffer_mutex);",
                "\t\tif (!is_oss_stream(substream) &&",
                "\t\t    atomic_read(&substream->mmap_count))",
                "\t\t\terr = -EBADFD;",
                "\t\terr = -EBADFD;",
                "\t\tbreak;",
                "\tif (err)",
                "\t\tgoto unlock;",
                "\terr = 0;",
                "\tif (err) {",
                "\t\t/* hardware might be unusable from this time,",
                "\t\t * so we force application to retry to set",
                "\t\t * the correct hardware parameter settings",
                "\t\t */",
                "\t\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);",
                "\t\tif (substream->ops->hw_free != NULL)",
                "\t\t\tsubstream->ops->hw_free(substream);",
                "\t\tif (substream->managed_buffer_alloc)",
                "\t\t\tsnd_pcm_lib_free_pages(substream);",
                "\t}",
                " unlock:",
                "\tmutex_unlock(&runtime->buffer_mutex);"
            ],
            "deleted": [
                "\tint err, usecs;",
                "\t\tsnd_pcm_stream_unlock_irq(substream);",
                "\t\treturn -EBADFD;",
                "#if IS_ENABLED(CONFIG_SND_PCM_OSS)",
                "\tif (!substream->oss.oss)",
                "#endif",
                "\t\tif (atomic_read(&substream->mmap_count))",
                "\t\t\treturn -EBADFD;",
                "\treturn 0;",
                "\t/* hardware might be unusable from this time,",
                "\t   so we force application to retry to set",
                "\t   the correct hardware parameter settings */",
                "\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);",
                "\tif (substream->ops->hw_free != NULL)",
                "\t\tsubstream->ops->hw_free(substream);",
                "\tif (substream->managed_buffer_alloc)",
                "\t\tsnd_pcm_lib_free_pages(substream);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s sound subsystem in the way a user triggers concurrent calls of PCM hw_params. The hw_free ioctls or similar race condition happens inside ALSA PCM for other ioctls. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3245
    },
    {
        "cve_id": "CVE-2022-32250",
        "code_before_change": "static struct nft_expr *nft_expr_init(const struct nft_ctx *ctx,\n\t\t\t\t      const struct nlattr *nla)\n{\n\tstruct nft_expr_info expr_info;\n\tstruct nft_expr *expr;\n\tstruct module *owner;\n\tint err;\n\n\terr = nf_tables_expr_parse(ctx, nla, &expr_info);\n\tif (err < 0)\n\t\tgoto err1;\n\n\terr = -ENOMEM;\n\texpr = kzalloc(expr_info.ops->size, GFP_KERNEL_ACCOUNT);\n\tif (expr == NULL)\n\t\tgoto err2;\n\n\terr = nf_tables_newexpr(ctx, &expr_info, expr);\n\tif (err < 0)\n\t\tgoto err3;\n\n\treturn expr;\nerr3:\n\tkfree(expr);\nerr2:\n\towner = expr_info.ops->type->owner;\n\tif (expr_info.ops->type->release_ops)\n\t\texpr_info.ops->type->release_ops(expr_info.ops);\n\n\tmodule_put(owner);\nerr1:\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "static struct nft_expr *nft_expr_init(const struct nft_ctx *ctx,\n\t\t\t\t      const struct nlattr *nla)\n{\n\tstruct nft_expr_info expr_info;\n\tstruct nft_expr *expr;\n\tstruct module *owner;\n\tint err;\n\n\terr = nf_tables_expr_parse(ctx, nla, &expr_info);\n\tif (err < 0)\n\t\tgoto err_expr_parse;\n\n\terr = -EOPNOTSUPP;\n\tif (!(expr_info.ops->type->flags & NFT_EXPR_STATEFUL))\n\t\tgoto err_expr_stateful;\n\n\terr = -ENOMEM;\n\texpr = kzalloc(expr_info.ops->size, GFP_KERNEL_ACCOUNT);\n\tif (expr == NULL)\n\t\tgoto err_expr_stateful;\n\n\terr = nf_tables_newexpr(ctx, &expr_info, expr);\n\tif (err < 0)\n\t\tgoto err_expr_new;\n\n\treturn expr;\nerr_expr_new:\n\tkfree(expr);\nerr_expr_stateful:\n\towner = expr_info.ops->type->owner;\n\tif (expr_info.ops->type->release_ops)\n\t\texpr_info.ops->type->release_ops(expr_info.ops);\n\n\tmodule_put(owner);\nerr_expr_parse:\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,26 +8,30 @@\n \n \terr = nf_tables_expr_parse(ctx, nla, &expr_info);\n \tif (err < 0)\n-\t\tgoto err1;\n+\t\tgoto err_expr_parse;\n+\n+\terr = -EOPNOTSUPP;\n+\tif (!(expr_info.ops->type->flags & NFT_EXPR_STATEFUL))\n+\t\tgoto err_expr_stateful;\n \n \terr = -ENOMEM;\n \texpr = kzalloc(expr_info.ops->size, GFP_KERNEL_ACCOUNT);\n \tif (expr == NULL)\n-\t\tgoto err2;\n+\t\tgoto err_expr_stateful;\n \n \terr = nf_tables_newexpr(ctx, &expr_info, expr);\n \tif (err < 0)\n-\t\tgoto err3;\n+\t\tgoto err_expr_new;\n \n \treturn expr;\n-err3:\n+err_expr_new:\n \tkfree(expr);\n-err2:\n+err_expr_stateful:\n \towner = expr_info.ops->type->owner;\n \tif (expr_info.ops->type->release_ops)\n \t\texpr_info.ops->type->release_ops(expr_info.ops);\n \n \tmodule_put(owner);\n-err1:\n+err_expr_parse:\n \treturn ERR_PTR(err);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err_expr_parse;",
                "",
                "\terr = -EOPNOTSUPP;",
                "\tif (!(expr_info.ops->type->flags & NFT_EXPR_STATEFUL))",
                "\t\tgoto err_expr_stateful;",
                "\t\tgoto err_expr_stateful;",
                "\t\tgoto err_expr_new;",
                "err_expr_new:",
                "err_expr_stateful:",
                "err_expr_parse:"
            ],
            "deleted": [
                "\t\tgoto err1;",
                "\t\tgoto err2;",
                "\t\tgoto err3;",
                "err3:",
                "err2:",
                "err1:"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "net/netfilter/nf_tables_api.c in the Linux kernel through 5.18.1 allows a local user (able to create user/net namespaces) to escalate privileges to root because an incorrect NFT_STATEFUL_EXPR check leads to a use-after-free.",
        "id": 3569
    },
    {
        "cve_id": "CVE-2022-1976",
        "code_before_change": "static s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)\n{\n\tif (tracked)\n\t\treturn 0;\n\treturn percpu_counter_sum(&tctx->inflight);\n}",
        "code_after_change": "static s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)\n{\n\tif (tracked)\n\t\treturn atomic_read(&tctx->inflight_tracked);\n\treturn percpu_counter_sum(&tctx->inflight);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)\n {\n \tif (tracked)\n-\t\treturn 0;\n+\t\treturn atomic_read(&tctx->inflight_tracked);\n \treturn percpu_counter_sum(&tctx->inflight);\n }",
        "function_modified_lines": {
            "added": [
                "\t\treturn atomic_read(&tctx->inflight_tracked);"
            ],
            "deleted": [
                "\t\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel\u2019s implementation of IO-URING. This flaw allows an attacker with local executable permission to create a string of requests that can cause a use-after-free flaw within the kernel. This issue leads to memory corruption and possible privilege escalation.",
        "id": 3322
    },
    {
        "cve_id": "CVE-2022-1976",
        "code_before_change": "static bool io_match_task(struct io_kiocb *head, struct task_struct *task,\n\t\t\t  bool cancel_all)\n\t__must_hold(&req->ctx->timeout_lock)\n{\n\tif (task && head->task != task)\n\t\treturn false;\n\treturn cancel_all;\n}",
        "code_after_change": "static bool io_match_task(struct io_kiocb *head, struct task_struct *task,\n\t\t\t  bool cancel_all)\n\t__must_hold(&req->ctx->timeout_lock)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (cancel_all)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (req->flags & REQ_F_INFLIGHT)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,16 @@\n \t\t\t  bool cancel_all)\n \t__must_hold(&req->ctx->timeout_lock)\n {\n+\tstruct io_kiocb *req;\n+\n \tif (task && head->task != task)\n \t\treturn false;\n-\treturn cancel_all;\n+\tif (cancel_all)\n+\t\treturn true;\n+\n+\tio_for_each_link(req, head) {\n+\t\tif (req->flags & REQ_F_INFLIGHT)\n+\t\t\treturn true;\n+\t}\n+\treturn false;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct io_kiocb *req;",
                "",
                "\tif (cancel_all)",
                "\t\treturn true;",
                "",
                "\tio_for_each_link(req, head) {",
                "\t\tif (req->flags & REQ_F_INFLIGHT)",
                "\t\t\treturn true;",
                "\t}",
                "\treturn false;"
            ],
            "deleted": [
                "\treturn cancel_all;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel\u2019s implementation of IO-URING. This flaw allows an attacker with local executable permission to create a string of requests that can cause a use-after-free flaw within the kernel. This issue leads to memory corruption and possible privilege escalation.",
        "id": 3329
    },
    {
        "cve_id": "CVE-2018-20836",
        "code_before_change": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n\n\tcomplete(&task->slow_task->completion);\n}",
        "code_after_change": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\t\tcomplete(&task->slow_task->completion);\n\t}\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,9 +5,9 @@\n \tunsigned long flags;\n \n \tspin_lock_irqsave(&task->task_state_lock, flags);\n-\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n+\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n \t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n+\t\tcomplete(&task->slow_task->completion);\n+\t}\n \tspin_unlock_irqrestore(&task->task_state_lock, flags);\n-\n-\tcomplete(&task->slow_task->completion);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {",
                "\t\tcomplete(&task->slow_task->completion);",
                "\t}"
            ],
            "deleted": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))",
                "",
                "\tcomplete(&task->slow_task->completion);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.20. There is a race condition in smp_task_timedout() and smp_task_done() in drivers/scsi/libsas/sas_expander.c, leading to a use-after-free.",
        "id": 1782
    },
    {
        "cve_id": "CVE-2019-25045",
        "code_before_change": "void xfrm_state_fini(struct net *net)\n{\n\tunsigned int sz;\n\n\tflush_work(&net->xfrm.state_hash_work);\n\tflush_work(&xfrm_state_gc_work);\n\txfrm_state_flush(net, IPSEC_PROTO_ANY, false, true);\n\n\tWARN_ON(!list_empty(&net->xfrm.state_all));\n\n\tsz = (net->xfrm.state_hmask + 1) * sizeof(struct hlist_head);\n\tWARN_ON(!hlist_empty(net->xfrm.state_byspi));\n\txfrm_hash_free(net->xfrm.state_byspi, sz);\n\tWARN_ON(!hlist_empty(net->xfrm.state_bysrc));\n\txfrm_hash_free(net->xfrm.state_bysrc, sz);\n\tWARN_ON(!hlist_empty(net->xfrm.state_bydst));\n\txfrm_hash_free(net->xfrm.state_bydst, sz);\n}",
        "code_after_change": "void xfrm_state_fini(struct net *net)\n{\n\tunsigned int sz;\n\n\tflush_work(&net->xfrm.state_hash_work);\n\tflush_work(&xfrm_state_gc_work);\n\txfrm_state_flush(net, 0, false, true);\n\n\tWARN_ON(!list_empty(&net->xfrm.state_all));\n\n\tsz = (net->xfrm.state_hmask + 1) * sizeof(struct hlist_head);\n\tWARN_ON(!hlist_empty(net->xfrm.state_byspi));\n\txfrm_hash_free(net->xfrm.state_byspi, sz);\n\tWARN_ON(!hlist_empty(net->xfrm.state_bysrc));\n\txfrm_hash_free(net->xfrm.state_bysrc, sz);\n\tWARN_ON(!hlist_empty(net->xfrm.state_bydst));\n\txfrm_hash_free(net->xfrm.state_bydst, sz);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \n \tflush_work(&net->xfrm.state_hash_work);\n \tflush_work(&xfrm_state_gc_work);\n-\txfrm_state_flush(net, IPSEC_PROTO_ANY, false, true);\n+\txfrm_state_flush(net, 0, false, true);\n \n \tWARN_ON(!list_empty(&net->xfrm.state_all));\n ",
        "function_modified_lines": {
            "added": [
                "\txfrm_state_flush(net, 0, false, true);"
            ],
            "deleted": [
                "\txfrm_state_flush(net, IPSEC_PROTO_ANY, false, true);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.19. The XFRM subsystem has a use-after-free, related to an xfrm_state_fini panic, aka CID-dbb2483b2a46.",
        "id": 2304
    },
    {
        "cve_id": "CVE-2019-25045",
        "code_before_change": "static int validate_tmpl(int nr, struct xfrm_user_tmpl *ut, u16 family)\n{\n\tu16 prev_family;\n\tint i;\n\n\tif (nr > XFRM_MAX_DEPTH)\n\t\treturn -EINVAL;\n\n\tprev_family = family;\n\n\tfor (i = 0; i < nr; i++) {\n\t\t/* We never validated the ut->family value, so many\n\t\t * applications simply leave it at zero.  The check was\n\t\t * never made and ut->family was ignored because all\n\t\t * templates could be assumed to have the same family as\n\t\t * the policy itself.  Now that we will have ipv4-in-ipv6\n\t\t * and ipv6-in-ipv4 tunnels, this is no longer true.\n\t\t */\n\t\tif (!ut[i].family)\n\t\t\tut[i].family = family;\n\n\t\tswitch (ut[i].mode) {\n\t\tcase XFRM_MODE_TUNNEL:\n\t\tcase XFRM_MODE_BEET:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (ut[i].family != prev_family)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (ut[i].mode >= XFRM_MODE_MAX)\n\t\t\treturn -EINVAL;\n\n\t\tprev_family = ut[i].family;\n\n\t\tswitch (ut[i].family) {\n\t\tcase AF_INET:\n\t\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tcase AF_INET6:\n\t\t\tbreak;\n#endif\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tswitch (ut[i].id.proto) {\n\t\tcase IPPROTO_AH:\n\t\tcase IPPROTO_ESP:\n\t\tcase IPPROTO_COMP:\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tcase IPPROTO_ROUTING:\n\t\tcase IPPROTO_DSTOPTS:\n#endif\n\t\tcase IPSEC_PROTO_ANY:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int validate_tmpl(int nr, struct xfrm_user_tmpl *ut, u16 family)\n{\n\tu16 prev_family;\n\tint i;\n\n\tif (nr > XFRM_MAX_DEPTH)\n\t\treturn -EINVAL;\n\n\tprev_family = family;\n\n\tfor (i = 0; i < nr; i++) {\n\t\t/* We never validated the ut->family value, so many\n\t\t * applications simply leave it at zero.  The check was\n\t\t * never made and ut->family was ignored because all\n\t\t * templates could be assumed to have the same family as\n\t\t * the policy itself.  Now that we will have ipv4-in-ipv6\n\t\t * and ipv6-in-ipv4 tunnels, this is no longer true.\n\t\t */\n\t\tif (!ut[i].family)\n\t\t\tut[i].family = family;\n\n\t\tswitch (ut[i].mode) {\n\t\tcase XFRM_MODE_TUNNEL:\n\t\tcase XFRM_MODE_BEET:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (ut[i].family != prev_family)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (ut[i].mode >= XFRM_MODE_MAX)\n\t\t\treturn -EINVAL;\n\n\t\tprev_family = ut[i].family;\n\n\t\tswitch (ut[i].family) {\n\t\tcase AF_INET:\n\t\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tcase AF_INET6:\n\t\t\tbreak;\n#endif\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!xfrm_id_proto_valid(ut[i].id.proto))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -44,20 +44,8 @@\n \t\t\treturn -EINVAL;\n \t\t}\n \n-\t\tswitch (ut[i].id.proto) {\n-\t\tcase IPPROTO_AH:\n-\t\tcase IPPROTO_ESP:\n-\t\tcase IPPROTO_COMP:\n-#if IS_ENABLED(CONFIG_IPV6)\n-\t\tcase IPPROTO_ROUTING:\n-\t\tcase IPPROTO_DSTOPTS:\n-#endif\n-\t\tcase IPSEC_PROTO_ANY:\n-\t\t\tbreak;\n-\t\tdefault:\n+\t\tif (!xfrm_id_proto_valid(ut[i].id.proto))\n \t\t\treturn -EINVAL;\n-\t\t}\n-\n \t}\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!xfrm_id_proto_valid(ut[i].id.proto))"
            ],
            "deleted": [
                "\t\tswitch (ut[i].id.proto) {",
                "\t\tcase IPPROTO_AH:",
                "\t\tcase IPPROTO_ESP:",
                "\t\tcase IPPROTO_COMP:",
                "#if IS_ENABLED(CONFIG_IPV6)",
                "\t\tcase IPPROTO_ROUTING:",
                "\t\tcase IPPROTO_DSTOPTS:",
                "#endif",
                "\t\tcase IPSEC_PROTO_ANY:",
                "\t\t\tbreak;",
                "\t\tdefault:",
                "\t\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.19. The XFRM subsystem has a use-after-free, related to an xfrm_state_fini panic, aka CID-dbb2483b2a46.",
        "id": 2305
    },
    {
        "cve_id": "CVE-2023-1079",
        "code_before_change": "static void asus_kbd_backlight_set(struct led_classdev *led_cdev,\n\t\t\t\t   enum led_brightness brightness)\n{\n\tstruct asus_kbd_leds *led = container_of(led_cdev, struct asus_kbd_leds,\n\t\t\t\t\t\t cdev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&led->lock, flags);\n\tled->brightness = brightness;\n\tspin_unlock_irqrestore(&led->lock, flags);\n\n\tschedule_work(&led->work);\n}",
        "code_after_change": "static void asus_kbd_backlight_set(struct led_classdev *led_cdev,\n\t\t\t\t   enum led_brightness brightness)\n{\n\tstruct asus_kbd_leds *led = container_of(led_cdev, struct asus_kbd_leds,\n\t\t\t\t\t\t cdev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&led->lock, flags);\n\tled->brightness = brightness;\n\tspin_unlock_irqrestore(&led->lock, flags);\n\n\tasus_schedule_work(led);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,5 +9,5 @@\n \tled->brightness = brightness;\n \tspin_unlock_irqrestore(&led->lock, flags);\n \n-\tschedule_work(&led->work);\n+\tasus_schedule_work(led);\n }",
        "function_modified_lines": {
            "added": [
                "\tasus_schedule_work(led);"
            ],
            "deleted": [
                "\tschedule_work(&led->work);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A use-after-free may be triggered in asus_kbd_backlight_set when plugging/disconnecting in a malicious USB device, which advertises itself as an Asus device. Similarly to the previous known CVE-2023-25012, but in asus devices, the work_struct may be scheduled by the LED controller while the device is disconnecting, triggering a use-after-free on the struct asus_kbd_leds *led structure. A malicious USB device may exploit the issue to cause memory corruption with controlled data.",
        "id": 3846
    },
    {
        "cve_id": "CVE-2019-15220",
        "code_before_change": "static int p54u_probe(struct usb_interface *intf,\n\t\t\t\tconst struct usb_device_id *id)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct ieee80211_hw *dev;\n\tstruct p54u_priv *priv;\n\tint err;\n\tunsigned int i, recognized_pipes;\n\n\tdev = p54_init_common(sizeof(*priv));\n\n\tif (!dev) {\n\t\tdev_err(&udev->dev, \"(p54usb) ieee80211 alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv = dev->priv;\n\tpriv->hw_type = P54U_INVALID_HW;\n\n\tSET_IEEE80211_DEV(dev, &intf->dev);\n\tusb_set_intfdata(intf, dev);\n\tpriv->udev = udev;\n\tpriv->intf = intf;\n\tskb_queue_head_init(&priv->rx_queue);\n\tinit_usb_anchor(&priv->submitted);\n\n\tusb_get_dev(udev);\n\n\t/* really lazy and simple way of figuring out if we're a 3887 */\n\t/* TODO: should just stick the identification in the device table */\n\ti = intf->altsetting->desc.bNumEndpoints;\n\trecognized_pipes = 0;\n\twhile (i--) {\n\t\tswitch (intf->altsetting->endpoint[i].desc.bEndpointAddress) {\n\t\tcase P54U_PIPE_DATA:\n\t\tcase P54U_PIPE_MGMT:\n\t\tcase P54U_PIPE_BRG:\n\t\tcase P54U_PIPE_DEV:\n\t\tcase P54U_PIPE_DATA | USB_DIR_IN:\n\t\tcase P54U_PIPE_MGMT | USB_DIR_IN:\n\t\tcase P54U_PIPE_BRG | USB_DIR_IN:\n\t\tcase P54U_PIPE_DEV | USB_DIR_IN:\n\t\tcase P54U_PIPE_INT | USB_DIR_IN:\n\t\t\trecognized_pipes++;\n\t\t}\n\t}\n\tpriv->common.open = p54u_open;\n\tpriv->common.stop = p54u_stop;\n\tif (recognized_pipes < P54U_PIPE_NUMBER) {\n#ifdef CONFIG_PM\n\t\t/* ISL3887 needs a full reset on resume */\n\t\tudev->reset_resume = 1;\n#endif /* CONFIG_PM */\n\t\terr = p54u_device_reset(dev);\n\n\t\tpriv->hw_type = P54U_3887;\n\t\tdev->extra_tx_headroom += sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_lm87;\n\t\tpriv->upload_fw = p54u_upload_firmware_3887;\n\t} else {\n\t\tpriv->hw_type = P54U_NET2280;\n\t\tdev->extra_tx_headroom += sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_net2280;\n\t\tpriv->upload_fw = p54u_upload_firmware_net2280;\n\t}\n\terr = p54u_load_firmware(dev, intf);\n\tif (err) {\n\t\tusb_put_dev(udev);\n\t\tp54_free_common(dev);\n\t}\n\treturn err;\n}",
        "code_after_change": "static int p54u_probe(struct usb_interface *intf,\n\t\t\t\tconst struct usb_device_id *id)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct ieee80211_hw *dev;\n\tstruct p54u_priv *priv;\n\tint err;\n\tunsigned int i, recognized_pipes;\n\n\tdev = p54_init_common(sizeof(*priv));\n\n\tif (!dev) {\n\t\tdev_err(&udev->dev, \"(p54usb) ieee80211 alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv = dev->priv;\n\tpriv->hw_type = P54U_INVALID_HW;\n\n\tSET_IEEE80211_DEV(dev, &intf->dev);\n\tusb_set_intfdata(intf, dev);\n\tpriv->udev = udev;\n\tpriv->intf = intf;\n\tskb_queue_head_init(&priv->rx_queue);\n\tinit_usb_anchor(&priv->submitted);\n\n\t/* really lazy and simple way of figuring out if we're a 3887 */\n\t/* TODO: should just stick the identification in the device table */\n\ti = intf->altsetting->desc.bNumEndpoints;\n\trecognized_pipes = 0;\n\twhile (i--) {\n\t\tswitch (intf->altsetting->endpoint[i].desc.bEndpointAddress) {\n\t\tcase P54U_PIPE_DATA:\n\t\tcase P54U_PIPE_MGMT:\n\t\tcase P54U_PIPE_BRG:\n\t\tcase P54U_PIPE_DEV:\n\t\tcase P54U_PIPE_DATA | USB_DIR_IN:\n\t\tcase P54U_PIPE_MGMT | USB_DIR_IN:\n\t\tcase P54U_PIPE_BRG | USB_DIR_IN:\n\t\tcase P54U_PIPE_DEV | USB_DIR_IN:\n\t\tcase P54U_PIPE_INT | USB_DIR_IN:\n\t\t\trecognized_pipes++;\n\t\t}\n\t}\n\tpriv->common.open = p54u_open;\n\tpriv->common.stop = p54u_stop;\n\tif (recognized_pipes < P54U_PIPE_NUMBER) {\n#ifdef CONFIG_PM\n\t\t/* ISL3887 needs a full reset on resume */\n\t\tudev->reset_resume = 1;\n#endif /* CONFIG_PM */\n\t\terr = p54u_device_reset(dev);\n\n\t\tpriv->hw_type = P54U_3887;\n\t\tdev->extra_tx_headroom += sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_lm87;\n\t\tpriv->upload_fw = p54u_upload_firmware_3887;\n\t} else {\n\t\tpriv->hw_type = P54U_NET2280;\n\t\tdev->extra_tx_headroom += sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_net2280;\n\t\tpriv->upload_fw = p54u_upload_firmware_net2280;\n\t}\n\terr = p54u_load_firmware(dev, intf);\n\tif (err)\n\t\tp54_free_common(dev);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,8 +23,6 @@\n \tpriv->intf = intf;\n \tskb_queue_head_init(&priv->rx_queue);\n \tinit_usb_anchor(&priv->submitted);\n-\n-\tusb_get_dev(udev);\n \n \t/* really lazy and simple way of figuring out if we're a 3887 */\n \t/* TODO: should just stick the identification in the device table */\n@@ -66,9 +64,7 @@\n \t\tpriv->upload_fw = p54u_upload_firmware_net2280;\n \t}\n \terr = p54u_load_firmware(dev, intf);\n-\tif (err) {\n-\t\tusb_put_dev(udev);\n+\tif (err)\n \t\tp54_free_common(dev);\n-\t}\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (err)"
            ],
            "deleted": [
                "",
                "\tusb_get_dev(udev);",
                "\tif (err) {",
                "\t\tusb_put_dev(udev);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.1. There is a use-after-free caused by a malicious USB device in the drivers/net/wireless/intersil/p54/p54usb.c driver.",
        "id": 2003
    },
    {
        "cve_id": "CVE-2019-15220",
        "code_before_change": "static void p54u_disconnect(struct usb_interface *intf)\n{\n\tstruct ieee80211_hw *dev = usb_get_intfdata(intf);\n\tstruct p54u_priv *priv;\n\n\tif (!dev)\n\t\treturn;\n\n\tpriv = dev->priv;\n\twait_for_completion(&priv->fw_wait_load);\n\tp54_unregister_common(dev);\n\n\tusb_put_dev(interface_to_usbdev(intf));\n\trelease_firmware(priv->fw);\n\tp54_free_common(dev);\n}",
        "code_after_change": "static void p54u_disconnect(struct usb_interface *intf)\n{\n\tstruct ieee80211_hw *dev = usb_get_intfdata(intf);\n\tstruct p54u_priv *priv;\n\n\tif (!dev)\n\t\treturn;\n\n\tpriv = dev->priv;\n\twait_for_completion(&priv->fw_wait_load);\n\tp54_unregister_common(dev);\n\n\trelease_firmware(priv->fw);\n\tp54_free_common(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,6 @@\n \twait_for_completion(&priv->fw_wait_load);\n \tp54_unregister_common(dev);\n \n-\tusb_put_dev(interface_to_usbdev(intf));\n \trelease_firmware(priv->fw);\n \tp54_free_common(dev);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tusb_put_dev(interface_to_usbdev(intf));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.1. There is a use-after-free caused by a malicious USB device in the drivers/net/wireless/intersil/p54/p54usb.c driver.",
        "id": 2005
    },
    {
        "cve_id": "CVE-2014-2568",
        "code_before_change": "static int queue_userspace_packet(struct datapath *dp, struct sk_buff *skb,\n\t\t\t\t  const struct dp_upcall_info *upcall_info)\n{\n\tstruct ovs_header *upcall;\n\tstruct sk_buff *nskb = NULL;\n\tstruct sk_buff *user_skb; /* to be queued to userspace */\n\tstruct nlattr *nla;\n\tstruct genl_info info = {\n\t\t.dst_sk = ovs_dp_get_net(dp)->genl_sock,\n\t\t.snd_portid = upcall_info->portid,\n\t};\n\tsize_t len;\n\tunsigned int hlen;\n\tint err, dp_ifindex;\n\n\tdp_ifindex = get_dpifindex(dp);\n\tif (!dp_ifindex)\n\t\treturn -ENODEV;\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb = __vlan_put_tag(nskb, nskb->vlan_proto, vlan_tx_tag_get(nskb));\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb->vlan_tci = 0;\n\t\tskb = nskb;\n\t}\n\n\tif (nla_attr_size(skb->len) > USHRT_MAX) {\n\t\terr = -EFBIG;\n\t\tgoto out;\n\t}\n\n\t/* Complete checksum if needed */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t    (err = skb_checksum_help(skb)))\n\t\tgoto out;\n\n\t/* Older versions of OVS user space enforce alignment of the last\n\t * Netlink attribute to NLA_ALIGNTO which would require extensive\n\t * padding logic. Only perform zerocopy if padding is not required.\n\t */\n\tif (dp->user_features & OVS_DP_F_UNALIGNED)\n\t\thlen = skb_zerocopy_headlen(skb);\n\telse\n\t\thlen = skb->len;\n\n\tlen = upcall_msg_size(upcall_info->userdata, hlen);\n\tuser_skb = genlmsg_new_unicast(len, &info, GFP_ATOMIC);\n\tif (!user_skb) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tupcall = genlmsg_put(user_skb, 0, 0, &dp_packet_genl_family,\n\t\t\t     0, upcall_info->cmd);\n\tupcall->dp_ifindex = dp_ifindex;\n\n\tnla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY);\n\tovs_nla_put_flow(upcall_info->key, upcall_info->key, user_skb);\n\tnla_nest_end(user_skb, nla);\n\n\tif (upcall_info->userdata)\n\t\t__nla_put(user_skb, OVS_PACKET_ATTR_USERDATA,\n\t\t\t  nla_len(upcall_info->userdata),\n\t\t\t  nla_data(upcall_info->userdata));\n\n\t/* Only reserve room for attribute header, packet data is added\n\t * in skb_zerocopy() */\n\tif (!(nla = nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, 0))) {\n\t\terr = -ENOBUFS;\n\t\tgoto out;\n\t}\n\tnla->nla_len = nla_attr_size(skb->len);\n\n\tskb_zerocopy(user_skb, skb, skb->len, hlen);\n\n\t/* Pad OVS_PACKET_ATTR_PACKET if linear copy was performed */\n\tif (!(dp->user_features & OVS_DP_F_UNALIGNED)) {\n\t\tsize_t plen = NLA_ALIGN(user_skb->len) - user_skb->len;\n\n\t\tif (plen > 0)\n\t\t\tmemset(skb_put(user_skb, plen), 0, plen);\n\t}\n\n\t((struct nlmsghdr *) user_skb->data)->nlmsg_len = user_skb->len;\n\n\terr = genlmsg_unicast(ovs_dp_get_net(dp), user_skb, upcall_info->portid);\nout:\n\tkfree_skb(nskb);\n\treturn err;\n}",
        "code_after_change": "static int queue_userspace_packet(struct datapath *dp, struct sk_buff *skb,\n\t\t\t\t  const struct dp_upcall_info *upcall_info)\n{\n\tstruct ovs_header *upcall;\n\tstruct sk_buff *nskb = NULL;\n\tstruct sk_buff *user_skb; /* to be queued to userspace */\n\tstruct nlattr *nla;\n\tstruct genl_info info = {\n\t\t.dst_sk = ovs_dp_get_net(dp)->genl_sock,\n\t\t.snd_portid = upcall_info->portid,\n\t};\n\tsize_t len;\n\tunsigned int hlen;\n\tint err, dp_ifindex;\n\n\tdp_ifindex = get_dpifindex(dp);\n\tif (!dp_ifindex)\n\t\treturn -ENODEV;\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb = __vlan_put_tag(nskb, nskb->vlan_proto, vlan_tx_tag_get(nskb));\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb->vlan_tci = 0;\n\t\tskb = nskb;\n\t}\n\n\tif (nla_attr_size(skb->len) > USHRT_MAX) {\n\t\terr = -EFBIG;\n\t\tgoto out;\n\t}\n\n\t/* Complete checksum if needed */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t    (err = skb_checksum_help(skb)))\n\t\tgoto out;\n\n\t/* Older versions of OVS user space enforce alignment of the last\n\t * Netlink attribute to NLA_ALIGNTO which would require extensive\n\t * padding logic. Only perform zerocopy if padding is not required.\n\t */\n\tif (dp->user_features & OVS_DP_F_UNALIGNED)\n\t\thlen = skb_zerocopy_headlen(skb);\n\telse\n\t\thlen = skb->len;\n\n\tlen = upcall_msg_size(upcall_info->userdata, hlen);\n\tuser_skb = genlmsg_new_unicast(len, &info, GFP_ATOMIC);\n\tif (!user_skb) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tupcall = genlmsg_put(user_skb, 0, 0, &dp_packet_genl_family,\n\t\t\t     0, upcall_info->cmd);\n\tupcall->dp_ifindex = dp_ifindex;\n\n\tnla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY);\n\tovs_nla_put_flow(upcall_info->key, upcall_info->key, user_skb);\n\tnla_nest_end(user_skb, nla);\n\n\tif (upcall_info->userdata)\n\t\t__nla_put(user_skb, OVS_PACKET_ATTR_USERDATA,\n\t\t\t  nla_len(upcall_info->userdata),\n\t\t\t  nla_data(upcall_info->userdata));\n\n\t/* Only reserve room for attribute header, packet data is added\n\t * in skb_zerocopy() */\n\tif (!(nla = nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, 0))) {\n\t\terr = -ENOBUFS;\n\t\tgoto out;\n\t}\n\tnla->nla_len = nla_attr_size(skb->len);\n\n\terr = skb_zerocopy(user_skb, skb, skb->len, hlen);\n\tif (err)\n\t\tgoto out;\n\n\t/* Pad OVS_PACKET_ATTR_PACKET if linear copy was performed */\n\tif (!(dp->user_features & OVS_DP_F_UNALIGNED)) {\n\t\tsize_t plen = NLA_ALIGN(user_skb->len) - user_skb->len;\n\n\t\tif (plen > 0)\n\t\t\tmemset(skb_put(user_skb, plen), 0, plen);\n\t}\n\n\t((struct nlmsghdr *) user_skb->data)->nlmsg_len = user_skb->len;\n\n\terr = genlmsg_unicast(ovs_dp_get_net(dp), user_skb, upcall_info->portid);\nout:\n\tif (err)\n\t\tskb_tx_error(skb);\n\tkfree_skb(nskb);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -77,7 +77,9 @@\n \t}\n \tnla->nla_len = nla_attr_size(skb->len);\n \n-\tskb_zerocopy(user_skb, skb, skb->len, hlen);\n+\terr = skb_zerocopy(user_skb, skb, skb->len, hlen);\n+\tif (err)\n+\t\tgoto out;\n \n \t/* Pad OVS_PACKET_ATTR_PACKET if linear copy was performed */\n \tif (!(dp->user_features & OVS_DP_F_UNALIGNED)) {\n@@ -91,6 +93,8 @@\n \n \terr = genlmsg_unicast(ovs_dp_get_net(dp), user_skb, upcall_info->portid);\n out:\n+\tif (err)\n+\t\tskb_tx_error(skb);\n \tkfree_skb(nskb);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\terr = skb_zerocopy(user_skb, skb, skb->len, hlen);",
                "\tif (err)",
                "\t\tgoto out;",
                "\tif (err)",
                "\t\tskb_tx_error(skb);"
            ],
            "deleted": [
                "\tskb_zerocopy(user_skb, skb, skb->len, hlen);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in the nfqnl_zcopy function in net/netfilter/nfnetlink_queue_core.c in the Linux kernel through 3.13.6 allows attackers to obtain sensitive information from kernel memory by leveraging the absence of a certain orphaning operation. NOTE: the affected code was moved to the skb_zerocopy function in net/core/skbuff.c before the vulnerability was announced.",
        "id": 485
    },
    {
        "cve_id": "CVE-2014-4653",
        "code_before_change": "static int snd_ctl_tlv_ioctl(struct snd_ctl_file *file,\n                             struct snd_ctl_tlv __user *_tlv,\n                             int op_flag)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_ctl_tlv tlv;\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int len;\n\tint err = 0;\n\n\tif (copy_from_user(&tlv, _tlv, sizeof(tlv)))\n\t\treturn -EFAULT;\n\tif (tlv.length < sizeof(unsigned int) * 2)\n\t\treturn -EINVAL;\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_numid(card, tlv.numid);\n\tif (kctl == NULL) {\n\t\terr = -ENOENT;\n\t\tgoto __kctl_end;\n\t}\n\tif (kctl->tlv.p == NULL) {\n\t\terr = -ENXIO;\n\t\tgoto __kctl_end;\n\t}\n\tvd = &kctl->vd[tlv.numid - kctl->id.numid];\n\tif ((op_flag == 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_READ) == 0) ||\n\t    (op_flag > 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_WRITE) == 0) ||\n\t    (op_flag < 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_COMMAND) == 0)) {\n\t    \terr = -ENXIO;\n\t    \tgoto __kctl_end;\n\t}\n\tif (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK) {\n\t\tif (vd->owner != NULL && vd->owner != file) {\n\t\t\terr = -EPERM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\terr = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);\n\t\tif (err > 0) {\n\t\t\tup_read(&card->controls_rwsem);\n\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &kctl->id);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tif (op_flag) {\n\t\t\terr = -ENXIO;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tlen = kctl->tlv.p[1] + 2 * sizeof(unsigned int);\n\t\tif (tlv.length < len) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tif (copy_to_user(_tlv->tlv, kctl->tlv.p, len))\n\t\t\terr = -EFAULT;\n\t}\n      __kctl_end:\n\tup_read(&card->controls_rwsem);\n\treturn err;\n}",
        "code_after_change": "static int snd_ctl_tlv_ioctl(struct snd_ctl_file *file,\n                             struct snd_ctl_tlv __user *_tlv,\n                             int op_flag)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_ctl_tlv tlv;\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int len;\n\tint err = 0;\n\n\tif (copy_from_user(&tlv, _tlv, sizeof(tlv)))\n\t\treturn -EFAULT;\n\tif (tlv.length < sizeof(unsigned int) * 2)\n\t\treturn -EINVAL;\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_numid(card, tlv.numid);\n\tif (kctl == NULL) {\n\t\terr = -ENOENT;\n\t\tgoto __kctl_end;\n\t}\n\tif (kctl->tlv.p == NULL) {\n\t\terr = -ENXIO;\n\t\tgoto __kctl_end;\n\t}\n\tvd = &kctl->vd[tlv.numid - kctl->id.numid];\n\tif ((op_flag == 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_READ) == 0) ||\n\t    (op_flag > 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_WRITE) == 0) ||\n\t    (op_flag < 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_COMMAND) == 0)) {\n\t    \terr = -ENXIO;\n\t    \tgoto __kctl_end;\n\t}\n\tif (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK) {\n\t\tif (vd->owner != NULL && vd->owner != file) {\n\t\t\terr = -EPERM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\terr = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);\n\t\tif (err > 0) {\n\t\t\tstruct snd_ctl_elem_id id = kctl->id;\n\t\t\tup_read(&card->controls_rwsem);\n\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &id);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tif (op_flag) {\n\t\t\terr = -ENXIO;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tlen = kctl->tlv.p[1] + 2 * sizeof(unsigned int);\n\t\tif (tlv.length < len) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tif (copy_to_user(_tlv->tlv, kctl->tlv.p, len))\n\t\t\terr = -EFAULT;\n\t}\n      __kctl_end:\n\tup_read(&card->controls_rwsem);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -37,8 +37,9 @@\n \t\t}\n \t\terr = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);\n \t\tif (err > 0) {\n+\t\t\tstruct snd_ctl_elem_id id = kctl->id;\n \t\t\tup_read(&card->controls_rwsem);\n-\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &kctl->id);\n+\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &id);\n \t\t\treturn 0;\n \t\t}\n \t} else {",
        "function_modified_lines": {
            "added": [
                "\t\t\tstruct snd_ctl_elem_id id = kctl->id;",
                "\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &id);"
            ],
            "deleted": [
                "\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &kctl->id);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 does not ensure possession of a read/write lock, which allows local users to cause a denial of service (use-after-free) and obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "id": 568
    },
    {
        "cve_id": "CVE-2014-4653",
        "code_before_change": "int snd_ctl_add(struct snd_card *card, struct snd_kcontrol *kcontrol)\n{\n\tstruct snd_ctl_elem_id id;\n\tunsigned int idx;\n\tint err = -EINVAL;\n\n\tif (! kcontrol)\n\t\treturn err;\n\tif (snd_BUG_ON(!card || !kcontrol->info))\n\t\tgoto error;\n\tid = kcontrol->id;\n\tdown_write(&card->controls_rwsem);\n\tif (snd_ctl_find_id(card, &id)) {\n\t\tup_write(&card->controls_rwsem);\n\t\tdev_err(card->dev, \"control %i:%i:%i:%s:%i is already present\\n\",\n\t\t\t\t\tid.iface,\n\t\t\t\t\tid.device,\n\t\t\t\t\tid.subdevice,\n\t\t\t\t\tid.name,\n\t\t\t\t\tid.index);\n\t\terr = -EBUSY;\n\t\tgoto error;\n\t}\n\tif (snd_ctl_find_hole(card, kcontrol->count) < 0) {\n\t\tup_write(&card->controls_rwsem);\n\t\terr = -ENOMEM;\n\t\tgoto error;\n\t}\n\tlist_add_tail(&kcontrol->list, &card->controls);\n\tcard->controls_count += kcontrol->count;\n\tkcontrol->id.numid = card->last_numid + 1;\n\tcard->last_numid += kcontrol->count;\n\tup_write(&card->controls_rwsem);\n\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)\n\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n\treturn 0;\n\n error:\n\tsnd_ctl_free_one(kcontrol);\n\treturn err;\n}",
        "code_after_change": "int snd_ctl_add(struct snd_card *card, struct snd_kcontrol *kcontrol)\n{\n\tstruct snd_ctl_elem_id id;\n\tunsigned int idx;\n\tunsigned int count;\n\tint err = -EINVAL;\n\n\tif (! kcontrol)\n\t\treturn err;\n\tif (snd_BUG_ON(!card || !kcontrol->info))\n\t\tgoto error;\n\tid = kcontrol->id;\n\tdown_write(&card->controls_rwsem);\n\tif (snd_ctl_find_id(card, &id)) {\n\t\tup_write(&card->controls_rwsem);\n\t\tdev_err(card->dev, \"control %i:%i:%i:%s:%i is already present\\n\",\n\t\t\t\t\tid.iface,\n\t\t\t\t\tid.device,\n\t\t\t\t\tid.subdevice,\n\t\t\t\t\tid.name,\n\t\t\t\t\tid.index);\n\t\terr = -EBUSY;\n\t\tgoto error;\n\t}\n\tif (snd_ctl_find_hole(card, kcontrol->count) < 0) {\n\t\tup_write(&card->controls_rwsem);\n\t\terr = -ENOMEM;\n\t\tgoto error;\n\t}\n\tlist_add_tail(&kcontrol->list, &card->controls);\n\tcard->controls_count += kcontrol->count;\n\tkcontrol->id.numid = card->last_numid + 1;\n\tcard->last_numid += kcontrol->count;\n\tcount = kcontrol->count;\n\tup_write(&card->controls_rwsem);\n\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)\n\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n\treturn 0;\n\n error:\n\tsnd_ctl_free_one(kcontrol);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct snd_ctl_elem_id id;\n \tunsigned int idx;\n+\tunsigned int count;\n \tint err = -EINVAL;\n \n \tif (! kcontrol)\n@@ -30,8 +31,9 @@\n \tcard->controls_count += kcontrol->count;\n \tkcontrol->id.numid = card->last_numid + 1;\n \tcard->last_numid += kcontrol->count;\n+\tcount = kcontrol->count;\n \tup_write(&card->controls_rwsem);\n-\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)\n+\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)\n \t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\tunsigned int count;",
                "\tcount = kcontrol->count;",
                "\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)"
            ],
            "deleted": [
                "\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 does not ensure possession of a read/write lock, which allows local users to cause a denial of service (use-after-free) and obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "id": 567
    },
    {
        "cve_id": "CVE-2020-29660",
        "code_before_change": "static int tiocspgrp(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tstruct pid *pgrp;\n\tpid_t pgrp_nr;\n\tint retval = tty_check_change(real_tty);\n\n\tif (retval == -EIO)\n\t\treturn -ENOTTY;\n\tif (retval)\n\t\treturn retval;\n\tif (!current->signal->tty ||\n\t    (current->signal->tty != real_tty) ||\n\t    (real_tty->session != task_session(current)))\n\t\treturn -ENOTTY;\n\tif (get_user(pgrp_nr, p))\n\t\treturn -EFAULT;\n\tif (pgrp_nr < 0)\n\t\treturn -EINVAL;\n\trcu_read_lock();\n\tpgrp = find_vpid(pgrp_nr);\n\tretval = -ESRCH;\n\tif (!pgrp)\n\t\tgoto out_unlock;\n\tretval = -EPERM;\n\tif (session_of_pgrp(pgrp) != task_session(current))\n\t\tgoto out_unlock;\n\tretval = 0;\n\tspin_lock_irq(&real_tty->ctrl_lock);\n\tput_pid(real_tty->pgrp);\n\treal_tty->pgrp = get_pid(pgrp);\n\tspin_unlock_irq(&real_tty->ctrl_lock);\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}",
        "code_after_change": "static int tiocspgrp(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tstruct pid *pgrp;\n\tpid_t pgrp_nr;\n\tint retval = tty_check_change(real_tty);\n\n\tif (retval == -EIO)\n\t\treturn -ENOTTY;\n\tif (retval)\n\t\treturn retval;\n\n\tif (get_user(pgrp_nr, p))\n\t\treturn -EFAULT;\n\tif (pgrp_nr < 0)\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&real_tty->ctrl_lock);\n\tif (!current->signal->tty ||\n\t    (current->signal->tty != real_tty) ||\n\t    (real_tty->session != task_session(current))) {\n\t\tretval = -ENOTTY;\n\t\tgoto out_unlock_ctrl;\n\t}\n\trcu_read_lock();\n\tpgrp = find_vpid(pgrp_nr);\n\tretval = -ESRCH;\n\tif (!pgrp)\n\t\tgoto out_unlock;\n\tretval = -EPERM;\n\tif (session_of_pgrp(pgrp) != task_session(current))\n\t\tgoto out_unlock;\n\tretval = 0;\n\tput_pid(real_tty->pgrp);\n\treal_tty->pgrp = get_pid(pgrp);\nout_unlock:\n\trcu_read_unlock();\nout_unlock_ctrl:\n\tspin_unlock_irq(&real_tty->ctrl_lock);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,14 +8,19 @@\n \t\treturn -ENOTTY;\n \tif (retval)\n \t\treturn retval;\n-\tif (!current->signal->tty ||\n-\t    (current->signal->tty != real_tty) ||\n-\t    (real_tty->session != task_session(current)))\n-\t\treturn -ENOTTY;\n+\n \tif (get_user(pgrp_nr, p))\n \t\treturn -EFAULT;\n \tif (pgrp_nr < 0)\n \t\treturn -EINVAL;\n+\n+\tspin_lock_irq(&real_tty->ctrl_lock);\n+\tif (!current->signal->tty ||\n+\t    (current->signal->tty != real_tty) ||\n+\t    (real_tty->session != task_session(current))) {\n+\t\tretval = -ENOTTY;\n+\t\tgoto out_unlock_ctrl;\n+\t}\n \trcu_read_lock();\n \tpgrp = find_vpid(pgrp_nr);\n \tretval = -ESRCH;\n@@ -25,11 +30,11 @@\n \tif (session_of_pgrp(pgrp) != task_session(current))\n \t\tgoto out_unlock;\n \tretval = 0;\n-\tspin_lock_irq(&real_tty->ctrl_lock);\n \tput_pid(real_tty->pgrp);\n \treal_tty->pgrp = get_pid(pgrp);\n-\tspin_unlock_irq(&real_tty->ctrl_lock);\n out_unlock:\n \trcu_read_unlock();\n+out_unlock_ctrl:\n+\tspin_unlock_irq(&real_tty->ctrl_lock);\n \treturn retval;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "",
                "\tspin_lock_irq(&real_tty->ctrl_lock);",
                "\tif (!current->signal->tty ||",
                "\t    (current->signal->tty != real_tty) ||",
                "\t    (real_tty->session != task_session(current))) {",
                "\t\tretval = -ENOTTY;",
                "\t\tgoto out_unlock_ctrl;",
                "\t}",
                "out_unlock_ctrl:",
                "\tspin_unlock_irq(&real_tty->ctrl_lock);"
            ],
            "deleted": [
                "\tif (!current->signal->tty ||",
                "\t    (current->signal->tty != real_tty) ||",
                "\t    (real_tty->session != task_session(current)))",
                "\t\treturn -ENOTTY;",
                "\tspin_lock_irq(&real_tty->ctrl_lock);",
                "\tspin_unlock_irq(&real_tty->ctrl_lock);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "id": 2704
    },
    {
        "cve_id": "CVE-2023-5197",
        "code_before_change": "static void __nft_release_table(struct net *net, struct nft_table *table)\n{\n\tstruct nft_flowtable *flowtable, *nf;\n\tstruct nft_chain *chain, *nc;\n\tstruct nft_object *obj, *ne;\n\tstruct nft_rule *rule, *nr;\n\tstruct nft_set *set, *ns;\n\tstruct nft_ctx ctx = {\n\t\t.net\t= net,\n\t\t.family\t= NFPROTO_NETDEV,\n\t};\n\n\tctx.family = table->family;\n\tctx.table = table;\n\tlist_for_each_entry(chain, &table->chains, list) {\n\t\tif (nft_chain_is_bound(chain))\n\t\t\tcontinue;\n\n\t\tctx.chain = chain;\n\t\tlist_for_each_entry_safe(rule, nr, &chain->rules, list) {\n\t\t\tlist_del(&rule->list);\n\t\t\tnft_use_dec(&chain->use);\n\t\t\tnf_tables_rule_release(&ctx, rule);\n\t\t}\n\t}\n\tlist_for_each_entry_safe(flowtable, nf, &table->flowtables, list) {\n\t\tlist_del(&flowtable->list);\n\t\tnft_use_dec(&table->use);\n\t\tnf_tables_flowtable_destroy(flowtable);\n\t}\n\tlist_for_each_entry_safe(set, ns, &table->sets, list) {\n\t\tlist_del(&set->list);\n\t\tnft_use_dec(&table->use);\n\t\tif (set->flags & (NFT_SET_MAP | NFT_SET_OBJECT))\n\t\t\tnft_map_deactivate(&ctx, set);\n\n\t\tnft_set_destroy(&ctx, set);\n\t}\n\tlist_for_each_entry_safe(obj, ne, &table->objects, list) {\n\t\tnft_obj_del(obj);\n\t\tnft_use_dec(&table->use);\n\t\tnft_obj_destroy(&ctx, obj);\n\t}\n\tlist_for_each_entry_safe(chain, nc, &table->chains, list) {\n\t\tctx.chain = chain;\n\t\tnft_chain_del(chain);\n\t\tnft_use_dec(&table->use);\n\t\tnf_tables_chain_destroy(&ctx);\n\t}\n\tnf_tables_table_destroy(&ctx);\n}",
        "code_after_change": "static void __nft_release_table(struct net *net, struct nft_table *table)\n{\n\tstruct nft_flowtable *flowtable, *nf;\n\tstruct nft_chain *chain, *nc;\n\tstruct nft_object *obj, *ne;\n\tstruct nft_rule *rule, *nr;\n\tstruct nft_set *set, *ns;\n\tstruct nft_ctx ctx = {\n\t\t.net\t= net,\n\t\t.family\t= NFPROTO_NETDEV,\n\t};\n\n\tctx.family = table->family;\n\tctx.table = table;\n\tlist_for_each_entry(chain, &table->chains, list) {\n\t\tif (nft_chain_binding(chain))\n\t\t\tcontinue;\n\n\t\tctx.chain = chain;\n\t\tlist_for_each_entry_safe(rule, nr, &chain->rules, list) {\n\t\t\tlist_del(&rule->list);\n\t\t\tnft_use_dec(&chain->use);\n\t\t\tnf_tables_rule_release(&ctx, rule);\n\t\t}\n\t}\n\tlist_for_each_entry_safe(flowtable, nf, &table->flowtables, list) {\n\t\tlist_del(&flowtable->list);\n\t\tnft_use_dec(&table->use);\n\t\tnf_tables_flowtable_destroy(flowtable);\n\t}\n\tlist_for_each_entry_safe(set, ns, &table->sets, list) {\n\t\tlist_del(&set->list);\n\t\tnft_use_dec(&table->use);\n\t\tif (set->flags & (NFT_SET_MAP | NFT_SET_OBJECT))\n\t\t\tnft_map_deactivate(&ctx, set);\n\n\t\tnft_set_destroy(&ctx, set);\n\t}\n\tlist_for_each_entry_safe(obj, ne, &table->objects, list) {\n\t\tnft_obj_del(obj);\n\t\tnft_use_dec(&table->use);\n\t\tnft_obj_destroy(&ctx, obj);\n\t}\n\tlist_for_each_entry_safe(chain, nc, &table->chains, list) {\n\t\tctx.chain = chain;\n\t\tnft_chain_del(chain);\n\t\tnft_use_dec(&table->use);\n\t\tnf_tables_chain_destroy(&ctx);\n\t}\n\tnf_tables_table_destroy(&ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,7 @@\n \tctx.family = table->family;\n \tctx.table = table;\n \tlist_for_each_entry(chain, &table->chains, list) {\n-\t\tif (nft_chain_is_bound(chain))\n+\t\tif (nft_chain_binding(chain))\n \t\t\tcontinue;\n \n \t\tctx.chain = chain;",
        "function_modified_lines": {
            "added": [
                "\t\tif (nft_chain_binding(chain))"
            ],
            "deleted": [
                "\t\tif (nft_chain_is_bound(chain))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nAddition and removal of rules from chain bindings within the same transaction causes leads to use-after-free.\n\nWe recommend upgrading past commit f15f29fd4779be8a418b66e9d52979bb6d6c2325.\n\n",
        "id": 4264
    },
    {
        "cve_id": "CVE-2023-5197",
        "code_before_change": "static int nf_tables_delchain(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t      const struct nlattr * const nla[])\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu8 genmask = nft_genmask_next(info->net);\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct net *net = info->net;\n\tconst struct nlattr *attr;\n\tstruct nft_table *table;\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\tstruct nft_ctx ctx;\n\tu64 handle;\n\tu32 use;\n\tint err;\n\n\ttable = nft_table_lookup(net, nla[NFTA_CHAIN_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_CHAIN_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_CHAIN_HANDLE]) {\n\t\tattr = nla[NFTA_CHAIN_HANDLE];\n\t\thandle = be64_to_cpu(nla_get_be64(attr));\n\t\tchain = nft_chain_lookup_byhandle(table, handle, genmask);\n\t} else {\n\t\tattr = nla[NFTA_CHAIN_NAME];\n\t\tchain = nft_chain_lookup(net, table, attr, genmask);\n\t}\n\tif (IS_ERR(chain)) {\n\t\tif (PTR_ERR(chain) == -ENOENT &&\n\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYCHAIN)\n\t\t\treturn 0;\n\n\t\tNL_SET_BAD_ATTR(extack, attr);\n\t\treturn PTR_ERR(chain);\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tif (nla[NFTA_CHAIN_HOOK]) {\n\t\tif (chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (nft_is_base_chain(chain)) {\n\t\t\tstruct nft_base_chain *basechain = nft_base_chain(chain);\n\n\t\t\tif (nft_base_chain_netdev(table->family, basechain->ops.hooknum))\n\t\t\t\treturn nft_delchain_hook(&ctx, basechain, extack);\n\t\t}\n\t}\n\n\tif (info->nlh->nlmsg_flags & NLM_F_NONREC &&\n\t    chain->use > 0)\n\t\treturn -EBUSY;\n\n\tuse = chain->use;\n\tlist_for_each_entry(rule, &chain->rules, list) {\n\t\tif (!nft_is_active_next(net, rule))\n\t\t\tcontinue;\n\t\tuse--;\n\n\t\terr = nft_delrule(&ctx, rule);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\t/* There are rules and elements that are still holding references to us,\n\t * we cannot do a recursive removal in this case.\n\t */\n\tif (use > 0) {\n\t\tNL_SET_BAD_ATTR(extack, attr);\n\t\treturn -EBUSY;\n\t}\n\n\treturn nft_delchain(&ctx);\n}",
        "code_after_change": "static int nf_tables_delchain(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t      const struct nlattr * const nla[])\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu8 genmask = nft_genmask_next(info->net);\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct net *net = info->net;\n\tconst struct nlattr *attr;\n\tstruct nft_table *table;\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\tstruct nft_ctx ctx;\n\tu64 handle;\n\tu32 use;\n\tint err;\n\n\ttable = nft_table_lookup(net, nla[NFTA_CHAIN_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_CHAIN_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_CHAIN_HANDLE]) {\n\t\tattr = nla[NFTA_CHAIN_HANDLE];\n\t\thandle = be64_to_cpu(nla_get_be64(attr));\n\t\tchain = nft_chain_lookup_byhandle(table, handle, genmask);\n\t} else {\n\t\tattr = nla[NFTA_CHAIN_NAME];\n\t\tchain = nft_chain_lookup(net, table, attr, genmask);\n\t}\n\tif (IS_ERR(chain)) {\n\t\tif (PTR_ERR(chain) == -ENOENT &&\n\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYCHAIN)\n\t\t\treturn 0;\n\n\t\tNL_SET_BAD_ATTR(extack, attr);\n\t\treturn PTR_ERR(chain);\n\t}\n\n\tif (nft_chain_binding(chain))\n\t\treturn -EOPNOTSUPP;\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tif (nla[NFTA_CHAIN_HOOK]) {\n\t\tif (chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (nft_is_base_chain(chain)) {\n\t\t\tstruct nft_base_chain *basechain = nft_base_chain(chain);\n\n\t\t\tif (nft_base_chain_netdev(table->family, basechain->ops.hooknum))\n\t\t\t\treturn nft_delchain_hook(&ctx, basechain, extack);\n\t\t}\n\t}\n\n\tif (info->nlh->nlmsg_flags & NLM_F_NONREC &&\n\t    chain->use > 0)\n\t\treturn -EBUSY;\n\n\tuse = chain->use;\n\tlist_for_each_entry(rule, &chain->rules, list) {\n\t\tif (!nft_is_active_next(net, rule))\n\t\t\tcontinue;\n\t\tuse--;\n\n\t\terr = nft_delrule(&ctx, rule);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\t/* There are rules and elements that are still holding references to us,\n\t * we cannot do a recursive removal in this case.\n\t */\n\tif (use > 0) {\n\t\tNL_SET_BAD_ATTR(extack, attr);\n\t\treturn -EBUSY;\n\t}\n\n\treturn nft_delchain(&ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,6 +38,9 @@\n \t\treturn PTR_ERR(chain);\n \t}\n \n+\tif (nft_chain_binding(chain))\n+\t\treturn -EOPNOTSUPP;\n+\n \tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n \n \tif (nla[NFTA_CHAIN_HOOK]) {",
        "function_modified_lines": {
            "added": [
                "\tif (nft_chain_binding(chain))",
                "\t\treturn -EOPNOTSUPP;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nAddition and removal of rules from chain bindings within the same transaction causes leads to use-after-free.\n\nWe recommend upgrading past commit f15f29fd4779be8a418b66e9d52979bb6d6c2325.\n\n",
        "id": 4265
    },
    {
        "cve_id": "CVE-2020-10690",
        "code_before_change": "static int posix_clock_open(struct inode *inode, struct file *fp)\n{\n\tint err;\n\tstruct posix_clock *clk =\n\t\tcontainer_of(inode->i_cdev, struct posix_clock, cdev);\n\n\tdown_read(&clk->rwsem);\n\n\tif (clk->zombie) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (clk->ops.open)\n\t\terr = clk->ops.open(clk, fp->f_mode);\n\telse\n\t\terr = 0;\n\n\tif (!err) {\n\t\tkref_get(&clk->kref);\n\t\tfp->private_data = clk;\n\t}\nout:\n\tup_read(&clk->rwsem);\n\treturn err;\n}",
        "code_after_change": "static int posix_clock_open(struct inode *inode, struct file *fp)\n{\n\tint err;\n\tstruct posix_clock *clk =\n\t\tcontainer_of(inode->i_cdev, struct posix_clock, cdev);\n\n\tdown_read(&clk->rwsem);\n\n\tif (clk->zombie) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (clk->ops.open)\n\t\terr = clk->ops.open(clk, fp->f_mode);\n\telse\n\t\terr = 0;\n\n\tif (!err) {\n\t\tget_device(clk->dev);\n\t\tfp->private_data = clk;\n\t}\nout:\n\tup_read(&clk->rwsem);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,7 @@\n \t\terr = 0;\n \n \tif (!err) {\n-\t\tkref_get(&clk->kref);\n+\t\tget_device(clk->dev);\n \t\tfp->private_data = clk;\n \t}\n out:",
        "function_modified_lines": {
            "added": [
                "\t\tget_device(clk->dev);"
            ],
            "deleted": [
                "\t\tkref_get(&clk->kref);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free in kernel versions before 5.5 due to a race condition between the release of ptp_clock and cdev while resource deallocation. When a (high privileged) process allocates a ptp device file (like /dev/ptpX) and voluntarily goes to sleep. During this time if the underlying device is removed, it can cause an exploitable condition as the process wakes up to terminate and clean all attached files. The system crashes due to the cdev structure being invalid (as already freed) which is pointed to by the inode.",
        "id": 2402
    },
    {
        "cve_id": "CVE-2019-2025",
        "code_before_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t *offp, *off_end, *off_start;\n\tbinder_size_t off_min;\n\tu8 *sg_bufp, *sg_buf_end;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tstruct binder_buffer_object *last_fixup_obj = NULL;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\te->context_name = proc->context->name;\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc == proc) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tt->buffer->allow_user_free = 0;\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\toff_start = (binder_size_t *)(t->buffer->data +\n\t\t\t\t      ALIGN(tr->data_size, sizeof(void *)));\n\toffp = off_start;\n\n\tif (copy_from_user(t->buffer->data, (const void __user *)(uintptr_t)\n\t\t\t   tr->data.ptr.buffer, tr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (copy_from_user(offp, (const void __user *)(uintptr_t)\n\t\t\t   tr->data.ptr.offsets, tr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_end = (void *)off_start + tr->offsets_size;\n\tsg_bufp = (u8 *)(PTR_ALIGN(off_end, sizeof(void *)));\n\tsg_buf_end = sg_bufp + extra_buffers_size;\n\toff_min = 0;\n\tfor (; offp < off_end; offp++) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = binder_validate_object(t->buffer, *offp);\n\n\t\tif (object_size == 0 || *offp < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid, (u64)*offp,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = (struct binder_object_header *)(t->buffer->data + *offp);\n\t\toff_min = *offp + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tint ret = binder_translate_fd(&fp->fd, t, thread,\n\t\t\t\t\t\t      in_reply_to);\n\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tfp->pad_binder = 0;\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(t->buffer, fda->parent,\n\t\t\t\t\t\t    off_start,\n\t\t\t\t\t\t    offp - off_start);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(t->buffer, off_start,\n\t\t\t\t\t\t   parent, fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj = parent;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end - sg_bufp;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (copy_from_user(sg_bufp,\n\t\t\t\t\t   (const void __user *)(uintptr_t)\n\t\t\t\t\t   bp->buffer, bp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)sg_bufp +\n\t\t\t\tbinder_alloc_get_user_buffer_offset(\n\t\t\t\t\t\t&target_proc->alloc);\n\t\t\tsg_bufp += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tret = binder_fixup_parent(t, thread, bp, off_start,\n\t\t\t\t\t\t  offp - off_start,\n\t\t\t\t\t\t  last_fixup_obj,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj = bp;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer, offp);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "code_after_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t *offp, *off_end, *off_start;\n\tbinder_size_t off_min;\n\tu8 *sg_bufp, *sg_buf_end;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tstruct binder_buffer_object *last_fixup_obj = NULL;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\te->context_name = proc->context->name;\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc == proc) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\toff_start = (binder_size_t *)(t->buffer->data +\n\t\t\t\t      ALIGN(tr->data_size, sizeof(void *)));\n\toffp = off_start;\n\n\tif (copy_from_user(t->buffer->data, (const void __user *)(uintptr_t)\n\t\t\t   tr->data.ptr.buffer, tr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (copy_from_user(offp, (const void __user *)(uintptr_t)\n\t\t\t   tr->data.ptr.offsets, tr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_end = (void *)off_start + tr->offsets_size;\n\tsg_bufp = (u8 *)(PTR_ALIGN(off_end, sizeof(void *)));\n\tsg_buf_end = sg_bufp + extra_buffers_size;\n\toff_min = 0;\n\tfor (; offp < off_end; offp++) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = binder_validate_object(t->buffer, *offp);\n\n\t\tif (object_size == 0 || *offp < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid, (u64)*offp,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = (struct binder_object_header *)(t->buffer->data + *offp);\n\t\toff_min = *offp + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tint ret = binder_translate_fd(&fp->fd, t, thread,\n\t\t\t\t\t\t      in_reply_to);\n\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tfp->pad_binder = 0;\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(t->buffer, fda->parent,\n\t\t\t\t\t\t    off_start,\n\t\t\t\t\t\t    offp - off_start);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(t->buffer, off_start,\n\t\t\t\t\t\t   parent, fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj = parent;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end - sg_bufp;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (copy_from_user(sg_bufp,\n\t\t\t\t\t   (const void __user *)(uintptr_t)\n\t\t\t\t\t   bp->buffer, bp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)sg_bufp +\n\t\t\t\tbinder_alloc_get_user_buffer_offset(\n\t\t\t\t\t\t&target_proc->alloc);\n\t\t\tsg_bufp += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tret = binder_fixup_parent(t, thread, bp, off_start,\n\t\t\t\t\t\t  offp - off_start,\n\t\t\t\t\t\t  last_fixup_obj,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj = bp;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer, offp);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -278,7 +278,6 @@\n \t\tt->buffer = NULL;\n \t\tgoto err_binder_alloc_buf_failed;\n \t}\n-\tt->buffer->allow_user_free = 0;\n \tt->buffer->debug_id = t->debug_id;\n \tt->buffer->transaction = t;\n \tt->buffer->target_node = target_node;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tt->buffer->allow_user_free = 0;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_thread_read of binder.c, there is a possible use-after-free due to improper locking. This could lead to local escalation of privilege in the kernel with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-116855682References: Upstream kernel",
        "id": 2275
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "code_after_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,11 +43,15 @@\n \t{\n \t\tstruct ion_handle *handle;\n \n-\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n-\t\tif (IS_ERR(handle))\n+\t\tmutex_lock(&client->lock);\n+\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n+\t\tif (IS_ERR(handle)) {\n+\t\t\tmutex_unlock(&client->lock);\n \t\t\treturn PTR_ERR(handle);\n-\t\tion_free(client, handle);\n-\t\tion_handle_put(handle);\n+\t\t}\n+\t\tion_free_nolock(client, handle);\n+\t\tion_handle_put_nolock(handle);\n+\t\tmutex_unlock(&client->lock);\n \t\tbreak;\n \t}\n \tcase ION_IOC_SHARE:",
        "function_modified_lines": {
            "added": [
                "\t\tmutex_lock(&client->lock);",
                "\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);",
                "\t\tif (IS_ERR(handle)) {",
                "\t\t\tmutex_unlock(&client->lock);",
                "\t\t}",
                "\t\tion_free_nolock(client, handle);",
                "\t\tion_handle_put_nolock(handle);",
                "\t\tmutex_unlock(&client->lock);"
            ],
            "deleted": [
                "\t\thandle = ion_handle_get_by_id(client, data.handle.handle);",
                "\t\tif (IS_ERR(handle))",
                "\t\tion_free(client, handle);",
                "\t\tion_handle_put(handle);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1140
    },
    {
        "cve_id": "CVE-2023-35823",
        "code_before_change": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "code_after_change": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\tdel_timer_sync(&dev->video_q.timeout);\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n void saa7134_video_fini(struct saa7134_dev *dev)\n {\n+\tdel_timer_sync(&dev->video_q.timeout);\n \t/* free stuff */\n \tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n \tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->video_q.timeout);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c.",
        "id": 4111
    },
    {
        "cve_id": "CVE-2022-3176",
        "code_before_change": "static void io_poll_remove_entries(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = io_poll_get_single(req);\n\tstruct io_poll_iocb *poll_double = io_poll_get_double(req);\n\n\tif (poll->head)\n\t\tio_poll_remove_entry(poll);\n\tif (poll_double && poll_double->head)\n\t\tio_poll_remove_entry(poll_double);\n}",
        "code_after_change": "static void io_poll_remove_entries(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = io_poll_get_single(req);\n\tstruct io_poll_iocb *poll_double = io_poll_get_double(req);\n\n\t/*\n\t * While we hold the waitqueue lock and the waitqueue is nonempty,\n\t * wake_up_pollfree() will wait for us.  However, taking the waitqueue\n\t * lock in the first place can race with the waitqueue being freed.\n\t *\n\t * We solve this as eventpoll does: by taking advantage of the fact that\n\t * all users of wake_up_pollfree() will RCU-delay the actual free.  If\n\t * we enter rcu_read_lock() and see that the pointer to the queue is\n\t * non-NULL, we can then lock it without the memory being freed out from\n\t * under us.\n\t *\n\t * Keep holding rcu_read_lock() as long as we hold the queue lock, in\n\t * case the caller deletes the entry from the queue, leaving it empty.\n\t * In that case, only RCU prevents the queue memory from being freed.\n\t */\n\trcu_read_lock();\n\tio_poll_remove_entry(poll);\n\tif (poll_double)\n\t\tio_poll_remove_entry(poll_double);\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,8 +3,24 @@\n \tstruct io_poll_iocb *poll = io_poll_get_single(req);\n \tstruct io_poll_iocb *poll_double = io_poll_get_double(req);\n \n-\tif (poll->head)\n-\t\tio_poll_remove_entry(poll);\n-\tif (poll_double && poll_double->head)\n+\t/*\n+\t * While we hold the waitqueue lock and the waitqueue is nonempty,\n+\t * wake_up_pollfree() will wait for us.  However, taking the waitqueue\n+\t * lock in the first place can race with the waitqueue being freed.\n+\t *\n+\t * We solve this as eventpoll does: by taking advantage of the fact that\n+\t * all users of wake_up_pollfree() will RCU-delay the actual free.  If\n+\t * we enter rcu_read_lock() and see that the pointer to the queue is\n+\t * non-NULL, we can then lock it without the memory being freed out from\n+\t * under us.\n+\t *\n+\t * Keep holding rcu_read_lock() as long as we hold the queue lock, in\n+\t * case the caller deletes the entry from the queue, leaving it empty.\n+\t * In that case, only RCU prevents the queue memory from being freed.\n+\t */\n+\trcu_read_lock();\n+\tio_poll_remove_entry(poll);\n+\tif (poll_double)\n \t\tio_poll_remove_entry(poll_double);\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * While we hold the waitqueue lock and the waitqueue is nonempty,",
                "\t * wake_up_pollfree() will wait for us.  However, taking the waitqueue",
                "\t * lock in the first place can race with the waitqueue being freed.",
                "\t *",
                "\t * We solve this as eventpoll does: by taking advantage of the fact that",
                "\t * all users of wake_up_pollfree() will RCU-delay the actual free.  If",
                "\t * we enter rcu_read_lock() and see that the pointer to the queue is",
                "\t * non-NULL, we can then lock it without the memory being freed out from",
                "\t * under us.",
                "\t *",
                "\t * Keep holding rcu_read_lock() as long as we hold the queue lock, in",
                "\t * case the caller deletes the entry from the queue, leaving it empty.",
                "\t * In that case, only RCU prevents the queue memory from being freed.",
                "\t */",
                "\trcu_read_lock();",
                "\tio_poll_remove_entry(poll);",
                "\tif (poll_double)",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tif (poll->head)",
                "\t\tio_poll_remove_entry(poll);",
                "\tif (poll_double && poll_double->head)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There exists a use-after-free in io_uring in the Linux kernel. Signalfd_poll() and binder_poll() use a waitqueue whose lifetime is the current task. It will send a POLLFREE notification to all waiters before the queue is freed. Unfortunately, the io_uring poll doesn't handle POLLFREE. This allows a use-after-free to occur if a signalfd or binder fd is polled with io_uring poll, and the waitqueue gets freed. We recommend upgrading past commit fc78b2fc21f10c4c9c4d5d659a685710ffa63659",
        "id": 3564
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "code_after_change": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,6 @@\n \tint ret;\n \tint chk_addr_ret;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr_len < sizeof(struct sockaddr_l2tpip))\n \t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET)\n@@ -22,6 +20,9 @@\n \tread_unlock_bh(&l2tp_ip_lock);\n \n \tlock_sock(sk);\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out;\n+\n \tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n \t\tgoto out;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "id": 898
    },
    {
        "cve_id": "CVE-2022-20158",
        "code_before_change": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev_has_header(dev)) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (do_vnet &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\t/* Always timestamp; prefer an existing software timestamp taken\n\t * closer to the time of capture.\n\t */\n\tts_status = tpacket_get_timestamp(skb, &ts,\n\t\t\t\t\t  po->tp_tstamp | SOF_TIMESTAMPING_SOFTWARE);\n\tif (!ts_status)\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}",
        "code_after_change": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev_has_header(dev)) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb) {\n\t\t\t\t\tmemset(&PACKET_SKB_CB(copy_skb)->sa.ll, 0,\n\t\t\t\t\t       sizeof(PACKET_SKB_CB(copy_skb)->sa.ll));\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (do_vnet &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\t/* Always timestamp; prefer an existing software timestamp taken\n\t * closer to the time of capture.\n\t */\n\tts_status = tpacket_get_timestamp(skb, &ts,\n\t\t\t\t\t  po->tp_tstamp | SOF_TIMESTAMPING_SOFTWARE);\n\tif (!ts_status)\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}",
        "patch": "--- code before\n+++ code after\n@@ -93,8 +93,11 @@\n \t\t\t\t\tcopy_skb = skb_get(skb);\n \t\t\t\t\tskb_head = skb->data;\n \t\t\t\t}\n-\t\t\t\tif (copy_skb)\n+\t\t\t\tif (copy_skb) {\n+\t\t\t\t\tmemset(&PACKET_SKB_CB(copy_skb)->sa.ll, 0,\n+\t\t\t\t\t       sizeof(PACKET_SKB_CB(copy_skb)->sa.ll));\n \t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n+\t\t\t\t}\n \t\t\t}\n \t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n \t\t\tif ((int)snaplen < 0) {",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tif (copy_skb) {",
                "\t\t\t\t\tmemset(&PACKET_SKB_CB(copy_skb)->sa.ll, 0,",
                "\t\t\t\t\t       sizeof(PACKET_SKB_CB(copy_skb)->sa.ll));",
                "\t\t\t\t}"
            ],
            "deleted": [
                "\t\t\t\tif (copy_skb)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In bdi_put and bdi_unregister of backing-dev.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-182815710References: Upstream kernel",
        "id": 3344
    },
    {
        "cve_id": "CVE-2018-20976",
        "code_before_change": "STATIC void\nxfs_fs_put_super(\n\tstruct super_block\t*sb)\n{\n\tstruct xfs_mount\t*mp = XFS_M(sb);\n\n\txfs_notice(mp, \"Unmounting Filesystem\");\n\txfs_filestream_unmount(mp);\n\txfs_unmountfs(mp);\n\n\txfs_freesb(mp);\n\tfree_percpu(mp->m_stats.xs_stats);\n\txfs_destroy_percpu_counters(mp);\n\txfs_destroy_mount_workqueues(mp);\n\txfs_close_devices(mp);\n\txfs_free_fsname(mp);\n\tkfree(mp);\n}",
        "code_after_change": "STATIC void\nxfs_fs_put_super(\n\tstruct super_block\t*sb)\n{\n\tstruct xfs_mount\t*mp = XFS_M(sb);\n\n\t/* if ->fill_super failed, we have no mount to tear down */\n\tif (!sb->s_fs_info)\n\t\treturn;\n\n\txfs_notice(mp, \"Unmounting Filesystem\");\n\txfs_filestream_unmount(mp);\n\txfs_unmountfs(mp);\n\n\txfs_freesb(mp);\n\tfree_percpu(mp->m_stats.xs_stats);\n\txfs_destroy_percpu_counters(mp);\n\txfs_destroy_mount_workqueues(mp);\n\txfs_close_devices(mp);\n\n\tsb->s_fs_info = NULL;\n\txfs_free_fsname(mp);\n\tkfree(mp);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,10 @@\n \tstruct super_block\t*sb)\n {\n \tstruct xfs_mount\t*mp = XFS_M(sb);\n+\n+\t/* if ->fill_super failed, we have no mount to tear down */\n+\tif (!sb->s_fs_info)\n+\t\treturn;\n \n \txfs_notice(mp, \"Unmounting Filesystem\");\n \txfs_filestream_unmount(mp);\n@@ -13,6 +17,8 @@\n \txfs_destroy_percpu_counters(mp);\n \txfs_destroy_mount_workqueues(mp);\n \txfs_close_devices(mp);\n+\n+\tsb->s_fs_info = NULL;\n \txfs_free_fsname(mp);\n \tkfree(mp);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* if ->fill_super failed, we have no mount to tear down */",
                "\tif (!sb->s_fs_info)",
                "\t\treturn;",
                "",
                "\tsb->s_fs_info = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in fs/xfs/xfs_super.c in the Linux kernel before 4.18. A use after free exists, related to xfs_fs_fill_super failure.",
        "id": 1790
    },
    {
        "cve_id": "CVE-2021-29657",
        "code_before_change": "int nested_svm_vmrun(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *hsave = svm->nested.hsave;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (is_smm(&svm->vcpu)) {\n\t\tkvm_queue_exception(&svm->vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(&svm->vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(&svm->vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(&svm->vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tif (!nested_vmcb_checks(svm, vmcb12)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(&svm->vcpu);\n\tkvm_clear_interrupt_queue(&svm->vcpu);\n\n\t/*\n\t * Save the old vmcb, so we don't need to pick what we save, but can\n\t * restore everything when a VMEXIT occurs\n\t */\n\thsave->save.es     = vmcb->save.es;\n\thsave->save.cs     = vmcb->save.cs;\n\thsave->save.ss     = vmcb->save.ss;\n\thsave->save.ds     = vmcb->save.ds;\n\thsave->save.gdtr   = vmcb->save.gdtr;\n\thsave->save.idtr   = vmcb->save.idtr;\n\thsave->save.efer   = svm->vcpu.arch.efer;\n\thsave->save.cr0    = kvm_read_cr0(&svm->vcpu);\n\thsave->save.cr4    = svm->vcpu.arch.cr4;\n\thsave->save.rflags = kvm_get_rflags(&svm->vcpu);\n\thsave->save.rip    = kvm_rip_read(&svm->vcpu);\n\thsave->save.rsp    = vmcb->save.rsp;\n\thsave->save.rax    = vmcb->save.rax;\n\tif (npt_enabled)\n\t\thsave->save.cr3    = vmcb->save.cr3;\n\telse\n\t\thsave->save.cr3    = kvm_read_cr3(&svm->vcpu);\n\n\tcopy_vmcb_control_area(&hsave->control, &vmcb->control);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(svm, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(&svm->vcpu, &map, true);\n\n\treturn ret;\n}",
        "code_after_change": "int nested_svm_vmrun(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *hsave = svm->nested.hsave;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (is_smm(&svm->vcpu)) {\n\t\tkvm_queue_exception(&svm->vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(&svm->vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(&svm->vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(&svm->vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\n\tif (!nested_vmcb_check_save(svm, vmcb12) ||\n\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(&svm->vcpu);\n\tkvm_clear_interrupt_queue(&svm->vcpu);\n\n\t/*\n\t * Save the old vmcb, so we don't need to pick what we save, but can\n\t * restore everything when a VMEXIT occurs\n\t */\n\thsave->save.es     = vmcb->save.es;\n\thsave->save.cs     = vmcb->save.cs;\n\thsave->save.ss     = vmcb->save.ss;\n\thsave->save.ds     = vmcb->save.ds;\n\thsave->save.gdtr   = vmcb->save.gdtr;\n\thsave->save.idtr   = vmcb->save.idtr;\n\thsave->save.efer   = svm->vcpu.arch.efer;\n\thsave->save.cr0    = kvm_read_cr0(&svm->vcpu);\n\thsave->save.cr4    = svm->vcpu.arch.cr4;\n\thsave->save.rflags = kvm_get_rflags(&svm->vcpu);\n\thsave->save.rip    = kvm_rip_read(&svm->vcpu);\n\thsave->save.rsp    = vmcb->save.rsp;\n\thsave->save.rax    = vmcb->save.rax;\n\tif (npt_enabled)\n\t\thsave->save.cr3    = vmcb->save.cr3;\n\telse\n\t\thsave->save.cr3    = kvm_read_cr3(&svm->vcpu);\n\n\tcopy_vmcb_control_area(&hsave->control, &vmcb->control);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(svm, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(&svm->vcpu, &map, true);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,10 @@\n \tif (WARN_ON_ONCE(!svm->nested.initialized))\n \t\treturn -EINVAL;\n \n-\tif (!nested_vmcb_checks(svm, vmcb12)) {\n+\tload_nested_vmcb_control(svm, &vmcb12->control);\n+\n+\tif (!nested_vmcb_check_save(svm, vmcb12) ||\n+\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {\n \t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n \t\tvmcb12->control.exit_code_hi = 0;\n \t\tvmcb12->control.exit_info_1  = 0;",
        "function_modified_lines": {
            "added": [
                "\tload_nested_vmcb_control(svm, &vmcb12->control);",
                "",
                "\tif (!nested_vmcb_check_save(svm, vmcb12) ||",
                "\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {"
            ],
            "deleted": [
                "\tif (!nested_vmcb_checks(svm, vmcb12)) {"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-367"
        ],
        "cve_description": "arch/x86/kvm/svm/nested.c in the Linux kernel before 5.11.12 has a use-after-free in which an AMD KVM guest can bypass access control on host OS MSRs when there are nested guests, aka CID-a58d9166a756. This occurs because of a TOCTOU race condition associated with a VMCB12 double fetch in nested_svm_vmrun.",
        "id": 2957
    },
    {
        "cve_id": "CVE-2022-1419",
        "code_before_change": "static int vgem_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\n\t\t\t\tstruct drm_mode_create_dumb *args)\n{\n\tstruct drm_gem_object *gem_object;\n\tu64 pitch, size;\n\n\tpitch = args->width * DIV_ROUND_UP(args->bpp, 8);\n\tsize = args->height * pitch;\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\tgem_object = vgem_gem_create(dev, file, &args->handle, size);\n\tif (IS_ERR(gem_object))\n\t\treturn PTR_ERR(gem_object);\n\n\targs->size = gem_object->size;\n\targs->pitch = pitch;\n\n\tDRM_DEBUG(\"Created object of size %lld\\n\", size);\n\n\treturn 0;\n}",
        "code_after_change": "static int vgem_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\n\t\t\t\tstruct drm_mode_create_dumb *args)\n{\n\tstruct drm_gem_object *gem_object;\n\tu64 pitch, size;\n\n\tpitch = args->width * DIV_ROUND_UP(args->bpp, 8);\n\tsize = args->height * pitch;\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\tgem_object = vgem_gem_create(dev, file, &args->handle, size);\n\tif (IS_ERR(gem_object))\n\t\treturn PTR_ERR(gem_object);\n\n\targs->size = gem_object->size;\n\targs->pitch = pitch;\n\n\tdrm_gem_object_put_unlocked(gem_object);\n\n\tDRM_DEBUG(\"Created object of size %llu\\n\", args->size);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,9 @@\n \targs->size = gem_object->size;\n \targs->pitch = pitch;\n \n-\tDRM_DEBUG(\"Created object of size %lld\\n\", size);\n+\tdrm_gem_object_put_unlocked(gem_object);\n+\n+\tDRM_DEBUG(\"Created object of size %llu\\n\", args->size);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tdrm_gem_object_put_unlocked(gem_object);",
                "",
                "\tDRM_DEBUG(\"Created object of size %llu\\n\", args->size);"
            ],
            "deleted": [
                "\tDRM_DEBUG(\"Created object of size %lld\\n\", size);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The root cause of this vulnerability is that the ioctl$DRM_IOCTL_MODE_DESTROY_DUMB can decrease refcount of *drm_vgem_gem_object *(created in *vgem_gem_dumb_create*) concurrently, and *vgem_gem_dumb_create *will access the freed drm_vgem_gem_object.",
        "id": 3260
    },
    {
        "cve_id": "CVE-2015-8963",
        "code_before_change": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (!head) {\n\t\t/*\n\t\t * We can race with cpu hotplug code. Do not\n\t\t * WARN if the cpu just got unplugged.\n\t\t */\n\t\tWARN_ON_ONCE(swhash->online);\n\t\treturn -EINVAL;\n\t}\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}",
        "code_after_change": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (WARN_ON_ONCE(!head))\n\t\treturn -EINVAL;\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,14 +12,8 @@\n \thwc->state = !(flags & PERF_EF_START);\n \n \thead = find_swevent_head(swhash, event);\n-\tif (!head) {\n-\t\t/*\n-\t\t * We can race with cpu hotplug code. Do not\n-\t\t * WARN if the cpu just got unplugged.\n-\t\t */\n-\t\tWARN_ON_ONCE(swhash->online);\n+\tif (WARN_ON_ONCE(!head))\n \t\treturn -EINVAL;\n-\t}\n \n \thlist_add_head_rcu(&event->hlist_entry, head);\n \tperf_event_update_userpage(event);",
        "function_modified_lines": {
            "added": [
                "\tif (WARN_ON_ONCE(!head))"
            ],
            "deleted": [
                "\tif (!head) {",
                "\t\t/*",
                "\t\t * We can race with cpu hotplug code. Do not",
                "\t\t * WARN if the cpu just got unplugged.",
                "\t\t */",
                "\t\tWARN_ON_ONCE(swhash->online);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "id": 871
    },
    {
        "cve_id": "CVE-2020-27784",
        "code_before_change": "static int\nprinter_close(struct inode *inode, struct file *fd)\n{\n\tstruct printer_dev\t*dev = fd->private_data;\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tdev->printer_cdev_open = 0;\n\tfd->private_data = NULL;\n\t/* Change printer status to show that the printer is off-line. */\n\tdev->printer_status &= ~PRINTER_SELECTED;\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\tDBG(dev, \"printer_close\\n\");\n\n\treturn 0;\n}",
        "code_after_change": "static int\nprinter_close(struct inode *inode, struct file *fd)\n{\n\tstruct printer_dev\t*dev = fd->private_data;\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tdev->printer_cdev_open = 0;\n\tfd->private_data = NULL;\n\t/* Change printer status to show that the printer is off-line. */\n\tdev->printer_status &= ~PRINTER_SELECTED;\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\tkref_put(&dev->kref, printer_dev_free);\n\tDBG(dev, \"printer_close\\n\");\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \tdev->printer_status &= ~PRINTER_SELECTED;\n \tspin_unlock_irqrestore(&dev->lock, flags);\n \n+\tkref_put(&dev->kref, printer_dev_free);\n \tDBG(dev, \"printer_close\\n\");\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tkref_put(&dev->kref, printer_dev_free);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel, where accessing a deallocated instance in printer_ioctl() printer_ioctl() tries to access of a printer_dev instance. However, use-after-free arises because it had been freed by gprinter_free().",
        "id": 2630
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
        "code_after_change": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,11 @@\n \t\tif (kthread_should_stop())\n \t\t\tbreak;\n \n-\t\tmutex_lock(&dev->mutex);\n+\t\tif (!mutex_trylock(&dev->mutex)) {\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tcur_jiffies = jiffies;\n \t\tif (dev->out_seq_resync) {\n \t\t\tdev->jiffies_vid_out = cur_jiffies;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mutex_trylock(&dev->mutex)) {",
                "\t\t\tschedule_timeout_uninterruptible(1);",
                "\t\t\tcontinue;",
                "\t\t}",
                ""
            ],
            "deleted": [
                "\t\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free.",
        "id": 2093
    },
    {
        "cve_id": "CVE-2020-0429",
        "code_before_change": "int l2tp_session_delete(struct l2tp_session *session)\n{\n\tif (session->ref)\n\t\t(*session->ref)(session);\n\t__l2tp_session_unhash(session);\n\tl2tp_session_queue_purge(session);\n\tif (session->session_close != NULL)\n\t\t(*session->session_close)(session);\n\tif (session->deref)\n\t\t(*session->deref)(session);\n\tl2tp_session_dec_refcount(session);\n\treturn 0;\n}",
        "code_after_change": "int l2tp_session_delete(struct l2tp_session *session)\n{\n\tif (test_and_set_bit(0, &session->dead))\n\t\treturn 0;\n\n\tif (session->ref)\n\t\t(*session->ref)(session);\n\t__l2tp_session_unhash(session);\n\tl2tp_session_queue_purge(session);\n\tif (session->session_close != NULL)\n\t\t(*session->session_close)(session);\n\tif (session->deref)\n\t\t(*session->deref)(session);\n\tl2tp_session_dec_refcount(session);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,8 @@\n int l2tp_session_delete(struct l2tp_session *session)\n {\n+\tif (test_and_set_bit(0, &session->dead))\n+\t\treturn 0;\n+\n \tif (session->ref)\n \t\t(*session->ref)(session);\n \t__l2tp_session_unhash(session);",
        "function_modified_lines": {
            "added": [
                "\tif (test_and_set_bit(0, &session->dead))",
                "\t\treturn 0;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In l2tp_session_delete and related functions of l2tp_core.c, there is possible memory corruption due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152735806",
        "id": 2382
    },
    {
        "cve_id": "CVE-2018-14625",
        "code_before_change": "static int\nvhost_transport_cancel_pkt(struct vsock_sock *vsk)\n{\n\tstruct vhost_vsock *vsock;\n\tstruct virtio_vsock_pkt *pkt, *n;\n\tint cnt = 0;\n\tLIST_HEAD(freeme);\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\n\tif (!vsock)\n\t\treturn -ENODEV;\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\n\t\tif (pkt->vsk != vsk)\n\t\t\tcontinue;\n\t\tlist_move(&pkt->list, &freeme);\n\t}\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tlist_for_each_entry_safe(pkt, n, &freeme, list) {\n\t\tif (pkt->reply)\n\t\t\tcnt++;\n\t\tlist_del(&pkt->list);\n\t\tvirtio_transport_free_pkt(pkt);\n\t}\n\n\tif (cnt) {\n\t\tstruct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];\n\t\tint new_cnt;\n\n\t\tnew_cnt = atomic_sub_return(cnt, &vsock->queued_replies);\n\t\tif (new_cnt + cnt >= tx_vq->num && new_cnt < tx_vq->num)\n\t\t\tvhost_poll_queue(&tx_vq->poll);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int\nvhost_transport_cancel_pkt(struct vsock_sock *vsk)\n{\n\tstruct vhost_vsock *vsock;\n\tstruct virtio_vsock_pkt *pkt, *n;\n\tint cnt = 0;\n\tint ret = -ENODEV;\n\tLIST_HEAD(freeme);\n\n\trcu_read_lock();\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\n\tif (!vsock)\n\t\tgoto out;\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\n\t\tif (pkt->vsk != vsk)\n\t\t\tcontinue;\n\t\tlist_move(&pkt->list, &freeme);\n\t}\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tlist_for_each_entry_safe(pkt, n, &freeme, list) {\n\t\tif (pkt->reply)\n\t\t\tcnt++;\n\t\tlist_del(&pkt->list);\n\t\tvirtio_transport_free_pkt(pkt);\n\t}\n\n\tif (cnt) {\n\t\tstruct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];\n\t\tint new_cnt;\n\n\t\tnew_cnt = atomic_sub_return(cnt, &vsock->queued_replies);\n\t\tif (new_cnt + cnt >= tx_vq->num && new_cnt < tx_vq->num)\n\t\t\tvhost_poll_queue(&tx_vq->poll);\n\t}\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,12 +4,15 @@\n \tstruct vhost_vsock *vsock;\n \tstruct virtio_vsock_pkt *pkt, *n;\n \tint cnt = 0;\n+\tint ret = -ENODEV;\n \tLIST_HEAD(freeme);\n+\n+\trcu_read_lock();\n \n \t/* Find the vhost_vsock according to guest context id  */\n \tvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\n \tif (!vsock)\n-\t\treturn -ENODEV;\n+\t\tgoto out;\n \n \tspin_lock_bh(&vsock->send_pkt_list_lock);\n \tlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\n@@ -35,5 +38,8 @@\n \t\t\tvhost_poll_queue(&tx_vq->poll);\n \t}\n \n-\treturn 0;\n+\tret = 0;\n+out:\n+\trcu_read_unlock();\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint ret = -ENODEV;",
                "",
                "\trcu_read_lock();",
                "\t\tgoto out;",
                "\tret = 0;",
                "out:",
                "\trcu_read_unlock();",
                "\treturn ret;"
            ],
            "deleted": [
                "\t\treturn -ENODEV;",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux Kernel where an attacker may be able to have an uncontrolled read to kernel-memory from within a vm guest. A race condition between connect() and close() function may allow an attacker using the AF_VSOCK protocol to gather a 4 byte information leak or possibly intercept or corrupt AF_VSOCK messages destined to other clients.",
        "id": 1694
    },
    {
        "cve_id": "CVE-2018-14625",
        "code_before_change": "static int\nvhost_transport_send_pkt(struct virtio_vsock_pkt *pkt)\n{\n\tstruct vhost_vsock *vsock;\n\tint len = pkt->len;\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\n\tif (!vsock) {\n\t\tvirtio_transport_free_pkt(pkt);\n\t\treturn -ENODEV;\n\t}\n\n\tif (pkt->reply)\n\t\tatomic_inc(&vsock->queued_replies);\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_add_tail(&pkt->list, &vsock->send_pkt_list);\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\n\treturn len;\n}",
        "code_after_change": "static int\nvhost_transport_send_pkt(struct virtio_vsock_pkt *pkt)\n{\n\tstruct vhost_vsock *vsock;\n\tint len = pkt->len;\n\n\trcu_read_lock();\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\n\tif (!vsock) {\n\t\trcu_read_unlock();\n\t\tvirtio_transport_free_pkt(pkt);\n\t\treturn -ENODEV;\n\t}\n\n\tif (pkt->reply)\n\t\tatomic_inc(&vsock->queued_replies);\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_add_tail(&pkt->list, &vsock->send_pkt_list);\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\n\n\trcu_read_unlock();\n\treturn len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,12 @@\n \tstruct vhost_vsock *vsock;\n \tint len = pkt->len;\n \n+\trcu_read_lock();\n+\n \t/* Find the vhost_vsock according to guest context id  */\n \tvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\n \tif (!vsock) {\n+\t\trcu_read_unlock();\n \t\tvirtio_transport_free_pkt(pkt);\n \t\treturn -ENODEV;\n \t}\n@@ -19,5 +22,7 @@\n \tspin_unlock_bh(&vsock->send_pkt_list_lock);\n \n \tvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\n+\n+\trcu_read_unlock();\n \treturn len;\n }",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "",
                "\t\trcu_read_unlock();",
                "",
                "\trcu_read_unlock();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux Kernel where an attacker may be able to have an uncontrolled read to kernel-memory from within a vm guest. A race condition between connect() and close() function may allow an attacker using the AF_VSOCK protocol to gather a 4 byte information leak or possibly intercept or corrupt AF_VSOCK messages destined to other clients.",
        "id": 1695
    },
    {
        "cve_id": "CVE-2023-6111",
        "code_before_change": "static struct nft_trans_gc *nft_trans_gc_catchall(struct nft_trans_gc *gc,\n\t\t\t\t\t\t  unsigned int gc_seq,\n\t\t\t\t\t\t  bool sync)\n{\n\tstruct nft_set_elem_catchall *catchall;\n\tconst struct nft_set *set = gc->set;\n\tstruct nft_set_ext *ext;\n\n\tlist_for_each_entry_rcu(catchall, &set->catchall_list, list) {\n\t\text = nft_set_elem_ext(set, catchall->elem);\n\n\t\tif (!nft_set_elem_expired(ext))\n\t\t\tcontinue;\n\t\tif (nft_set_elem_is_dead(ext))\n\t\t\tgoto dead_elem;\n\n\t\tnft_set_elem_dead(ext);\ndead_elem:\n\t\tif (sync)\n\t\t\tgc = nft_trans_gc_queue_sync(gc, GFP_ATOMIC);\n\t\telse\n\t\t\tgc = nft_trans_gc_queue_async(gc, gc_seq, GFP_ATOMIC);\n\n\t\tif (!gc)\n\t\t\treturn NULL;\n\n\t\tnft_trans_gc_elem_add(gc, catchall->elem);\n\t}\n\n\treturn gc;\n}",
        "code_after_change": "static struct nft_trans_gc *nft_trans_gc_catchall(struct nft_trans_gc *gc,\n\t\t\t\t\t\t  unsigned int gc_seq,\n\t\t\t\t\t\t  bool sync)\n{\n\tstruct nft_set_elem_catchall *catchall, *next;\n\tconst struct nft_set *set = gc->set;\n\tstruct nft_elem_priv *elem_priv;\n\tstruct nft_set_ext *ext;\n\n\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {\n\t\text = nft_set_elem_ext(set, catchall->elem);\n\n\t\tif (!nft_set_elem_expired(ext))\n\t\t\tcontinue;\n\t\tif (nft_set_elem_is_dead(ext))\n\t\t\tgoto dead_elem;\n\n\t\tnft_set_elem_dead(ext);\ndead_elem:\n\t\tif (sync)\n\t\t\tgc = nft_trans_gc_queue_sync(gc, GFP_ATOMIC);\n\t\telse\n\t\t\tgc = nft_trans_gc_queue_async(gc, gc_seq, GFP_ATOMIC);\n\n\t\tif (!gc)\n\t\t\treturn NULL;\n\n\t\telem_priv = catchall->elem;\n\t\tif (sync) {\n\t\t\tnft_setelem_data_deactivate(gc->net, gc->set, elem_priv);\n\t\t\tnft_setelem_catchall_destroy(catchall);\n\t\t}\n\n\t\tnft_trans_gc_elem_add(gc, elem_priv);\n\t}\n\n\treturn gc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,11 +2,12 @@\n \t\t\t\t\t\t  unsigned int gc_seq,\n \t\t\t\t\t\t  bool sync)\n {\n-\tstruct nft_set_elem_catchall *catchall;\n+\tstruct nft_set_elem_catchall *catchall, *next;\n \tconst struct nft_set *set = gc->set;\n+\tstruct nft_elem_priv *elem_priv;\n \tstruct nft_set_ext *ext;\n \n-\tlist_for_each_entry_rcu(catchall, &set->catchall_list, list) {\n+\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {\n \t\text = nft_set_elem_ext(set, catchall->elem);\n \n \t\tif (!nft_set_elem_expired(ext))\n@@ -24,7 +25,13 @@\n \t\tif (!gc)\n \t\t\treturn NULL;\n \n-\t\tnft_trans_gc_elem_add(gc, catchall->elem);\n+\t\telem_priv = catchall->elem;\n+\t\tif (sync) {\n+\t\t\tnft_setelem_data_deactivate(gc->net, gc->set, elem_priv);\n+\t\t\tnft_setelem_catchall_destroy(catchall);\n+\t\t}\n+\n+\t\tnft_trans_gc_elem_add(gc, elem_priv);\n \t}\n \n \treturn gc;",
        "function_modified_lines": {
            "added": [
                "\tstruct nft_set_elem_catchall *catchall, *next;",
                "\tstruct nft_elem_priv *elem_priv;",
                "\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {",
                "\t\telem_priv = catchall->elem;",
                "\t\tif (sync) {",
                "\t\t\tnft_setelem_data_deactivate(gc->net, gc->set, elem_priv);",
                "\t\t\tnft_setelem_catchall_destroy(catchall);",
                "\t\t}",
                "",
                "\t\tnft_trans_gc_elem_add(gc, elem_priv);"
            ],
            "deleted": [
                "\tstruct nft_set_elem_catchall *catchall;",
                "\tlist_for_each_entry_rcu(catchall, &set->catchall_list, list) {",
                "\t\tnft_trans_gc_elem_add(gc, catchall->elem);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nThe function nft_trans_gc_catchall did not remove the catchall set element from the catchall_list when the argument sync is true, making it possible to free a catchall set element many times.\n\nWe recommend upgrading past commit 93995bf4af2c5a99e2a87f0cd5ce547d31eb7630.\n\n",
        "id": 4296
    },
    {
        "cve_id": "CVE-2019-15292",
        "code_before_change": "void __exit atalk_proc_exit(void)\n{\n\tremove_proc_subtree(\"atalk\", init_net.proc_net);\n}",
        "code_after_change": "void atalk_proc_exit(void)\n{\n\tremove_proc_subtree(\"atalk\", init_net.proc_net);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-void __exit atalk_proc_exit(void)\n+void atalk_proc_exit(void)\n {\n \tremove_proc_subtree(\"atalk\", init_net.proc_net);\n }",
        "function_modified_lines": {
            "added": [
                "void atalk_proc_exit(void)"
            ],
            "deleted": [
                "void __exit atalk_proc_exit(void)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.9. There is a use-after-free in atalk_proc_exit, related to net/appletalk/atalk_proc.c, net/appletalk/ddp.c, and net/appletalk/sysctl_net_atalk.c.",
        "id": 2015
    },
    {
        "cve_id": "CVE-2019-9003",
        "code_before_change": "static void free_user(struct kref *ref)\n{\n\tstruct ipmi_user *user = container_of(ref, struct ipmi_user, refcount);\n\tkfree(user);\n}",
        "code_after_change": "static void free_user(struct kref *ref)\n{\n\tstruct ipmi_user *user = container_of(ref, struct ipmi_user, refcount);\n\tcleanup_srcu_struct(&user->release_barrier);\n\tkfree(user);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static void free_user(struct kref *ref)\n {\n \tstruct ipmi_user *user = container_of(ref, struct ipmi_user, refcount);\n+\tcleanup_srcu_struct(&user->release_barrier);\n \tkfree(user);\n }",
        "function_modified_lines": {
            "added": [
                "\tcleanup_srcu_struct(&user->release_barrier);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 4.20.5, attackers can trigger a drivers/char/ipmi/ipmi_msghandler.c use-after-free and OOPS by arranging for certain simultaneous execution of the code, as demonstrated by a \"service ipmievd restart\" loop.",
        "id": 2350
    },
    {
        "cve_id": "CVE-2023-20928",
        "code_before_change": "void binder_selftest_alloc(struct binder_alloc *alloc)\n{\n\tsize_t end_offset[BUFFER_NUM];\n\n\tif (!binder_selftest_run)\n\t\treturn;\n\tmutex_lock(&binder_selftest_lock);\n\tif (!binder_selftest_run || !alloc->vma)\n\t\tgoto done;\n\tpr_info(\"STARTED\\n\");\n\tbinder_selftest_alloc_offset(alloc, end_offset, 0);\n\tbinder_selftest_run = false;\n\tif (binder_selftest_failures > 0)\n\t\tpr_info(\"%d tests FAILED\\n\", binder_selftest_failures);\n\telse\n\t\tpr_info(\"PASSED\\n\");\n\ndone:\n\tmutex_unlock(&binder_selftest_lock);\n}",
        "code_after_change": "void binder_selftest_alloc(struct binder_alloc *alloc)\n{\n\tsize_t end_offset[BUFFER_NUM];\n\n\tif (!binder_selftest_run)\n\t\treturn;\n\tmutex_lock(&binder_selftest_lock);\n\tif (!binder_selftest_run || !alloc->vma_addr)\n\t\tgoto done;\n\tpr_info(\"STARTED\\n\");\n\tbinder_selftest_alloc_offset(alloc, end_offset, 0);\n\tbinder_selftest_run = false;\n\tif (binder_selftest_failures > 0)\n\t\tpr_info(\"%d tests FAILED\\n\", binder_selftest_failures);\n\telse\n\t\tpr_info(\"PASSED\\n\");\n\ndone:\n\tmutex_unlock(&binder_selftest_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tif (!binder_selftest_run)\n \t\treturn;\n \tmutex_lock(&binder_selftest_lock);\n-\tif (!binder_selftest_run || !alloc->vma)\n+\tif (!binder_selftest_run || !alloc->vma_addr)\n \t\tgoto done;\n \tpr_info(\"STARTED\\n\");\n \tbinder_selftest_alloc_offset(alloc, end_offset, 0);",
        "function_modified_lines": {
            "added": [
                "\tif (!binder_selftest_run || !alloc->vma_addr)"
            ],
            "deleted": [
                "\tif (!binder_selftest_run || !alloc->vma)"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_vma_close of binder.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-254837884References: Upstream kernel",
        "id": 3911
    },
    {
        "cve_id": "CVE-2021-32606",
        "code_before_change": "static int isotp_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t    sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct isotp_sock *so = isotp_sk(sk);\n\tint ret = 0;\n\n\tif (level != SOL_CAN_ISOTP)\n\t\treturn -EINVAL;\n\n\tif (so->bound)\n\t\treturn -EISCONN;\n\n\tswitch (optname) {\n\tcase CAN_ISOTP_OPTS:\n\t\tif (optlen != sizeof(struct can_isotp_options))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&so->opt, optval, optlen))\n\t\t\treturn -EFAULT;\n\n\t\t/* no separate rx_ext_address is given => use ext_address */\n\t\tif (!(so->opt.flags & CAN_ISOTP_RX_EXT_ADDR))\n\t\t\tso->opt.rx_ext_address = so->opt.ext_address;\n\t\tbreak;\n\n\tcase CAN_ISOTP_RECV_FC:\n\t\tif (optlen != sizeof(struct can_isotp_fc_options))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&so->rxfc, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\n\tcase CAN_ISOTP_TX_STMIN:\n\t\tif (optlen != sizeof(u32))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&so->force_tx_stmin, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\n\tcase CAN_ISOTP_RX_STMIN:\n\t\tif (optlen != sizeof(u32))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&so->force_rx_stmin, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\n\tcase CAN_ISOTP_LL_OPTS:\n\t\tif (optlen == sizeof(struct can_isotp_ll_options)) {\n\t\t\tstruct can_isotp_ll_options ll;\n\n\t\t\tif (copy_from_sockptr(&ll, optval, optlen))\n\t\t\t\treturn -EFAULT;\n\n\t\t\t/* check for correct ISO 11898-1 DLC data length */\n\t\t\tif (ll.tx_dl != padlen(ll.tx_dl))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (ll.mtu != CAN_MTU && ll.mtu != CANFD_MTU)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (ll.mtu == CAN_MTU &&\n\t\t\t    (ll.tx_dl > CAN_MAX_DLEN || ll.tx_flags != 0))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tmemcpy(&so->ll, &ll, sizeof(ll));\n\n\t\t\t/* set ll_dl for tx path to similar place as for rx */\n\t\t\tso->tx.ll_dl = ll.tx_dl;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int isotp_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t    sockptr_t optval, unsigned int optlen)\n\n{\n\tstruct sock *sk = sock->sk;\n\tint ret;\n\n\tif (level != SOL_CAN_ISOTP)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tret = isotp_setsockopt_locked(sock, level, optname, optval, optlen);\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,83 +1,15 @@\n static int isotp_setsockopt(struct socket *sock, int level, int optname,\n \t\t\t    sockptr_t optval, unsigned int optlen)\n+\n {\n \tstruct sock *sk = sock->sk;\n-\tstruct isotp_sock *so = isotp_sk(sk);\n-\tint ret = 0;\n+\tint ret;\n \n \tif (level != SOL_CAN_ISOTP)\n \t\treturn -EINVAL;\n \n-\tif (so->bound)\n-\t\treturn -EISCONN;\n-\n-\tswitch (optname) {\n-\tcase CAN_ISOTP_OPTS:\n-\t\tif (optlen != sizeof(struct can_isotp_options))\n-\t\t\treturn -EINVAL;\n-\n-\t\tif (copy_from_sockptr(&so->opt, optval, optlen))\n-\t\t\treturn -EFAULT;\n-\n-\t\t/* no separate rx_ext_address is given => use ext_address */\n-\t\tif (!(so->opt.flags & CAN_ISOTP_RX_EXT_ADDR))\n-\t\t\tso->opt.rx_ext_address = so->opt.ext_address;\n-\t\tbreak;\n-\n-\tcase CAN_ISOTP_RECV_FC:\n-\t\tif (optlen != sizeof(struct can_isotp_fc_options))\n-\t\t\treturn -EINVAL;\n-\n-\t\tif (copy_from_sockptr(&so->rxfc, optval, optlen))\n-\t\t\treturn -EFAULT;\n-\t\tbreak;\n-\n-\tcase CAN_ISOTP_TX_STMIN:\n-\t\tif (optlen != sizeof(u32))\n-\t\t\treturn -EINVAL;\n-\n-\t\tif (copy_from_sockptr(&so->force_tx_stmin, optval, optlen))\n-\t\t\treturn -EFAULT;\n-\t\tbreak;\n-\n-\tcase CAN_ISOTP_RX_STMIN:\n-\t\tif (optlen != sizeof(u32))\n-\t\t\treturn -EINVAL;\n-\n-\t\tif (copy_from_sockptr(&so->force_rx_stmin, optval, optlen))\n-\t\t\treturn -EFAULT;\n-\t\tbreak;\n-\n-\tcase CAN_ISOTP_LL_OPTS:\n-\t\tif (optlen == sizeof(struct can_isotp_ll_options)) {\n-\t\t\tstruct can_isotp_ll_options ll;\n-\n-\t\t\tif (copy_from_sockptr(&ll, optval, optlen))\n-\t\t\t\treturn -EFAULT;\n-\n-\t\t\t/* check for correct ISO 11898-1 DLC data length */\n-\t\t\tif (ll.tx_dl != padlen(ll.tx_dl))\n-\t\t\t\treturn -EINVAL;\n-\n-\t\t\tif (ll.mtu != CAN_MTU && ll.mtu != CANFD_MTU)\n-\t\t\t\treturn -EINVAL;\n-\n-\t\t\tif (ll.mtu == CAN_MTU &&\n-\t\t\t    (ll.tx_dl > CAN_MAX_DLEN || ll.tx_flags != 0))\n-\t\t\t\treturn -EINVAL;\n-\n-\t\t\tmemcpy(&so->ll, &ll, sizeof(ll));\n-\n-\t\t\t/* set ll_dl for tx path to similar place as for rx */\n-\t\t\tso->tx.ll_dl = ll.tx_dl;\n-\t\t} else {\n-\t\t\treturn -EINVAL;\n-\t\t}\n-\t\tbreak;\n-\n-\tdefault:\n-\t\tret = -ENOPROTOOPT;\n-\t}\n-\n+\tlock_sock(sk);\n+\tret = isotp_setsockopt_locked(sock, level, optname, optval, optlen);\n+\trelease_sock(sk);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tint ret;",
                "\tlock_sock(sk);",
                "\tret = isotp_setsockopt_locked(sock, level, optname, optval, optlen);",
                "\trelease_sock(sk);"
            ],
            "deleted": [
                "\tstruct isotp_sock *so = isotp_sk(sk);",
                "\tint ret = 0;",
                "\tif (so->bound)",
                "\t\treturn -EISCONN;",
                "",
                "\tswitch (optname) {",
                "\tcase CAN_ISOTP_OPTS:",
                "\t\tif (optlen != sizeof(struct can_isotp_options))",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (copy_from_sockptr(&so->opt, optval, optlen))",
                "\t\t\treturn -EFAULT;",
                "",
                "\t\t/* no separate rx_ext_address is given => use ext_address */",
                "\t\tif (!(so->opt.flags & CAN_ISOTP_RX_EXT_ADDR))",
                "\t\t\tso->opt.rx_ext_address = so->opt.ext_address;",
                "\t\tbreak;",
                "",
                "\tcase CAN_ISOTP_RECV_FC:",
                "\t\tif (optlen != sizeof(struct can_isotp_fc_options))",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (copy_from_sockptr(&so->rxfc, optval, optlen))",
                "\t\t\treturn -EFAULT;",
                "\t\tbreak;",
                "",
                "\tcase CAN_ISOTP_TX_STMIN:",
                "\t\tif (optlen != sizeof(u32))",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (copy_from_sockptr(&so->force_tx_stmin, optval, optlen))",
                "\t\t\treturn -EFAULT;",
                "\t\tbreak;",
                "",
                "\tcase CAN_ISOTP_RX_STMIN:",
                "\t\tif (optlen != sizeof(u32))",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (copy_from_sockptr(&so->force_rx_stmin, optval, optlen))",
                "\t\t\treturn -EFAULT;",
                "\t\tbreak;",
                "",
                "\tcase CAN_ISOTP_LL_OPTS:",
                "\t\tif (optlen == sizeof(struct can_isotp_ll_options)) {",
                "\t\t\tstruct can_isotp_ll_options ll;",
                "",
                "\t\t\tif (copy_from_sockptr(&ll, optval, optlen))",
                "\t\t\t\treturn -EFAULT;",
                "",
                "\t\t\t/* check for correct ISO 11898-1 DLC data length */",
                "\t\t\tif (ll.tx_dl != padlen(ll.tx_dl))",
                "\t\t\t\treturn -EINVAL;",
                "",
                "\t\t\tif (ll.mtu != CAN_MTU && ll.mtu != CANFD_MTU)",
                "\t\t\t\treturn -EINVAL;",
                "",
                "\t\t\tif (ll.mtu == CAN_MTU &&",
                "\t\t\t    (ll.tx_dl > CAN_MAX_DLEN || ll.tx_flags != 0))",
                "\t\t\t\treturn -EINVAL;",
                "",
                "\t\t\tmemcpy(&so->ll, &ll, sizeof(ll));",
                "",
                "\t\t\t/* set ll_dl for tx path to similar place as for rx */",
                "\t\t\tso->tx.ll_dl = ll.tx_dl;",
                "\t\t} else {",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "\t\tbreak;",
                "",
                "\tdefault:",
                "\t\tret = -ENOPROTOOPT;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.11 through 5.12.2, isotp_setsockopt in net/can/isotp.c allows privilege escalation to root by leveraging a use-after-free. (This does not affect earlier versions that lack CAN ISOTP SF_BROADCAST support.)",
        "id": 2966
    },
    {
        "cve_id": "CVE-2019-11811",
        "code_before_change": "int ipmi_si_mem_setup(struct si_sm_io *io)\n{\n\tunsigned long addr = io->addr_data;\n\tint           mapsize, idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\tio->io_cleanup = mem_cleanup;\n\n\t/*\n\t * Figure out the actual readb/readw/readl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = intf_mem_inb;\n\t\tio->outputb = intf_mem_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = intf_mem_inw;\n\t\tio->outputb = intf_mem_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = intf_mem_inl;\n\t\tio->outputb = intf_mem_outl;\n\t\tbreak;\n#ifdef readq\n\tcase 8:\n\t\tio->inputb = mem_inq;\n\t\tio->outputb = mem_outq;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint memory regions in their ACPI\n\t * tables.  This causes problems when trying to request the\n\t * entire region.  Therefore we must request each register\n\t * separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_mem_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\tmem_region_cleanup(io, idx);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the total amount of memory to claim.  This is an\n\t * unusual looking calculation, but it avoids claiming any\n\t * more memory than it has to.  It will claim everything\n\t * between the first address to the end of the last full\n\t * register.\n\t */\n\tmapsize = ((io->io_size * io->regspacing)\n\t\t   - (io->regspacing - io->regsize));\n\tio->addr = ioremap(addr, mapsize);\n\tif (io->addr == NULL) {\n\t\tmem_region_cleanup(io, io->io_size);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ipmi_si_mem_setup(struct si_sm_io *io)\n{\n\tunsigned long addr = io->addr_data;\n\tint           mapsize, idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\t/*\n\t * Figure out the actual readb/readw/readl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = intf_mem_inb;\n\t\tio->outputb = intf_mem_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = intf_mem_inw;\n\t\tio->outputb = intf_mem_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = intf_mem_inl;\n\t\tio->outputb = intf_mem_outl;\n\t\tbreak;\n#ifdef readq\n\tcase 8:\n\t\tio->inputb = mem_inq;\n\t\tio->outputb = mem_outq;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint memory regions in their ACPI\n\t * tables.  This causes problems when trying to request the\n\t * entire region.  Therefore we must request each register\n\t * separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_mem_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\tmem_region_cleanup(io, idx);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the total amount of memory to claim.  This is an\n\t * unusual looking calculation, but it avoids claiming any\n\t * more memory than it has to.  It will claim everything\n\t * between the first address to the end of the last full\n\t * register.\n\t */\n\tmapsize = ((io->io_size * io->regspacing)\n\t\t   - (io->regspacing - io->regsize));\n\tio->addr = ioremap(addr, mapsize);\n\tif (io->addr == NULL) {\n\t\tmem_region_cleanup(io, io->io_size);\n\t\treturn -EIO;\n\t}\n\n\tio->io_cleanup = mem_cleanup;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,6 @@\n \n \tif (!addr)\n \t\treturn -ENODEV;\n-\n-\tio->io_cleanup = mem_cleanup;\n \n \t/*\n \t * Figure out the actual readb/readw/readl/etc routine to use based\n@@ -66,5 +64,8 @@\n \t\tmem_region_cleanup(io, io->io_size);\n \t\treturn -EIO;\n \t}\n+\n+\tio->io_cleanup = mem_cleanup;\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tio->io_cleanup = mem_cleanup;",
                ""
            ],
            "deleted": [
                "",
                "\tio->io_cleanup = mem_cleanup;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.4. There is a use-after-free upon attempted read access to /proc/ioports after the ipmi_si module is removed, related to drivers/char/ipmi/ipmi_si_intf.c, drivers/char/ipmi/ipmi_si_mem_io.c, and drivers/char/ipmi/ipmi_si_port_io.c.",
        "id": 1933
    },
    {
        "cve_id": "CVE-2022-4379",
        "code_before_change": "static __be32\nnfsd4_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tunion nfsd4_op_u *u)\n{\n\tstruct nfsd4_copy *copy = &u->copy;\n\t__be32 status;\n\tstruct nfsd4_copy *async_copy = NULL;\n\n\tif (nfsd4_ssc_is_inter(copy)) {\n\t\tif (!inter_copy_offload_enable || nfsd4_copy_is_sync(copy)) {\n\t\t\tstatus = nfserr_notsupp;\n\t\t\tgoto out;\n\t\t}\n\t\tstatus = nfsd4_setup_inter_ssc(rqstp, cstate, copy,\n\t\t\t\t&copy->ss_mnt);\n\t\tif (status)\n\t\t\treturn nfserr_offload_denied;\n\t} else {\n\t\tstatus = nfsd4_setup_intra_ssc(rqstp, cstate, copy);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\n\tcopy->cp_clp = cstate->clp;\n\tmemcpy(&copy->fh, &cstate->current_fh.fh_handle,\n\t\tsizeof(struct knfsd_fh));\n\tif (nfsd4_copy_is_async(copy)) {\n\t\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\t\tstatus = nfserrno(-ENOMEM);\n\t\tasync_copy = kzalloc(sizeof(struct nfsd4_copy), GFP_KERNEL);\n\t\tif (!async_copy)\n\t\t\tgoto out_err;\n\t\tasync_copy->cp_src = kmalloc(sizeof(*async_copy->cp_src), GFP_KERNEL);\n\t\tif (!async_copy->cp_src)\n\t\t\tgoto out_err;\n\t\tif (!nfs4_init_copy_state(nn, copy))\n\t\t\tgoto out_err;\n\t\trefcount_set(&async_copy->refcount, 1);\n\t\tmemcpy(&copy->cp_res.cb_stateid, &copy->cp_stateid.cs_stid,\n\t\t\tsizeof(copy->cp_res.cb_stateid));\n\t\tdup_copy_fields(copy, async_copy);\n\t\tasync_copy->copy_task = kthread_create(nfsd4_do_async_copy,\n\t\t\t\tasync_copy, \"%s\", \"copy thread\");\n\t\tif (IS_ERR(async_copy->copy_task))\n\t\t\tgoto out_err;\n\t\tspin_lock(&async_copy->cp_clp->async_lock);\n\t\tlist_add(&async_copy->copies,\n\t\t\t\t&async_copy->cp_clp->async_copies);\n\t\tspin_unlock(&async_copy->cp_clp->async_lock);\n\t\twake_up_process(async_copy->copy_task);\n\t\tstatus = nfs_ok;\n\t} else {\n\t\tstatus = nfsd4_do_copy(copy, copy->nf_src->nf_file,\n\t\t\t\t       copy->nf_dst->nf_file, true);\n\t\tnfsd4_cleanup_intra_ssc(copy->nf_src, copy->nf_dst);\n\t}\nout:\n\treturn status;\nout_err:\n\tif (async_copy)\n\t\tcleanup_async_copy(async_copy);\n\tstatus = nfserrno(-ENOMEM);\n\tif (nfsd4_ssc_is_inter(copy))\n\t\tnfsd4_interssc_disconnect(copy->ss_mnt);\n\tgoto out;\n}",
        "code_after_change": "static __be32\nnfsd4_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tunion nfsd4_op_u *u)\n{\n\tstruct nfsd4_copy *copy = &u->copy;\n\t__be32 status;\n\tstruct nfsd4_copy *async_copy = NULL;\n\n\tif (nfsd4_ssc_is_inter(copy)) {\n\t\tif (!inter_copy_offload_enable || nfsd4_copy_is_sync(copy)) {\n\t\t\tstatus = nfserr_notsupp;\n\t\t\tgoto out;\n\t\t}\n\t\tstatus = nfsd4_setup_inter_ssc(rqstp, cstate, copy,\n\t\t\t\t&copy->ss_mnt);\n\t\tif (status)\n\t\t\treturn nfserr_offload_denied;\n\t} else {\n\t\tstatus = nfsd4_setup_intra_ssc(rqstp, cstate, copy);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\n\tcopy->cp_clp = cstate->clp;\n\tmemcpy(&copy->fh, &cstate->current_fh.fh_handle,\n\t\tsizeof(struct knfsd_fh));\n\tif (nfsd4_copy_is_async(copy)) {\n\t\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\t\tstatus = nfserrno(-ENOMEM);\n\t\tasync_copy = kzalloc(sizeof(struct nfsd4_copy), GFP_KERNEL);\n\t\tif (!async_copy)\n\t\t\tgoto out_err;\n\t\tasync_copy->cp_src = kmalloc(sizeof(*async_copy->cp_src), GFP_KERNEL);\n\t\tif (!async_copy->cp_src)\n\t\t\tgoto out_err;\n\t\tif (!nfs4_init_copy_state(nn, copy))\n\t\t\tgoto out_err;\n\t\trefcount_set(&async_copy->refcount, 1);\n\t\tmemcpy(&copy->cp_res.cb_stateid, &copy->cp_stateid.cs_stid,\n\t\t\tsizeof(copy->cp_res.cb_stateid));\n\t\tdup_copy_fields(copy, async_copy);\n\t\tasync_copy->copy_task = kthread_create(nfsd4_do_async_copy,\n\t\t\t\tasync_copy, \"%s\", \"copy thread\");\n\t\tif (IS_ERR(async_copy->copy_task))\n\t\t\tgoto out_err;\n\t\tspin_lock(&async_copy->cp_clp->async_lock);\n\t\tlist_add(&async_copy->copies,\n\t\t\t\t&async_copy->cp_clp->async_copies);\n\t\tspin_unlock(&async_copy->cp_clp->async_lock);\n\t\twake_up_process(async_copy->copy_task);\n\t\tstatus = nfs_ok;\n\t} else {\n\t\tstatus = nfsd4_do_copy(copy, copy->nf_src->nf_file,\n\t\t\t\t       copy->nf_dst->nf_file, true);\n\t\tnfsd4_cleanup_intra_ssc(copy->nf_src, copy->nf_dst);\n\t}\nout:\n\treturn status;\nout_err:\n\tif (async_copy)\n\t\tcleanup_async_copy(async_copy);\n\tstatus = nfserrno(-ENOMEM);\n\t/*\n\t * source's vfsmount of inter-copy will be unmounted\n\t * by the laundromat\n\t */\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -61,7 +61,9 @@\n \tif (async_copy)\n \t\tcleanup_async_copy(async_copy);\n \tstatus = nfserrno(-ENOMEM);\n-\tif (nfsd4_ssc_is_inter(copy))\n-\t\tnfsd4_interssc_disconnect(copy->ss_mnt);\n+\t/*\n+\t * source's vfsmount of inter-copy will be unmounted\n+\t * by the laundromat\n+\t */\n \tgoto out;\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * source's vfsmount of inter-copy will be unmounted",
                "\t * by the laundromat",
                "\t */"
            ],
            "deleted": [
                "\tif (nfsd4_ssc_is_inter(copy))",
                "\t\tnfsd4_interssc_disconnect(copy->ss_mnt);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in __nfs42_ssc_open() in fs/nfs/nfs4file.c in the Linux kernel. This flaw allows an attacker to conduct a remote denial",
        "id": 3746
    },
    {
        "cve_id": "CVE-2023-1193",
        "code_before_change": "int setup_async_work(struct ksmbd_work *work, void (*fn)(void **), void **arg)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct ksmbd_conn *conn = work->conn;\n\tint id;\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\trsp_hdr->Flags |= SMB2_FLAGS_ASYNC_COMMAND;\n\n\tid = ksmbd_acquire_async_msg_id(&conn->async_ida);\n\tif (id < 0) {\n\t\tpr_err(\"Failed to alloc async message id\\n\");\n\t\treturn id;\n\t}\n\twork->synchronous = false;\n\twork->async_id = id;\n\trsp_hdr->Id.AsyncId = cpu_to_le64(id);\n\n\tksmbd_debug(SMB,\n\t\t    \"Send interim Response to inform async request id : %d\\n\",\n\t\t    work->async_id);\n\n\twork->cancel_fn = fn;\n\twork->cancel_argv = arg;\n\n\tif (list_empty(&work->async_request_entry)) {\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_add_tail(&work->async_request_entry, &conn->async_requests);\n\t\tspin_unlock(&conn->request_lock);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int setup_async_work(struct ksmbd_work *work, void (*fn)(void **), void **arg)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct ksmbd_conn *conn = work->conn;\n\tint id;\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\trsp_hdr->Flags |= SMB2_FLAGS_ASYNC_COMMAND;\n\n\tid = ksmbd_acquire_async_msg_id(&conn->async_ida);\n\tif (id < 0) {\n\t\tpr_err(\"Failed to alloc async message id\\n\");\n\t\treturn id;\n\t}\n\twork->asynchronous = true;\n\twork->async_id = id;\n\trsp_hdr->Id.AsyncId = cpu_to_le64(id);\n\n\tksmbd_debug(SMB,\n\t\t    \"Send interim Response to inform async request id : %d\\n\",\n\t\t    work->async_id);\n\n\twork->cancel_fn = fn;\n\twork->cancel_argv = arg;\n\n\tif (list_empty(&work->async_request_entry)) {\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_add_tail(&work->async_request_entry, &conn->async_requests);\n\t\tspin_unlock(&conn->request_lock);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,7 @@\n \t\tpr_err(\"Failed to alloc async message id\\n\");\n \t\treturn id;\n \t}\n-\twork->synchronous = false;\n+\twork->asynchronous = true;\n \twork->async_id = id;\n \trsp_hdr->Id.AsyncId = cpu_to_le64(id);\n ",
        "function_modified_lines": {
            "added": [
                "\twork->asynchronous = true;"
            ],
            "deleted": [
                "\twork->synchronous = false;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in setup_async_work in the KSMBD implementation of the in-kernel samba server and CIFS in the Linux kernel. This issue could allow an attacker to crash the system by accessing freed work.",
        "id": 3853
    },
    {
        "cve_id": "CVE-2023-1193",
        "code_before_change": "int ksmbd_conn_try_dequeue_request(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tint ret = 1;\n\n\tif (list_empty(&work->request_entry) &&\n\t    list_empty(&work->async_request_entry))\n\t\treturn 0;\n\n\tif (!work->multiRsp)\n\t\tatomic_dec(&conn->req_running);\n\tspin_lock(&conn->request_lock);\n\tif (!work->multiRsp) {\n\t\tlist_del_init(&work->request_entry);\n\t\tif (!work->synchronous)\n\t\t\tlist_del_init(&work->async_request_entry);\n\t\tret = 0;\n\t}\n\tspin_unlock(&conn->request_lock);\n\n\twake_up_all(&conn->req_running_q);\n\treturn ret;\n}",
        "code_after_change": "int ksmbd_conn_try_dequeue_request(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tint ret = 1;\n\n\tif (list_empty(&work->request_entry) &&\n\t    list_empty(&work->async_request_entry))\n\t\treturn 0;\n\n\tif (!work->multiRsp)\n\t\tatomic_dec(&conn->req_running);\n\tif (!work->multiRsp) {\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_del_init(&work->request_entry);\n\t\tspin_unlock(&conn->request_lock);\n\t\tif (work->asynchronous)\n\t\t\trelease_async_work(work);\n\t\tret = 0;\n\t}\n\n\twake_up_all(&conn->req_running_q);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,14 +9,14 @@\n \n \tif (!work->multiRsp)\n \t\tatomic_dec(&conn->req_running);\n-\tspin_lock(&conn->request_lock);\n \tif (!work->multiRsp) {\n+\t\tspin_lock(&conn->request_lock);\n \t\tlist_del_init(&work->request_entry);\n-\t\tif (!work->synchronous)\n-\t\t\tlist_del_init(&work->async_request_entry);\n+\t\tspin_unlock(&conn->request_lock);\n+\t\tif (work->asynchronous)\n+\t\t\trelease_async_work(work);\n \t\tret = 0;\n \t}\n-\tspin_unlock(&conn->request_lock);\n \n \twake_up_all(&conn->req_running_q);\n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "\t\tspin_lock(&conn->request_lock);",
                "\t\tspin_unlock(&conn->request_lock);",
                "\t\tif (work->asynchronous)",
                "\t\t\trelease_async_work(work);"
            ],
            "deleted": [
                "\tspin_lock(&conn->request_lock);",
                "\t\tif (!work->synchronous)",
                "\t\t\tlist_del_init(&work->async_request_entry);",
                "\tspin_unlock(&conn->request_lock);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in setup_async_work in the KSMBD implementation of the in-kernel samba server and CIFS in the Linux kernel. This issue could allow an attacker to crash the system by accessing freed work.",
        "id": 3851
    },
    {
        "cve_id": "CVE-2023-1193",
        "code_before_change": "int init_smb2_rsp_hdr(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr = smb2_get_msg(work->response_buf);\n\tstruct smb2_hdr *rcv_hdr = smb2_get_msg(work->request_buf);\n\tstruct ksmbd_conn *conn = work->conn;\n\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\trsp_hdr->ProtocolId = rcv_hdr->ProtocolId;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->Command = rcv_hdr->Command;\n\n\t/*\n\t * Message is response. We don't grant oplock yet.\n\t */\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = rcv_hdr->MessageId;\n\trsp_hdr->Id.SyncId.ProcessId = rcv_hdr->Id.SyncId.ProcessId;\n\trsp_hdr->Id.SyncId.TreeId = rcv_hdr->Id.SyncId.TreeId;\n\trsp_hdr->SessionId = rcv_hdr->SessionId;\n\tmemcpy(rsp_hdr->Signature, rcv_hdr->Signature, 16);\n\n\twork->synchronous = true;\n\tif (work->async_id) {\n\t\tksmbd_release_id(&conn->async_ida, work->async_id);\n\t\twork->async_id = 0;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int init_smb2_rsp_hdr(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr = smb2_get_msg(work->response_buf);\n\tstruct smb2_hdr *rcv_hdr = smb2_get_msg(work->request_buf);\n\tstruct ksmbd_conn *conn = work->conn;\n\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\trsp_hdr->ProtocolId = rcv_hdr->ProtocolId;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->Command = rcv_hdr->Command;\n\n\t/*\n\t * Message is response. We don't grant oplock yet.\n\t */\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = rcv_hdr->MessageId;\n\trsp_hdr->Id.SyncId.ProcessId = rcv_hdr->Id.SyncId.ProcessId;\n\trsp_hdr->Id.SyncId.TreeId = rcv_hdr->Id.SyncId.TreeId;\n\trsp_hdr->SessionId = rcv_hdr->SessionId;\n\tmemcpy(rsp_hdr->Signature, rcv_hdr->Signature, 16);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,11 +22,5 @@\n \trsp_hdr->SessionId = rcv_hdr->SessionId;\n \tmemcpy(rsp_hdr->Signature, rcv_hdr->Signature, 16);\n \n-\twork->synchronous = true;\n-\tif (work->async_id) {\n-\t\tksmbd_release_id(&conn->async_ida, work->async_id);\n-\t\twork->async_id = 0;\n-\t}\n-\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\twork->synchronous = true;",
                "\tif (work->async_id) {",
                "\t\tksmbd_release_id(&conn->async_ida, work->async_id);",
                "\t\twork->async_id = 0;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in setup_async_work in the KSMBD implementation of the in-kernel samba server and CIFS in the Linux kernel. This issue could allow an attacker to crash the system by accessing freed work.",
        "id": 3854
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n}",
        "code_after_change": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,5 +4,5 @@\n \t\treturn -1;\n \tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n \t\treturn -1;\n-\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n+\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);"
            ],
            "deleted": [
                "\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "id": 2621
    },
    {
        "cve_id": "CVE-2023-2162",
        "code_before_change": "static int iscsi_sw_tcp_host_get_param(struct Scsi_Host *shost,\n\t\t\t\t       enum iscsi_host_param param, char *buf)\n{\n\tstruct iscsi_sw_tcp_host *tcp_sw_host = iscsi_host_priv(shost);\n\tstruct iscsi_session *session = tcp_sw_host->session;\n\tstruct iscsi_conn *conn;\n\tstruct iscsi_tcp_conn *tcp_conn;\n\tstruct iscsi_sw_tcp_conn *tcp_sw_conn;\n\tstruct sockaddr_in6 addr;\n\tstruct socket *sock;\n\tint rc;\n\n\tswitch (param) {\n\tcase ISCSI_HOST_PARAM_IPADDRESS:\n\t\tif (!session)\n\t\t\treturn -ENOTCONN;\n\n\t\tspin_lock_bh(&session->frwd_lock);\n\t\tconn = session->leadconn;\n\t\tif (!conn) {\n\t\t\tspin_unlock_bh(&session->frwd_lock);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\ttcp_conn = conn->dd_data;\n\t\ttcp_sw_conn = tcp_conn->dd_data;\n\t\t/*\n\t\t * The conn has been setup and bound, so just grab a ref\n\t\t * incase a destroy runs while we are in the net layer.\n\t\t */\n\t\tiscsi_get_conn(conn->cls_conn);\n\t\tspin_unlock_bh(&session->frwd_lock);\n\n\t\tmutex_lock(&tcp_sw_conn->sock_lock);\n\t\tsock = tcp_sw_conn->sock;\n\t\tif (!sock)\n\t\t\trc = -ENOTCONN;\n\t\telse\n\t\t\trc = kernel_getsockname(sock, (struct sockaddr *)&addr);\n\t\tmutex_unlock(&tcp_sw_conn->sock_lock);\n\t\tiscsi_put_conn(conn->cls_conn);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\n\t\treturn iscsi_conn_get_addr_param((struct sockaddr_storage *)\n\t\t\t\t\t\t &addr,\n\t\t\t\t\t\t (enum iscsi_param)param, buf);\n\tdefault:\n\t\treturn iscsi_host_get_param(shost, param, buf);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int iscsi_sw_tcp_host_get_param(struct Scsi_Host *shost,\n\t\t\t\t       enum iscsi_host_param param, char *buf)\n{\n\tstruct iscsi_sw_tcp_host *tcp_sw_host = iscsi_host_priv(shost);\n\tstruct iscsi_session *session;\n\tstruct iscsi_conn *conn;\n\tstruct iscsi_tcp_conn *tcp_conn;\n\tstruct iscsi_sw_tcp_conn *tcp_sw_conn;\n\tstruct sockaddr_in6 addr;\n\tstruct socket *sock;\n\tint rc;\n\n\tswitch (param) {\n\tcase ISCSI_HOST_PARAM_IPADDRESS:\n\t\tsession = tcp_sw_host->session;\n\t\tif (!session)\n\t\t\treturn -ENOTCONN;\n\n\t\tspin_lock_bh(&session->frwd_lock);\n\t\tconn = session->leadconn;\n\t\tif (!conn) {\n\t\t\tspin_unlock_bh(&session->frwd_lock);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\ttcp_conn = conn->dd_data;\n\t\ttcp_sw_conn = tcp_conn->dd_data;\n\t\t/*\n\t\t * The conn has been setup and bound, so just grab a ref\n\t\t * incase a destroy runs while we are in the net layer.\n\t\t */\n\t\tiscsi_get_conn(conn->cls_conn);\n\t\tspin_unlock_bh(&session->frwd_lock);\n\n\t\tmutex_lock(&tcp_sw_conn->sock_lock);\n\t\tsock = tcp_sw_conn->sock;\n\t\tif (!sock)\n\t\t\trc = -ENOTCONN;\n\t\telse\n\t\t\trc = kernel_getsockname(sock, (struct sockaddr *)&addr);\n\t\tmutex_unlock(&tcp_sw_conn->sock_lock);\n\t\tiscsi_put_conn(conn->cls_conn);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\n\t\treturn iscsi_conn_get_addr_param((struct sockaddr_storage *)\n\t\t\t\t\t\t &addr,\n\t\t\t\t\t\t (enum iscsi_param)param, buf);\n\tdefault:\n\t\treturn iscsi_host_get_param(shost, param, buf);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n \t\t\t\t       enum iscsi_host_param param, char *buf)\n {\n \tstruct iscsi_sw_tcp_host *tcp_sw_host = iscsi_host_priv(shost);\n-\tstruct iscsi_session *session = tcp_sw_host->session;\n+\tstruct iscsi_session *session;\n \tstruct iscsi_conn *conn;\n \tstruct iscsi_tcp_conn *tcp_conn;\n \tstruct iscsi_sw_tcp_conn *tcp_sw_conn;\n@@ -12,6 +12,7 @@\n \n \tswitch (param) {\n \tcase ISCSI_HOST_PARAM_IPADDRESS:\n+\t\tsession = tcp_sw_host->session;\n \t\tif (!session)\n \t\t\treturn -ENOTCONN;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct iscsi_session *session;",
                "\t\tsession = tcp_sw_host->session;"
            ],
            "deleted": [
                "\tstruct iscsi_session *session = tcp_sw_host->session;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in iscsi_sw_tcp_session_create in drivers/scsi/iscsi_tcp.c in SCSI sub-component in the Linux Kernel. In this flaw an attacker could leak kernel internal information.",
        "id": 3923
    },
    {
        "cve_id": "CVE-2017-16527",
        "code_before_change": "static void snd_usb_mixer_free(struct usb_mixer_interface *mixer)\n{\n\tkfree(mixer->id_elems);\n\tif (mixer->urb) {\n\t\tkfree(mixer->urb->transfer_buffer);\n\t\tusb_free_urb(mixer->urb);\n\t}\n\tusb_free_urb(mixer->rc_urb);\n\tkfree(mixer->rc_setup_packet);\n\tkfree(mixer);\n}",
        "code_after_change": "static void snd_usb_mixer_free(struct usb_mixer_interface *mixer)\n{\n\t/* kill pending URBs */\n\tsnd_usb_mixer_disconnect(mixer);\n\n\tkfree(mixer->id_elems);\n\tif (mixer->urb) {\n\t\tkfree(mixer->urb->transfer_buffer);\n\t\tusb_free_urb(mixer->urb);\n\t}\n\tusb_free_urb(mixer->rc_urb);\n\tkfree(mixer->rc_setup_packet);\n\tkfree(mixer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,8 @@\n static void snd_usb_mixer_free(struct usb_mixer_interface *mixer)\n {\n+\t/* kill pending URBs */\n+\tsnd_usb_mixer_disconnect(mixer);\n+\n \tkfree(mixer->id_elems);\n \tif (mixer->urb) {\n \t\tkfree(mixer->urb->transfer_buffer);",
        "function_modified_lines": {
            "added": [
                "\t/* kill pending URBs */",
                "\tsnd_usb_mixer_disconnect(mixer);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "sound/usb/mixer.c in the Linux kernel before 4.13.8 allows local users to cause a denial of service (snd_usb_mixer_interrupt use-after-free and system crash) or possibly have unspecified other impact via a crafted USB device.",
        "id": 1312
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static int __init l2tp_eth_init(void)\n{\n\tint err = 0;\n\n\terr = l2tp_nl_register_ops(L2TP_PWTYPE_ETH, &l2tp_eth_nl_cmd_ops);\n\tif (err)\n\t\tgoto out;\n\n\terr = register_pernet_device(&l2tp_eth_net_ops);\n\tif (err)\n\t\tgoto out_unreg;\n\n\tpr_info(\"L2TP ethernet pseudowire support (L2TPv3)\\n\");\n\n\treturn 0;\n\nout_unreg:\n\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\nout:\n\treturn err;\n}",
        "code_after_change": "static int __init l2tp_eth_init(void)\n{\n\tint err = 0;\n\n\terr = l2tp_nl_register_ops(L2TP_PWTYPE_ETH, &l2tp_eth_nl_cmd_ops);\n\tif (err)\n\t\tgoto err;\n\n\tpr_info(\"L2TP ethernet pseudowire support (L2TPv3)\\n\");\n\n\treturn 0;\n\nerr:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,18 +4,12 @@\n \n \terr = l2tp_nl_register_ops(L2TP_PWTYPE_ETH, &l2tp_eth_nl_cmd_ops);\n \tif (err)\n-\t\tgoto out;\n-\n-\terr = register_pernet_device(&l2tp_eth_net_ops);\n-\tif (err)\n-\t\tgoto out_unreg;\n+\t\tgoto err;\n \n \tpr_info(\"L2TP ethernet pseudowire support (L2TPv3)\\n\");\n \n \treturn 0;\n \n-out_unreg:\n-\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\n-out:\n+err:\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err;",
                "err:"
            ],
            "deleted": [
                "\t\tgoto out;",
                "",
                "\terr = register_pernet_device(&l2tp_eth_net_ops);",
                "\tif (err)",
                "\t\tgoto out_unreg;",
                "out_unreg:",
                "\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);",
                "out:"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173",
        "id": 2613
    },
    {
        "cve_id": "CVE-2023-2513",
        "code_before_change": "int ext4_xattr_ibody_find(struct inode *inode, struct ext4_xattr_info *i,\n\t\t\t  struct ext4_xattr_ibody_find *is)\n{\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_inode *raw_inode;\n\tint error;\n\n\tif (EXT4_I(inode)->i_extra_isize == 0)\n\t\treturn 0;\n\traw_inode = ext4_raw_inode(&is->iloc);\n\theader = IHDR(inode, raw_inode);\n\tis->s.base = is->s.first = IFIRST(header);\n\tis->s.here = is->s.first;\n\tis->s.end = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\terror = xattr_check_inode(inode, header, is->s.end);\n\t\tif (error)\n\t\t\treturn error;\n\t\t/* Find the named attribute. */\n\t\terror = xattr_find_entry(inode, &is->s.here, is->s.end,\n\t\t\t\t\t i->name_index, i->name, 0);\n\t\tif (error && error != -ENODATA)\n\t\t\treturn error;\n\t\tis->s.not_found = error;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ext4_xattr_ibody_find(struct inode *inode, struct ext4_xattr_info *i,\n\t\t\t  struct ext4_xattr_ibody_find *is)\n{\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_inode *raw_inode;\n\tint error;\n\n\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))\n\t\treturn 0;\n\n\traw_inode = ext4_raw_inode(&is->iloc);\n\theader = IHDR(inode, raw_inode);\n\tis->s.base = is->s.first = IFIRST(header);\n\tis->s.here = is->s.first;\n\tis->s.end = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\terror = xattr_check_inode(inode, header, is->s.end);\n\t\tif (error)\n\t\t\treturn error;\n\t\t/* Find the named attribute. */\n\t\terror = xattr_find_entry(inode, &is->s.here, is->s.end,\n\t\t\t\t\t i->name_index, i->name, 0);\n\t\tif (error && error != -ENODATA)\n\t\t\treturn error;\n\t\tis->s.not_found = error;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,9 @@\n \tstruct ext4_inode *raw_inode;\n \tint error;\n \n-\tif (EXT4_I(inode)->i_extra_isize == 0)\n+\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))\n \t\treturn 0;\n+\n \traw_inode = ext4_raw_inode(&is->iloc);\n \theader = IHDR(inode, raw_inode);\n \tis->s.base = is->s.first = IFIRST(header);",
        "function_modified_lines": {
            "added": [
                "\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))",
                ""
            ],
            "deleted": [
                "\tif (EXT4_I(inode)->i_extra_isize == 0)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in the Linux kernel's ext4 filesystem in the way it handled the extra inode size for extended attributes. This flaw could allow a privileged local user to cause a system crash or other undefined behaviors.",
        "id": 3960
    },
    {
        "cve_id": "CVE-2017-16939",
        "code_before_change": "static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n\tstruct xfrm_dump_info info;\n\n\tBUILD_BUG_ON(sizeof(struct xfrm_policy_walk) >\n\t\t     sizeof(cb->args) - sizeof(cb->args[0]));\n\n\tinfo.in_skb = cb->skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = NLM_F_MULTI;\n\n\tif (!cb->args[0]) {\n\t\tcb->args[0] = 1;\n\t\txfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);\n\t}\n\n\t(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);\n\n\treturn skb->len;\n}",
        "code_after_change": "static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;\n\tstruct xfrm_dump_info info;\n\n\tinfo.in_skb = cb->skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = NLM_F_MULTI;\n\n\t(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);\n\n\treturn skb->len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,22 +1,14 @@\n static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)\n {\n \tstruct net *net = sock_net(skb->sk);\n-\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n+\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;\n \tstruct xfrm_dump_info info;\n-\n-\tBUILD_BUG_ON(sizeof(struct xfrm_policy_walk) >\n-\t\t     sizeof(cb->args) - sizeof(cb->args[0]));\n \n \tinfo.in_skb = cb->skb;\n \tinfo.out_skb = skb;\n \tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n \tinfo.nlmsg_flags = NLM_F_MULTI;\n \n-\tif (!cb->args[0]) {\n-\t\tcb->args[0] = 1;\n-\t\txfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);\n-\t}\n-\n \t(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);\n \n \treturn skb->len;",
        "function_modified_lines": {
            "added": [
                "\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;"
            ],
            "deleted": [
                "\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];",
                "",
                "\tBUILD_BUG_ON(sizeof(struct xfrm_policy_walk) >",
                "\t\t     sizeof(cb->args) - sizeof(cb->args[0]));",
                "\tif (!cb->args[0]) {",
                "\t\tcb->args[0] = 1;",
                "\t\txfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The XFRM dump policy implementation in net/xfrm/xfrm_user.c in the Linux kernel before 4.13.11 allows local users to gain privileges or cause a denial of service (use-after-free) via a crafted SO_RCVBUF setsockopt system call in conjunction with XFRM_MSG_GETPOLICY Netlink messages.",
        "id": 1353
    },
    {
        "cve_id": "CVE-2017-15265",
        "code_before_change": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\tsprintf(new_port->name, \"port-%d\", num);\n\n\treturn new_port;\n}",
        "code_after_change": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\tsnd_use_lock_use(&new_port->use_lock);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\tsprintf(new_port->name, \"port-%d\", num);\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\n\treturn new_port;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,7 @@\n \tsnd_use_lock_init(&new_port->use_lock);\n \tport_subs_info_init(&new_port->c_src);\n \tport_subs_info_init(&new_port->c_dest);\n+\tsnd_use_lock_use(&new_port->use_lock);\n \n \tnum = port >= 0 ? port : 0;\n \tmutex_lock(&client->ports_mutex);\n@@ -40,9 +41,9 @@\n \tlist_add_tail(&new_port->list, &p->list);\n \tclient->num_ports++;\n \tnew_port->addr.port = num;\t/* store the port number in the port */\n+\tsprintf(new_port->name, \"port-%d\", num);\n \twrite_unlock_irqrestore(&client->ports_lock, flags);\n \tmutex_unlock(&client->ports_mutex);\n-\tsprintf(new_port->name, \"port-%d\", num);\n \n \treturn new_port;\n }",
        "function_modified_lines": {
            "added": [
                "\tsnd_use_lock_use(&new_port->use_lock);",
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ],
            "deleted": [
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ALSA subsystem in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via crafted /dev/snd/seq ioctl calls, related to sound/core/seq/seq_clientmgr.c and sound/core/seq/seq_ports.c.",
        "id": 1301
    },
    {
        "cve_id": "CVE-2023-0030",
        "code_before_change": "static void\nnvkm_vmm_put_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *prev, *next;\n\n\tif ((prev = node(vma, prev)) && !prev->used) {\n\t\trb_erase(&prev->tree, &vmm->free);\n\t\tlist_del(&prev->head);\n\t\tvma->addr  = prev->addr;\n\t\tvma->size += prev->size;\n\t\tkfree(prev);\n\t}\n\n\tif ((next = node(vma, next)) && !next->used) {\n\t\trb_erase(&next->tree, &vmm->free);\n\t\tlist_del(&next->head);\n\t\tvma->size += next->size;\n\t\tkfree(next);\n\t}\n\n\tnvkm_vmm_free_insert(vmm, vma);\n}",
        "code_after_change": "static void\nnvkm_vmm_put_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *prev, *next;\n\n\tif ((prev = node(vma, prev)) && !prev->used) {\n\t\tvma->addr  = prev->addr;\n\t\tvma->size += prev->size;\n\t\tnvkm_vmm_free_delete(vmm, prev);\n\t}\n\n\tif ((next = node(vma, next)) && !next->used) {\n\t\tvma->size += next->size;\n\t\tnvkm_vmm_free_delete(vmm, next);\n\t}\n\n\tnvkm_vmm_free_insert(vmm, vma);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,18 +4,14 @@\n \tstruct nvkm_vma *prev, *next;\n \n \tif ((prev = node(vma, prev)) && !prev->used) {\n-\t\trb_erase(&prev->tree, &vmm->free);\n-\t\tlist_del(&prev->head);\n \t\tvma->addr  = prev->addr;\n \t\tvma->size += prev->size;\n-\t\tkfree(prev);\n+\t\tnvkm_vmm_free_delete(vmm, prev);\n \t}\n \n \tif ((next = node(vma, next)) && !next->used) {\n-\t\trb_erase(&next->tree, &vmm->free);\n-\t\tlist_del(&next->head);\n \t\tvma->size += next->size;\n-\t\tkfree(next);\n+\t\tnvkm_vmm_free_delete(vmm, next);\n \t}\n \n \tnvkm_vmm_free_insert(vmm, vma);",
        "function_modified_lines": {
            "added": [
                "\t\tnvkm_vmm_free_delete(vmm, prev);",
                "\t\tnvkm_vmm_free_delete(vmm, next);"
            ],
            "deleted": [
                "\t\trb_erase(&prev->tree, &vmm->free);",
                "\t\tlist_del(&prev->head);",
                "\t\tkfree(prev);",
                "\t\trb_erase(&next->tree, &vmm->free);",
                "\t\tlist_del(&next->head);",
                "\t\tkfree(next);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s nouveau driver in how a user triggers a memory overflow that causes the nvkm_vma_tail function to fail. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3806
    },
    {
        "cve_id": "CVE-2020-8648",
        "code_before_change": "int paste_selection(struct tty_struct *tty)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tint\tpasted = 0;\n\tunsigned int count;\n\tstruct  tty_ldisc *ld;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint ret = 0;\n\n\tconsole_lock();\n\tpoke_blanked_console();\n\tconsole_unlock();\n\n\tld = tty_ldisc_ref_wait(tty);\n\tif (!ld)\n\t\treturn -EIO;\t/* ldisc was hung up */\n\ttty_buffer_lock_exclusive(&vc->port);\n\n\tadd_wait_queue(&vc->paste_wait, &wait);\n\twhile (sel_buffer && sel_buffer_lth > pasted) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_throttled(tty)) {\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t\tcount = sel_buffer_lth - pasted;\n\t\tcount = tty_ldisc_receive_buf(ld, sel_buffer + pasted, NULL,\n\t\t\t\t\t      count);\n\t\tpasted += count;\n\t}\n\tremove_wait_queue(&vc->paste_wait, &wait);\n\t__set_current_state(TASK_RUNNING);\n\n\ttty_buffer_unlock_exclusive(&vc->port);\n\ttty_ldisc_deref(ld);\n\treturn ret;\n}",
        "code_after_change": "int paste_selection(struct tty_struct *tty)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tint\tpasted = 0;\n\tunsigned int count;\n\tstruct  tty_ldisc *ld;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint ret = 0;\n\n\tconsole_lock();\n\tpoke_blanked_console();\n\tconsole_unlock();\n\n\tld = tty_ldisc_ref_wait(tty);\n\tif (!ld)\n\t\treturn -EIO;\t/* ldisc was hung up */\n\ttty_buffer_lock_exclusive(&vc->port);\n\n\tadd_wait_queue(&vc->paste_wait, &wait);\n\tmutex_lock(&sel_lock);\n\twhile (sel_buffer && sel_buffer_lth > pasted) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_throttled(tty)) {\n\t\t\tmutex_unlock(&sel_lock);\n\t\t\tschedule();\n\t\t\tmutex_lock(&sel_lock);\n\t\t\tcontinue;\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t\tcount = sel_buffer_lth - pasted;\n\t\tcount = tty_ldisc_receive_buf(ld, sel_buffer + pasted, NULL,\n\t\t\t\t\t      count);\n\t\tpasted += count;\n\t}\n\tmutex_unlock(&sel_lock);\n\tremove_wait_queue(&vc->paste_wait, &wait);\n\t__set_current_state(TASK_RUNNING);\n\n\ttty_buffer_unlock_exclusive(&vc->port);\n\ttty_ldisc_deref(ld);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \ttty_buffer_lock_exclusive(&vc->port);\n \n \tadd_wait_queue(&vc->paste_wait, &wait);\n+\tmutex_lock(&sel_lock);\n \twhile (sel_buffer && sel_buffer_lth > pasted) {\n \t\tset_current_state(TASK_INTERRUPTIBLE);\n \t\tif (signal_pending(current)) {\n@@ -24,7 +25,9 @@\n \t\t\tbreak;\n \t\t}\n \t\tif (tty_throttled(tty)) {\n+\t\t\tmutex_unlock(&sel_lock);\n \t\t\tschedule();\n+\t\t\tmutex_lock(&sel_lock);\n \t\t\tcontinue;\n \t\t}\n \t\t__set_current_state(TASK_RUNNING);\n@@ -33,6 +36,7 @@\n \t\t\t\t\t      count);\n \t\tpasted += count;\n \t}\n+\tmutex_unlock(&sel_lock);\n \tremove_wait_queue(&vc->paste_wait, &wait);\n \t__set_current_state(TASK_RUNNING);\n ",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&sel_lock);",
                "\t\t\tmutex_unlock(&sel_lock);",
                "\t\t\tmutex_lock(&sel_lock);",
                "\tmutex_unlock(&sel_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free vulnerability in the Linux kernel through 5.5.2 in the n_tty_receive_buf_common function in drivers/tty/n_tty.c.",
        "id": 2806
    },
    {
        "cve_id": "CVE-2023-1611",
        "code_before_change": "int btrfs_run_qgroups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret = 0;\n\n\tif (!fs_info->quota_root)\n\t\treturn ret;\n\n\tspin_lock(&fs_info->qgroup_lock);\n\twhile (!list_empty(&fs_info->dirty_qgroups)) {\n\t\tstruct btrfs_qgroup *qgroup;\n\t\tqgroup = list_first_entry(&fs_info->dirty_qgroups,\n\t\t\t\t\t  struct btrfs_qgroup, dirty);\n\t\tlist_del_init(&qgroup->dirty);\n\t\tspin_unlock(&fs_info->qgroup_lock);\n\t\tret = update_qgroup_info_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tret = update_qgroup_limit_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tspin_lock(&fs_info->qgroup_lock);\n\t}\n\tif (test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags))\n\t\tfs_info->qgroup_flags |= BTRFS_QGROUP_STATUS_FLAG_ON;\n\telse\n\t\tfs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_ON;\n\tspin_unlock(&fs_info->qgroup_lock);\n\n\tret = update_qgroup_status_item(trans);\n\tif (ret)\n\t\tqgroup_mark_inconsistent(fs_info);\n\n\treturn ret;\n}",
        "code_after_change": "int btrfs_run_qgroups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret = 0;\n\n\t/*\n\t * In case we are called from the qgroup assign ioctl, assert that we\n\t * are holding the qgroup_ioctl_lock, otherwise we can race with a quota\n\t * disable operation (ioctl) and access a freed quota root.\n\t */\n\tif (trans->transaction->state != TRANS_STATE_COMMIT_DOING)\n\t\tlockdep_assert_held(&fs_info->qgroup_ioctl_lock);\n\n\tif (!fs_info->quota_root)\n\t\treturn ret;\n\n\tspin_lock(&fs_info->qgroup_lock);\n\twhile (!list_empty(&fs_info->dirty_qgroups)) {\n\t\tstruct btrfs_qgroup *qgroup;\n\t\tqgroup = list_first_entry(&fs_info->dirty_qgroups,\n\t\t\t\t\t  struct btrfs_qgroup, dirty);\n\t\tlist_del_init(&qgroup->dirty);\n\t\tspin_unlock(&fs_info->qgroup_lock);\n\t\tret = update_qgroup_info_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tret = update_qgroup_limit_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tspin_lock(&fs_info->qgroup_lock);\n\t}\n\tif (test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags))\n\t\tfs_info->qgroup_flags |= BTRFS_QGROUP_STATUS_FLAG_ON;\n\telse\n\t\tfs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_ON;\n\tspin_unlock(&fs_info->qgroup_lock);\n\n\tret = update_qgroup_status_item(trans);\n\tif (ret)\n\t\tqgroup_mark_inconsistent(fs_info);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,14 @@\n {\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n \tint ret = 0;\n+\n+\t/*\n+\t * In case we are called from the qgroup assign ioctl, assert that we\n+\t * are holding the qgroup_ioctl_lock, otherwise we can race with a quota\n+\t * disable operation (ioctl) and access a freed quota root.\n+\t */\n+\tif (trans->transaction->state != TRANS_STATE_COMMIT_DOING)\n+\t\tlockdep_assert_held(&fs_info->qgroup_ioctl_lock);\n \n \tif (!fs_info->quota_root)\n \t\treturn ret;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * In case we are called from the qgroup assign ioctl, assert that we",
                "\t * are holding the qgroup_ioctl_lock, otherwise we can race with a quota",
                "\t * disable operation (ioctl) and access a freed quota root.",
                "\t */",
                "\tif (trans->transaction->state != TRANS_STATE_COMMIT_DOING)",
                "\t\tlockdep_assert_held(&fs_info->qgroup_ioctl_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in btrfs_search_slot in fs/btrfs/ctree.c in btrfs in the Linux Kernel.This flaw allows an attacker to crash the system and possibly cause a kernel information lea",
        "id": 3875
    },
    {
        "cve_id": "CVE-2021-3347",
        "code_before_change": "static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tint ret = 0;\n\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n\t\treturn ret ? ret : locked;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current) {\n\t\tret = fixup_pi_state_owner(uaddr, q, NULL);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current) {\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current)\n\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current) {\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,5 @@\n static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n {\n-\tint ret = 0;\n-\n \tif (locked) {\n \t\t/*\n \t\t * Got the lock. We might not be the anticipated owner if we\n@@ -12,8 +10,8 @@\n \t\t * stable state, anything else needs more attention.\n \t\t */\n \t\tif (q->pi_state->owner != current)\n-\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n-\t\treturn ret ? ret : locked;\n+\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n+\t\treturn 1;\n \t}\n \n \t/*\n@@ -24,10 +22,8 @@\n \t * Another speculative read; pi_state->owner == current is unstable\n \t * but needs our attention.\n \t */\n-\tif (q->pi_state->owner == current) {\n-\t\tret = fixup_pi_state_owner(uaddr, q, NULL);\n-\t\treturn ret;\n-\t}\n+\tif (q->pi_state->owner == current)\n+\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n \n \t/*\n \t * Paranoia check. If we did not take the lock, then we should not be\n@@ -40,5 +36,5 @@\n \t\t\t\tq->pi_state->owner);\n \t}\n \n-\treturn ret;\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\treturn fixup_pi_state_owner(uaddr, q, current);",
                "\t\treturn 1;",
                "\tif (q->pi_state->owner == current)",
                "\t\treturn fixup_pi_state_owner(uaddr, q, NULL);",
                "\treturn 0;"
            ],
            "deleted": [
                "\tint ret = 0;",
                "",
                "\t\t\tret = fixup_pi_state_owner(uaddr, q, current);",
                "\t\treturn ret ? ret : locked;",
                "\tif (q->pi_state->owner == current) {",
                "\t\tret = fixup_pi_state_owner(uaddr, q, NULL);",
                "\t\treturn ret;",
                "\t}",
                "\treturn ret;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.10.11. PI futexes have a kernel stack use-after-free during fault handling, allowing local users to execute code in the kernel, aka CID-34b1a1ce1458.",
        "id": 2978
    },
    {
        "cve_id": "CVE-2019-19813",
        "code_before_change": "struct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *sub_root = root;\n\tstruct btrfs_key location;\n\tint index;\n\tint ret = 0;\n\n\tif (dentry->d_name.len > BTRFS_NAME_LEN)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tret = btrfs_inode_by_name(dir, dentry, &location);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tif (location.type == BTRFS_INODE_ITEM_KEY) {\n\t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n\t\treturn inode;\n\t}\n\n\tindex = srcu_read_lock(&fs_info->subvol_srcu);\n\tret = fixup_tree_root_location(fs_info, dir, dentry,\n\t\t\t\t       &location, &sub_root);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT)\n\t\t\tinode = ERR_PTR(ret);\n\t\telse\n\t\t\tinode = new_simple_dir(dir->i_sb, &location, sub_root);\n\t} else {\n\t\tinode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n\t}\n\tsrcu_read_unlock(&fs_info->subvol_srcu, index);\n\n\tif (!IS_ERR(inode) && root != sub_root) {\n\t\tdown_read(&fs_info->cleanup_work_sem);\n\t\tif (!sb_rdonly(inode->i_sb))\n\t\t\tret = btrfs_orphan_cleanup(sub_root);\n\t\tup_read(&fs_info->cleanup_work_sem);\n\t\tif (ret) {\n\t\t\tiput(inode);\n\t\t\tinode = ERR_PTR(ret);\n\t\t}\n\t}\n\n\treturn inode;\n}",
        "code_after_change": "struct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *sub_root = root;\n\tstruct btrfs_key location;\n\tu8 di_type = 0;\n\tint index;\n\tint ret = 0;\n\n\tif (dentry->d_name.len > BTRFS_NAME_LEN)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tret = btrfs_inode_by_name(dir, dentry, &location, &di_type);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tif (location.type == BTRFS_INODE_ITEM_KEY) {\n\t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n\t\tif (IS_ERR(inode))\n\t\t\treturn inode;\n\n\t\t/* Do extra check against inode mode with di_type */\n\t\tif (btrfs_inode_type(inode) != di_type) {\n\t\t\tbtrfs_crit(fs_info,\n\"inode mode mismatch with dir: inode mode=0%o btrfs type=%u dir type=%u\",\n\t\t\t\t  inode->i_mode, btrfs_inode_type(inode),\n\t\t\t\t  di_type);\n\t\t\tiput(inode);\n\t\t\treturn ERR_PTR(-EUCLEAN);\n\t\t}\n\t\treturn inode;\n\t}\n\n\tindex = srcu_read_lock(&fs_info->subvol_srcu);\n\tret = fixup_tree_root_location(fs_info, dir, dentry,\n\t\t\t\t       &location, &sub_root);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT)\n\t\t\tinode = ERR_PTR(ret);\n\t\telse\n\t\t\tinode = new_simple_dir(dir->i_sb, &location, sub_root);\n\t} else {\n\t\tinode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n\t}\n\tsrcu_read_unlock(&fs_info->subvol_srcu, index);\n\n\tif (!IS_ERR(inode) && root != sub_root) {\n\t\tdown_read(&fs_info->cleanup_work_sem);\n\t\tif (!sb_rdonly(inode->i_sb))\n\t\t\tret = btrfs_orphan_cleanup(sub_root);\n\t\tup_read(&fs_info->cleanup_work_sem);\n\t\tif (ret) {\n\t\t\tiput(inode);\n\t\t\tinode = ERR_PTR(ret);\n\t\t}\n\t}\n\n\treturn inode;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,18 +5,31 @@\n \tstruct btrfs_root *root = BTRFS_I(dir)->root;\n \tstruct btrfs_root *sub_root = root;\n \tstruct btrfs_key location;\n+\tu8 di_type = 0;\n \tint index;\n \tint ret = 0;\n \n \tif (dentry->d_name.len > BTRFS_NAME_LEN)\n \t\treturn ERR_PTR(-ENAMETOOLONG);\n \n-\tret = btrfs_inode_by_name(dir, dentry, &location);\n+\tret = btrfs_inode_by_name(dir, dentry, &location, &di_type);\n \tif (ret < 0)\n \t\treturn ERR_PTR(ret);\n \n \tif (location.type == BTRFS_INODE_ITEM_KEY) {\n \t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n+\t\tif (IS_ERR(inode))\n+\t\t\treturn inode;\n+\n+\t\t/* Do extra check against inode mode with di_type */\n+\t\tif (btrfs_inode_type(inode) != di_type) {\n+\t\t\tbtrfs_crit(fs_info,\n+\"inode mode mismatch with dir: inode mode=0%o btrfs type=%u dir type=%u\",\n+\t\t\t\t  inode->i_mode, btrfs_inode_type(inode),\n+\t\t\t\t  di_type);\n+\t\t\tiput(inode);\n+\t\t\treturn ERR_PTR(-EUCLEAN);\n+\t\t}\n \t\treturn inode;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tu8 di_type = 0;",
                "\tret = btrfs_inode_by_name(dir, dentry, &location, &di_type);",
                "\t\tif (IS_ERR(inode))",
                "\t\t\treturn inode;",
                "",
                "\t\t/* Do extra check against inode mode with di_type */",
                "\t\tif (btrfs_inode_type(inode) != di_type) {",
                "\t\t\tbtrfs_crit(fs_info,",
                "\"inode mode mismatch with dir: inode mode=0%o btrfs type=%u dir type=%u\",",
                "\t\t\t\t  inode->i_mode, btrfs_inode_type(inode),",
                "\t\t\t\t  di_type);",
                "\t\t\tiput(inode);",
                "\t\t\treturn ERR_PTR(-EUCLEAN);",
                "\t\t}"
            ],
            "deleted": [
                "\tret = btrfs_inode_by_name(dir, dentry, &location);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.0.21, mounting a crafted btrfs filesystem image, performing some operations, and then making a syncfs system call can lead to a use-after-free in __mutex_lock in kernel/locking/mutex.c. This is related to mutex_can_spin_on_owner in kernel/locking/mutex.c, __btrfs_qgroup_free_meta in fs/btrfs/qgroup.c, and btrfs_insert_delayed_items in fs/btrfs/delayed-inode.c.",
        "id": 2245
    },
    {
        "cve_id": "CVE-2017-6346",
        "code_before_change": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tf = po->fanout;\n\tif (!f)\n\t\treturn;\n\n\tmutex_lock(&fanout_mutex);\n\tpo->fanout = NULL;\n\n\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\tlist_del(&f->list);\n\t\tdev_remove_pack(&f->prot_hook);\n\t\tfanout_release_data(f);\n\t\tkfree(f);\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\tif (po->rollover)\n\t\tkfree_rcu(po->rollover, rcu);\n}",
        "code_after_change": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\t\tlist_del(&f->list);\n\t\t\tdev_remove_pack(&f->prot_hook);\n\t\t\tfanout_release_data(f);\n\t\t\tkfree(f);\n\t\t}\n\n\t\tif (po->rollover)\n\t\t\tkfree_rcu(po->rollover, rcu);\n\t}\n\tmutex_unlock(&fanout_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,21 +3,20 @@\n \tstruct packet_sock *po = pkt_sk(sk);\n \tstruct packet_fanout *f;\n \n+\tmutex_lock(&fanout_mutex);\n \tf = po->fanout;\n-\tif (!f)\n-\t\treturn;\n+\tif (f) {\n+\t\tpo->fanout = NULL;\n \n-\tmutex_lock(&fanout_mutex);\n-\tpo->fanout = NULL;\n+\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n+\t\t\tlist_del(&f->list);\n+\t\t\tdev_remove_pack(&f->prot_hook);\n+\t\t\tfanout_release_data(f);\n+\t\t\tkfree(f);\n+\t\t}\n \n-\tif (atomic_dec_and_test(&f->sk_ref)) {\n-\t\tlist_del(&f->list);\n-\t\tdev_remove_pack(&f->prot_hook);\n-\t\tfanout_release_data(f);\n-\t\tkfree(f);\n+\t\tif (po->rollover)\n+\t\t\tkfree_rcu(po->rollover, rcu);\n \t}\n \tmutex_unlock(&fanout_mutex);\n-\n-\tif (po->rollover)\n-\t\tkfree_rcu(po->rollover, rcu);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&fanout_mutex);",
                "\tif (f) {",
                "\t\tpo->fanout = NULL;",
                "\t\tif (atomic_dec_and_test(&f->sk_ref)) {",
                "\t\t\tlist_del(&f->list);",
                "\t\t\tdev_remove_pack(&f->prot_hook);",
                "\t\t\tfanout_release_data(f);",
                "\t\t\tkfree(f);",
                "\t\t}",
                "\t\tif (po->rollover)",
                "\t\t\tkfree_rcu(po->rollover, rcu);"
            ],
            "deleted": [
                "\tif (!f)",
                "\t\treturn;",
                "\tmutex_lock(&fanout_mutex);",
                "\tpo->fanout = NULL;",
                "\tif (atomic_dec_and_test(&f->sk_ref)) {",
                "\t\tlist_del(&f->list);",
                "\t\tdev_remove_pack(&f->prot_hook);",
                "\t\tfanout_release_data(f);",
                "\t\tkfree(f);",
                "",
                "\tif (po->rollover)",
                "\t\tkfree_rcu(po->rollover, rcu);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel before 4.9.13 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a multithreaded application that makes PACKET_FANOUT setsockopt system calls.",
        "id": 1483
    },
    {
        "cve_id": "CVE-2019-19319",
        "code_before_change": "static int __check_block_validity(struct inode *inode, const char *func,\n\t\t\t\tunsigned int line,\n\t\t\t\tstruct ext4_map_blocks *map)\n{\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n\t\t\t\t   map->m_len)) {\n\t\text4_error_inode(inode, func, line, map->m_pblk,\n\t\t\t\t \"lblock %lu mapped to illegal pblock %llu \"\n\t\t\t\t \"(length %d)\", (unsigned long) map->m_lblk,\n\t\t\t\t map->m_pblk, map->m_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int __check_block_validity(struct inode *inode, const char *func,\n\t\t\t\tunsigned int line,\n\t\t\t\tstruct ext4_map_blocks *map)\n{\n\tif (ext4_has_feature_journal(inode->i_sb) &&\n\t    (inode->i_ino ==\n\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))\n\t\treturn 0;\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n\t\t\t\t   map->m_len)) {\n\t\text4_error_inode(inode, func, line, map->m_pblk,\n\t\t\t\t \"lblock %lu mapped to illegal pblock %llu \"\n\t\t\t\t \"(length %d)\", (unsigned long) map->m_lblk,\n\t\t\t\t map->m_pblk, map->m_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,10 @@\n \t\t\t\tunsigned int line,\n \t\t\t\tstruct ext4_map_blocks *map)\n {\n+\tif (ext4_has_feature_journal(inode->i_sb) &&\n+\t    (inode->i_ino ==\n+\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))\n+\t\treturn 0;\n \tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n \t\t\t\t   map->m_len)) {\n \t\text4_error_inode(inode, func, line, map->m_pblk,",
        "function_modified_lines": {
            "added": [
                "\tif (ext4_has_feature_journal(inode->i_sb) &&",
                "\t    (inode->i_ino ==",
                "\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))",
                "\t\treturn 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.2, a setxattr operation, after a mount of a crafted ext4 image, can cause a slab-out-of-bounds write access because of an ext4_xattr_set_entry use-after-free in fs/ext4/xattr.c when a large old_size value is used in a memset call, aka CID-345c0dbf3a30.",
        "id": 2189
    },
    {
        "cve_id": "CVE-2020-25656",
        "code_before_change": "static void k_fn(struct vc_data *vc, unsigned char value, char up_flag)\n{\n\tif (up_flag)\n\t\treturn;\n\n\tif ((unsigned)value < ARRAY_SIZE(func_table)) {\n\t\tif (func_table[value])\n\t\t\tputs_queue(vc, func_table[value]);\n\t} else\n\t\tpr_err(\"k_fn called with value=%d\\n\", value);\n}",
        "code_after_change": "static void k_fn(struct vc_data *vc, unsigned char value, char up_flag)\n{\n\tif (up_flag)\n\t\treturn;\n\n\tif ((unsigned)value < ARRAY_SIZE(func_table)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&func_buf_lock, flags);\n\t\tif (func_table[value])\n\t\t\tputs_queue(vc, func_table[value]);\n\t\tspin_unlock_irqrestore(&func_buf_lock, flags);\n\n\t} else\n\t\tpr_err(\"k_fn called with value=%d\\n\", value);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,8 +4,13 @@\n \t\treturn;\n \n \tif ((unsigned)value < ARRAY_SIZE(func_table)) {\n+\t\tunsigned long flags;\n+\n+\t\tspin_lock_irqsave(&func_buf_lock, flags);\n \t\tif (func_table[value])\n \t\t\tputs_queue(vc, func_table[value]);\n+\t\tspin_unlock_irqrestore(&func_buf_lock, flags);\n+\n \t} else\n \t\tpr_err(\"k_fn called with value=%d\\n\", value);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tunsigned long flags;",
                "",
                "\t\tspin_lock_irqsave(&func_buf_lock, flags);",
                "\t\tspin_unlock_irqrestore(&func_buf_lock, flags);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A use-after-free was found in the way the console subsystem was using ioctls KDGKBSENT and KDSKBSENT. A local user could use this flaw to get read memory access out of bounds. The highest threat from this vulnerability is to data confidentiality.",
        "id": 2594
    },
    {
        "cve_id": "CVE-2023-0266",
        "code_before_change": "static int snd_ctl_elem_read(struct snd_card *card,\n\t\t\t     struct snd_ctl_elem_value *control)\n{\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int index_offset;\n\tstruct snd_ctl_elem_info info;\n\tconst u32 pattern = 0xdeadbeef;\n\tint ret;\n\n\tkctl = snd_ctl_find_id(card, &control->id);\n\tif (kctl == NULL)\n\t\treturn -ENOENT;\n\n\tindex_offset = snd_ctl_get_ioff(kctl, &control->id);\n\tvd = &kctl->vd[index_offset];\n\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL)\n\t\treturn -EPERM;\n\n\tsnd_ctl_build_ioff(&control->id, kctl, index_offset);\n\n#ifdef CONFIG_SND_CTL_DEBUG\n\t/* info is needed only for validation */\n\tmemset(&info, 0, sizeof(info));\n\tinfo.id = control->id;\n\tret = __snd_ctl_elem_info(card, kctl, &info, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n#endif\n\n\tif (!snd_ctl_skip_validation(&info))\n\t\tfill_remaining_elem_value(control, &info, pattern);\n\tret = snd_power_ref_and_wait(card);\n\tif (!ret)\n\t\tret = kctl->get(kctl, control);\n\tsnd_power_unref(card);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (!snd_ctl_skip_validation(&info) &&\n\t    sanity_check_elem_value(card, control, &info, pattern) < 0) {\n\t\tdev_err(card->dev,\n\t\t\t\"control %i:%i:%i:%s:%i: access overflow\\n\",\n\t\t\tcontrol->id.iface, control->id.device,\n\t\t\tcontrol->id.subdevice, control->id.name,\n\t\t\tcontrol->id.index);\n\t\treturn -EINVAL;\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int snd_ctl_elem_read(struct snd_card *card,\n\t\t\t     struct snd_ctl_elem_value *control)\n{\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int index_offset;\n\tstruct snd_ctl_elem_info info;\n\tconst u32 pattern = 0xdeadbeef;\n\tint ret;\n\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_id(card, &control->id);\n\tif (kctl == NULL) {\n\t\tret = -ENOENT;\n\t\tgoto unlock;\n\t}\n\n\tindex_offset = snd_ctl_get_ioff(kctl, &control->id);\n\tvd = &kctl->vd[index_offset];\n\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL) {\n\t\tret = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\tsnd_ctl_build_ioff(&control->id, kctl, index_offset);\n\n#ifdef CONFIG_SND_CTL_DEBUG\n\t/* info is needed only for validation */\n\tmemset(&info, 0, sizeof(info));\n\tinfo.id = control->id;\n\tret = __snd_ctl_elem_info(card, kctl, &info, NULL);\n\tif (ret < 0)\n\t\tgoto unlock;\n#endif\n\n\tif (!snd_ctl_skip_validation(&info))\n\t\tfill_remaining_elem_value(control, &info, pattern);\n\tret = snd_power_ref_and_wait(card);\n\tif (!ret)\n\t\tret = kctl->get(kctl, control);\n\tsnd_power_unref(card);\n\tif (ret < 0)\n\t\tgoto unlock;\n\tif (!snd_ctl_skip_validation(&info) &&\n\t    sanity_check_elem_value(card, control, &info, pattern) < 0) {\n\t\tdev_err(card->dev,\n\t\t\t\"control %i:%i:%i:%s:%i: access overflow\\n\",\n\t\t\tcontrol->id.iface, control->id.device,\n\t\t\tcontrol->id.subdevice, control->id.name,\n\t\t\tcontrol->id.index);\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\nunlock:\n\tup_read(&card->controls_rwsem);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,14 +8,19 @@\n \tconst u32 pattern = 0xdeadbeef;\n \tint ret;\n \n+\tdown_read(&card->controls_rwsem);\n \tkctl = snd_ctl_find_id(card, &control->id);\n-\tif (kctl == NULL)\n-\t\treturn -ENOENT;\n+\tif (kctl == NULL) {\n+\t\tret = -ENOENT;\n+\t\tgoto unlock;\n+\t}\n \n \tindex_offset = snd_ctl_get_ioff(kctl, &control->id);\n \tvd = &kctl->vd[index_offset];\n-\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL)\n-\t\treturn -EPERM;\n+\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL) {\n+\t\tret = -EPERM;\n+\t\tgoto unlock;\n+\t}\n \n \tsnd_ctl_build_ioff(&control->id, kctl, index_offset);\n \n@@ -25,7 +30,7 @@\n \tinfo.id = control->id;\n \tret = __snd_ctl_elem_info(card, kctl, &info, NULL);\n \tif (ret < 0)\n-\t\treturn ret;\n+\t\tgoto unlock;\n #endif\n \n \tif (!snd_ctl_skip_validation(&info))\n@@ -35,7 +40,7 @@\n \t\tret = kctl->get(kctl, control);\n \tsnd_power_unref(card);\n \tif (ret < 0)\n-\t\treturn ret;\n+\t\tgoto unlock;\n \tif (!snd_ctl_skip_validation(&info) &&\n \t    sanity_check_elem_value(card, control, &info, pattern) < 0) {\n \t\tdev_err(card->dev,\n@@ -43,7 +48,10 @@\n \t\t\tcontrol->id.iface, control->id.device,\n \t\t\tcontrol->id.subdevice, control->id.name,\n \t\t\tcontrol->id.index);\n-\t\treturn -EINVAL;\n+\t\tret = -EINVAL;\n+\t\tgoto unlock;\n \t}\n+unlock:\n+\tup_read(&card->controls_rwsem);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tdown_read(&card->controls_rwsem);",
                "\tif (kctl == NULL) {",
                "\t\tret = -ENOENT;",
                "\t\tgoto unlock;",
                "\t}",
                "\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL) {",
                "\t\tret = -EPERM;",
                "\t\tgoto unlock;",
                "\t}",
                "\t\tgoto unlock;",
                "\t\tgoto unlock;",
                "\t\tret = -EINVAL;",
                "\t\tgoto unlock;",
                "unlock:",
                "\tup_read(&card->controls_rwsem);"
            ],
            "deleted": [
                "\tif (kctl == NULL)",
                "\t\treturn -ENOENT;",
                "\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL)",
                "\t\treturn -EPERM;",
                "\t\treturn ret;",
                "\t\treturn ret;",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free vulnerability exists in the ALSA PCM package in the Linux Kernel.\u00a0SNDRV_CTL_IOCTL_ELEM_{READ|WRITE}32 is missing locks that can be used in a use-after-free that can result in a priviledge escalation to gain ring0 access from the system user. We recommend upgrading past commit\u00a056b88b50565cd8b946a2d00b0c83927b7ebb055e\n",
        "id": 3821
    },
    {
        "cve_id": "CVE-2018-14734",
        "code_before_change": "static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)\n{\n\tstruct ucma_multicast *mc;\n\n\tmc = kzalloc(sizeof(*mc), GFP_KERNEL);\n\tif (!mc)\n\t\treturn NULL;\n\n\tmutex_lock(&mut);\n\tmc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);\n\tmutex_unlock(&mut);\n\tif (mc->id < 0)\n\t\tgoto error;\n\n\tmc->ctx = ctx;\n\tlist_add_tail(&mc->list, &ctx->mc_list);\n\treturn mc;\n\nerror:\n\tkfree(mc);\n\treturn NULL;\n}",
        "code_after_change": "static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)\n{\n\tstruct ucma_multicast *mc;\n\n\tmc = kzalloc(sizeof(*mc), GFP_KERNEL);\n\tif (!mc)\n\t\treturn NULL;\n\n\tmutex_lock(&mut);\n\tmc->id = idr_alloc(&multicast_idr, NULL, 0, 0, GFP_KERNEL);\n\tmutex_unlock(&mut);\n\tif (mc->id < 0)\n\t\tgoto error;\n\n\tmc->ctx = ctx;\n\tlist_add_tail(&mc->list, &ctx->mc_list);\n\treturn mc;\n\nerror:\n\tkfree(mc);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \t\treturn NULL;\n \n \tmutex_lock(&mut);\n-\tmc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);\n+\tmc->id = idr_alloc(&multicast_idr, NULL, 0, 0, GFP_KERNEL);\n \tmutex_unlock(&mut);\n \tif (mc->id < 0)\n \t\tgoto error;",
        "function_modified_lines": {
            "added": [
                "\tmc->id = idr_alloc(&multicast_idr, NULL, 0, 0, GFP_KERNEL);"
            ],
            "deleted": [
                "\tmc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "drivers/infiniband/core/ucma.c in the Linux kernel through 4.17.11 allows ucma_leave_multicast to access a certain data structure after a cleanup step in ucma_process_join, which allows attackers to cause a denial of service (use-after-free).",
        "id": 1706
    },
    {
        "cve_id": "CVE-2023-0461",
        "code_before_change": "int inet_csk_listen_start(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err;\n\n\treqsk_queue_alloc(&icsk->icsk_accept_queue);\n\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\tif (sk->sk_txrehash == SOCK_TXREHASH_DEFAULT)\n\t\tsk->sk_txrehash = READ_ONCE(sock_net(sk)->core.sysctl_txrehash);\n\n\t/* There is race window here: we announce ourselves listening,\n\t * but this transition is still not validated by get_port().\n\t * It is OK, because this socket enters to hash table only\n\t * after validation is complete.\n\t */\n\tinet_sk_state_store(sk, TCP_LISTEN);\n\terr = sk->sk_prot->get_port(sk, inet->inet_num);\n\tif (!err) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\terr = sk->sk_prot->hash(sk);\n\n\t\tif (likely(!err))\n\t\t\treturn 0;\n\t}\n\n\tinet_sk_set_state(sk, TCP_CLOSE);\n\treturn err;\n}",
        "code_after_change": "int inet_csk_listen_start(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err;\n\n\terr = inet_ulp_can_listen(sk);\n\tif (unlikely(err))\n\t\treturn err;\n\n\treqsk_queue_alloc(&icsk->icsk_accept_queue);\n\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\tif (sk->sk_txrehash == SOCK_TXREHASH_DEFAULT)\n\t\tsk->sk_txrehash = READ_ONCE(sock_net(sk)->core.sysctl_txrehash);\n\n\t/* There is race window here: we announce ourselves listening,\n\t * but this transition is still not validated by get_port().\n\t * It is OK, because this socket enters to hash table only\n\t * after validation is complete.\n\t */\n\tinet_sk_state_store(sk, TCP_LISTEN);\n\terr = sk->sk_prot->get_port(sk, inet->inet_num);\n\tif (!err) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\terr = sk->sk_prot->hash(sk);\n\n\t\tif (likely(!err))\n\t\t\treturn 0;\n\t}\n\n\tinet_sk_set_state(sk, TCP_CLOSE);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,10 @@\n \tstruct inet_connection_sock *icsk = inet_csk(sk);\n \tstruct inet_sock *inet = inet_sk(sk);\n \tint err;\n+\n+\terr = inet_ulp_can_listen(sk);\n+\tif (unlikely(err))\n+\t\treturn err;\n \n \treqsk_queue_alloc(&icsk->icsk_accept_queue);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\terr = inet_ulp_can_listen(sk);",
                "\tif (unlikely(err))",
                "\t\treturn err;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free vulnerability in the Linux Kernel which can be exploited to achieve local privilege escalation. To reach the vulnerability kernel configuration flag CONFIG_TLS\u00a0or CONFIG_XFRM_ESPINTCP\u00a0has to be configured, but the operation does not require any privilege.\n\nThere is a use-after-free bug of icsk_ulp_data\u00a0of a struct inet_connection_sock.\n\nWhen CONFIG_TLS\u00a0is enabled, user can install a tls context (struct tls_context) on a connected tcp socket. The context is not cleared if this socket is disconnected and reused as a listener. If a new socket is created from the listener, the context is inherited and vulnerable.\n\nThe setsockopt\u00a0TCP_ULP\u00a0operation does not require any privilege.\n\nWe recommend upgrading past commit\u00a02c02d41d71f90a5168391b6a5f2954112ba2307c",
        "id": 3828
    },
    {
        "cve_id": "CVE-2022-42720",
        "code_before_change": "struct cfg80211_internal_bss *\ncfg80211_bss_update(struct cfg80211_registered_device *rdev,\n\t\t    struct cfg80211_internal_bss *tmp,\n\t\t    bool signal_valid, unsigned long ts)\n{\n\tstruct cfg80211_internal_bss *found = NULL;\n\n\tif (WARN_ON(!tmp->pub.channel))\n\t\treturn NULL;\n\n\ttmp->ts = ts;\n\n\tspin_lock_bh(&rdev->bss_lock);\n\n\tif (WARN_ON(!rcu_access_pointer(tmp->pub.ies))) {\n\t\tspin_unlock_bh(&rdev->bss_lock);\n\t\treturn NULL;\n\t}\n\n\tfound = rb_find_bss(rdev, tmp, BSS_CMP_REGULAR);\n\n\tif (found) {\n\t\tif (!cfg80211_update_known_bss(rdev, found, tmp, signal_valid))\n\t\t\tgoto drop;\n\t} else {\n\t\tstruct cfg80211_internal_bss *new;\n\t\tstruct cfg80211_internal_bss *hidden;\n\t\tstruct cfg80211_bss_ies *ies;\n\n\t\t/*\n\t\t * create a copy -- the \"res\" variable that is passed in\n\t\t * is allocated on the stack since it's not needed in the\n\t\t * more common case of an update\n\t\t */\n\t\tnew = kzalloc(sizeof(*new) + rdev->wiphy.bss_priv_size,\n\t\t\t      GFP_ATOMIC);\n\t\tif (!new) {\n\t\t\ties = (void *)rcu_dereference(tmp->pub.beacon_ies);\n\t\t\tif (ies)\n\t\t\t\tkfree_rcu(ies, rcu_head);\n\t\t\ties = (void *)rcu_dereference(tmp->pub.proberesp_ies);\n\t\t\tif (ies)\n\t\t\t\tkfree_rcu(ies, rcu_head);\n\t\t\tgoto drop;\n\t\t}\n\t\tmemcpy(new, tmp, sizeof(*new));\n\t\tnew->refcount = 1;\n\t\tINIT_LIST_HEAD(&new->hidden_list);\n\t\tINIT_LIST_HEAD(&new->pub.nontrans_list);\n\n\t\tif (rcu_access_pointer(tmp->pub.proberesp_ies)) {\n\t\t\thidden = rb_find_bss(rdev, tmp, BSS_CMP_HIDE_ZLEN);\n\t\t\tif (!hidden)\n\t\t\t\thidden = rb_find_bss(rdev, tmp,\n\t\t\t\t\t\t     BSS_CMP_HIDE_NUL);\n\t\t\tif (hidden) {\n\t\t\t\tnew->pub.hidden_beacon_bss = &hidden->pub;\n\t\t\t\tlist_add(&new->hidden_list,\n\t\t\t\t\t &hidden->hidden_list);\n\t\t\t\thidden->refcount++;\n\t\t\t\trcu_assign_pointer(new->pub.beacon_ies,\n\t\t\t\t\t\t   hidden->pub.beacon_ies);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Ok so we found a beacon, and don't have an entry. If\n\t\t\t * it's a beacon with hidden SSID, we might be in for an\n\t\t\t * expensive search for any probe responses that should\n\t\t\t * be grouped with this beacon for updates ...\n\t\t\t */\n\t\t\tif (!cfg80211_combine_bsses(rdev, new)) {\n\t\t\t\tbss_ref_put(rdev, new);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\n\t\tif (rdev->bss_entries >= bss_entries_limit &&\n\t\t    !cfg80211_bss_expire_oldest(rdev)) {\n\t\t\tbss_ref_put(rdev, new);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* This must be before the call to bss_ref_get */\n\t\tif (tmp->pub.transmitted_bss) {\n\t\t\tstruct cfg80211_internal_bss *pbss =\n\t\t\t\tcontainer_of(tmp->pub.transmitted_bss,\n\t\t\t\t\t     struct cfg80211_internal_bss,\n\t\t\t\t\t     pub);\n\n\t\t\tnew->pub.transmitted_bss = tmp->pub.transmitted_bss;\n\t\t\tbss_ref_get(rdev, pbss);\n\t\t}\n\n\t\tlist_add_tail(&new->list, &rdev->bss_list);\n\t\trdev->bss_entries++;\n\t\trb_insert_bss(rdev, new);\n\t\tfound = new;\n\t}\n\n\trdev->bss_generation++;\n\tbss_ref_get(rdev, found);\n\tspin_unlock_bh(&rdev->bss_lock);\n\n\treturn found;\n drop:\n\tspin_unlock_bh(&rdev->bss_lock);\n\treturn NULL;\n}",
        "code_after_change": "struct cfg80211_internal_bss *\ncfg80211_bss_update(struct cfg80211_registered_device *rdev,\n\t\t    struct cfg80211_internal_bss *tmp,\n\t\t    bool signal_valid, unsigned long ts)\n{\n\tstruct cfg80211_internal_bss *found = NULL;\n\n\tif (WARN_ON(!tmp->pub.channel))\n\t\treturn NULL;\n\n\ttmp->ts = ts;\n\n\tspin_lock_bh(&rdev->bss_lock);\n\n\tif (WARN_ON(!rcu_access_pointer(tmp->pub.ies))) {\n\t\tspin_unlock_bh(&rdev->bss_lock);\n\t\treturn NULL;\n\t}\n\n\tfound = rb_find_bss(rdev, tmp, BSS_CMP_REGULAR);\n\n\tif (found) {\n\t\tif (!cfg80211_update_known_bss(rdev, found, tmp, signal_valid))\n\t\t\tgoto drop;\n\t} else {\n\t\tstruct cfg80211_internal_bss *new;\n\t\tstruct cfg80211_internal_bss *hidden;\n\t\tstruct cfg80211_bss_ies *ies;\n\n\t\t/*\n\t\t * create a copy -- the \"res\" variable that is passed in\n\t\t * is allocated on the stack since it's not needed in the\n\t\t * more common case of an update\n\t\t */\n\t\tnew = kzalloc(sizeof(*new) + rdev->wiphy.bss_priv_size,\n\t\t\t      GFP_ATOMIC);\n\t\tif (!new) {\n\t\t\ties = (void *)rcu_dereference(tmp->pub.beacon_ies);\n\t\t\tif (ies)\n\t\t\t\tkfree_rcu(ies, rcu_head);\n\t\t\ties = (void *)rcu_dereference(tmp->pub.proberesp_ies);\n\t\t\tif (ies)\n\t\t\t\tkfree_rcu(ies, rcu_head);\n\t\t\tgoto drop;\n\t\t}\n\t\tmemcpy(new, tmp, sizeof(*new));\n\t\tnew->refcount = 1;\n\t\tINIT_LIST_HEAD(&new->hidden_list);\n\t\tINIT_LIST_HEAD(&new->pub.nontrans_list);\n\t\t/* we'll set this later if it was non-NULL */\n\t\tnew->pub.transmitted_bss = NULL;\n\n\t\tif (rcu_access_pointer(tmp->pub.proberesp_ies)) {\n\t\t\thidden = rb_find_bss(rdev, tmp, BSS_CMP_HIDE_ZLEN);\n\t\t\tif (!hidden)\n\t\t\t\thidden = rb_find_bss(rdev, tmp,\n\t\t\t\t\t\t     BSS_CMP_HIDE_NUL);\n\t\t\tif (hidden) {\n\t\t\t\tnew->pub.hidden_beacon_bss = &hidden->pub;\n\t\t\t\tlist_add(&new->hidden_list,\n\t\t\t\t\t &hidden->hidden_list);\n\t\t\t\thidden->refcount++;\n\t\t\t\trcu_assign_pointer(new->pub.beacon_ies,\n\t\t\t\t\t\t   hidden->pub.beacon_ies);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Ok so we found a beacon, and don't have an entry. If\n\t\t\t * it's a beacon with hidden SSID, we might be in for an\n\t\t\t * expensive search for any probe responses that should\n\t\t\t * be grouped with this beacon for updates ...\n\t\t\t */\n\t\t\tif (!cfg80211_combine_bsses(rdev, new)) {\n\t\t\t\tbss_ref_put(rdev, new);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\n\t\tif (rdev->bss_entries >= bss_entries_limit &&\n\t\t    !cfg80211_bss_expire_oldest(rdev)) {\n\t\t\tbss_ref_put(rdev, new);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* This must be before the call to bss_ref_get */\n\t\tif (tmp->pub.transmitted_bss) {\n\t\t\tstruct cfg80211_internal_bss *pbss =\n\t\t\t\tcontainer_of(tmp->pub.transmitted_bss,\n\t\t\t\t\t     struct cfg80211_internal_bss,\n\t\t\t\t\t     pub);\n\n\t\t\tnew->pub.transmitted_bss = tmp->pub.transmitted_bss;\n\t\t\tbss_ref_get(rdev, pbss);\n\t\t}\n\n\t\tlist_add_tail(&new->list, &rdev->bss_list);\n\t\trdev->bss_entries++;\n\t\trb_insert_bss(rdev, new);\n\t\tfound = new;\n\t}\n\n\trdev->bss_generation++;\n\tbss_ref_get(rdev, found);\n\tspin_unlock_bh(&rdev->bss_lock);\n\n\treturn found;\n drop:\n\tspin_unlock_bh(&rdev->bss_lock);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,8 @@\n \t\tnew->refcount = 1;\n \t\tINIT_LIST_HEAD(&new->hidden_list);\n \t\tINIT_LIST_HEAD(&new->pub.nontrans_list);\n+\t\t/* we'll set this later if it was non-NULL */\n+\t\tnew->pub.transmitted_bss = NULL;\n \n \t\tif (rcu_access_pointer(tmp->pub.proberesp_ies)) {\n \t\t\thidden = rb_find_bss(rdev, tmp, BSS_CMP_HIDE_ZLEN);",
        "function_modified_lines": {
            "added": [
                "\t\t/* we'll set this later if it was non-NULL */",
                "\t\tnew->pub.transmitted_bss = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Various refcounting bugs in the multi-BSS handling in the mac80211 stack in the Linux kernel 5.1 through 5.19.x before 5.19.16 could be used by local attackers (able to inject WLAN frames) to trigger use-after-free conditions to potentially execute code.",
        "id": 3733
    },
    {
        "cve_id": "CVE-2022-42720",
        "code_before_change": "static inline void bss_ref_get(struct cfg80211_registered_device *rdev,\n\t\t\t       struct cfg80211_internal_bss *bss)\n{\n\tlockdep_assert_held(&rdev->bss_lock);\n\n\tbss->refcount++;\n\tif (bss->pub.hidden_beacon_bss) {\n\t\tbss = container_of(bss->pub.hidden_beacon_bss,\n\t\t\t\t   struct cfg80211_internal_bss,\n\t\t\t\t   pub);\n\t\tbss->refcount++;\n\t}\n\tif (bss->pub.transmitted_bss) {\n\t\tbss = container_of(bss->pub.transmitted_bss,\n\t\t\t\t   struct cfg80211_internal_bss,\n\t\t\t\t   pub);\n\t\tbss->refcount++;\n\t}\n}",
        "code_after_change": "static inline void bss_ref_get(struct cfg80211_registered_device *rdev,\n\t\t\t       struct cfg80211_internal_bss *bss)\n{\n\tlockdep_assert_held(&rdev->bss_lock);\n\n\tbss->refcount++;\n\n\tif (bss->pub.hidden_beacon_bss)\n\t\tbss_from_pub(bss->pub.hidden_beacon_bss)->refcount++;\n\n\tif (bss->pub.transmitted_bss)\n\t\tbss_from_pub(bss->pub.transmitted_bss)->refcount++;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,16 +4,10 @@\n \tlockdep_assert_held(&rdev->bss_lock);\n \n \tbss->refcount++;\n-\tif (bss->pub.hidden_beacon_bss) {\n-\t\tbss = container_of(bss->pub.hidden_beacon_bss,\n-\t\t\t\t   struct cfg80211_internal_bss,\n-\t\t\t\t   pub);\n-\t\tbss->refcount++;\n-\t}\n-\tif (bss->pub.transmitted_bss) {\n-\t\tbss = container_of(bss->pub.transmitted_bss,\n-\t\t\t\t   struct cfg80211_internal_bss,\n-\t\t\t\t   pub);\n-\t\tbss->refcount++;\n-\t}\n+\n+\tif (bss->pub.hidden_beacon_bss)\n+\t\tbss_from_pub(bss->pub.hidden_beacon_bss)->refcount++;\n+\n+\tif (bss->pub.transmitted_bss)\n+\t\tbss_from_pub(bss->pub.transmitted_bss)->refcount++;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (bss->pub.hidden_beacon_bss)",
                "\t\tbss_from_pub(bss->pub.hidden_beacon_bss)->refcount++;",
                "",
                "\tif (bss->pub.transmitted_bss)",
                "\t\tbss_from_pub(bss->pub.transmitted_bss)->refcount++;"
            ],
            "deleted": [
                "\tif (bss->pub.hidden_beacon_bss) {",
                "\t\tbss = container_of(bss->pub.hidden_beacon_bss,",
                "\t\t\t\t   struct cfg80211_internal_bss,",
                "\t\t\t\t   pub);",
                "\t\tbss->refcount++;",
                "\t}",
                "\tif (bss->pub.transmitted_bss) {",
                "\t\tbss = container_of(bss->pub.transmitted_bss,",
                "\t\t\t\t   struct cfg80211_internal_bss,",
                "\t\t\t\t   pub);",
                "\t\tbss->refcount++;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Various refcounting bugs in the multi-BSS handling in the mac80211 stack in the Linux kernel 5.1 through 5.19.x before 5.19.16 could be used by local attackers (able to inject WLAN frames) to trigger use-after-free conditions to potentially execute code.",
        "id": 3735
    },
    {
        "cve_id": "CVE-2021-33034",
        "code_before_change": "static void hci_loglink_complete_evt(struct hci_dev *hdev, struct sk_buff *skb)\n{\n\tstruct hci_ev_logical_link_complete *ev = (void *) skb->data;\n\tstruct hci_conn *hcon;\n\tstruct hci_chan *hchan;\n\tstruct amp_mgr *mgr;\n\n\tBT_DBG(\"%s log_handle 0x%4.4x phy_handle 0x%2.2x status 0x%2.2x\",\n\t       hdev->name, le16_to_cpu(ev->handle), ev->phy_handle,\n\t       ev->status);\n\n\thcon = hci_conn_hash_lookup_handle(hdev, ev->phy_handle);\n\tif (!hcon)\n\t\treturn;\n\n\t/* Create AMP hchan */\n\thchan = hci_chan_create(hcon);\n\tif (!hchan)\n\t\treturn;\n\n\thchan->handle = le16_to_cpu(ev->handle);\n\n\tBT_DBG(\"hcon %p mgr %p hchan %p\", hcon, hcon->amp_mgr, hchan);\n\n\tmgr = hcon->amp_mgr;\n\tif (mgr && mgr->bredr_chan) {\n\t\tstruct l2cap_chan *bredr_chan = mgr->bredr_chan;\n\n\t\tl2cap_chan_lock(bredr_chan);\n\n\t\tbredr_chan->conn->mtu = hdev->block_mtu;\n\t\tl2cap_logical_cfm(bredr_chan, hchan, 0);\n\t\thci_conn_hold(hcon);\n\n\t\tl2cap_chan_unlock(bredr_chan);\n\t}\n}",
        "code_after_change": "static void hci_loglink_complete_evt(struct hci_dev *hdev, struct sk_buff *skb)\n{\n\tstruct hci_ev_logical_link_complete *ev = (void *) skb->data;\n\tstruct hci_conn *hcon;\n\tstruct hci_chan *hchan;\n\tstruct amp_mgr *mgr;\n\n\tBT_DBG(\"%s log_handle 0x%4.4x phy_handle 0x%2.2x status 0x%2.2x\",\n\t       hdev->name, le16_to_cpu(ev->handle), ev->phy_handle,\n\t       ev->status);\n\n\thcon = hci_conn_hash_lookup_handle(hdev, ev->phy_handle);\n\tif (!hcon)\n\t\treturn;\n\n\t/* Create AMP hchan */\n\thchan = hci_chan_create(hcon);\n\tif (!hchan)\n\t\treturn;\n\n\thchan->handle = le16_to_cpu(ev->handle);\n\thchan->amp = true;\n\n\tBT_DBG(\"hcon %p mgr %p hchan %p\", hcon, hcon->amp_mgr, hchan);\n\n\tmgr = hcon->amp_mgr;\n\tif (mgr && mgr->bredr_chan) {\n\t\tstruct l2cap_chan *bredr_chan = mgr->bredr_chan;\n\n\t\tl2cap_chan_lock(bredr_chan);\n\n\t\tbredr_chan->conn->mtu = hdev->block_mtu;\n\t\tl2cap_logical_cfm(bredr_chan, hchan, 0);\n\t\thci_conn_hold(hcon);\n\n\t\tl2cap_chan_unlock(bredr_chan);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,6 +19,7 @@\n \t\treturn;\n \n \thchan->handle = le16_to_cpu(ev->handle);\n+\thchan->amp = true;\n \n \tBT_DBG(\"hcon %p mgr %p hchan %p\", hcon, hcon->amp_mgr, hchan);\n ",
        "function_modified_lines": {
            "added": [
                "\thchan->amp = true;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.12.4, net/bluetooth/hci_event.c has a use-after-free when destroying an hci_chan, aka CID-5c4c8c954409. This leads to writing an arbitrary value.",
        "id": 2968
    },
    {
        "cve_id": "CVE-2020-25669",
        "code_before_change": "static irqreturn_t sunkbd_interrupt(struct serio *serio,\n\t\tunsigned char data, unsigned int flags)\n{\n\tstruct sunkbd *sunkbd = serio_get_drvdata(serio);\n\n\tif (sunkbd->reset <= -1) {\n\t\t/*\n\t\t * If cp[i] is 0xff, sunkbd->reset will stay -1.\n\t\t * The keyboard sends 0xff 0xff 0xID on powerup.\n\t\t */\n\t\tsunkbd->reset = data;\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tgoto out;\n\t}\n\n\tif (sunkbd->layout == -1) {\n\t\tsunkbd->layout = data;\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tgoto out;\n\t}\n\n\tswitch (data) {\n\n\tcase SUNKBD_RET_RESET:\n\t\tschedule_work(&sunkbd->tq);\n\t\tsunkbd->reset = -1;\n\t\tbreak;\n\n\tcase SUNKBD_RET_LAYOUT:\n\t\tsunkbd->layout = -1;\n\t\tbreak;\n\n\tcase SUNKBD_RET_ALLUP: /* All keys released */\n\t\tbreak;\n\n\tdefault:\n\t\tif (!sunkbd->enabled)\n\t\t\tbreak;\n\n\t\tif (sunkbd->keycode[data & SUNKBD_KEY]) {\n\t\t\tinput_report_key(sunkbd->dev,\n\t\t\t\t\t sunkbd->keycode[data & SUNKBD_KEY],\n\t\t\t\t\t !(data & SUNKBD_RELEASE));\n\t\t\tinput_sync(sunkbd->dev);\n\t\t} else {\n\t\t\tprintk(KERN_WARNING\n\t\t\t\t\"sunkbd.c: Unknown key (scancode %#x) %s.\\n\",\n\t\t\t\tdata & SUNKBD_KEY,\n\t\t\t\tdata & SUNKBD_RELEASE ? \"released\" : \"pressed\");\n\t\t}\n\t}\nout:\n\treturn IRQ_HANDLED;\n}",
        "code_after_change": "static irqreturn_t sunkbd_interrupt(struct serio *serio,\n\t\tunsigned char data, unsigned int flags)\n{\n\tstruct sunkbd *sunkbd = serio_get_drvdata(serio);\n\n\tif (sunkbd->reset <= -1) {\n\t\t/*\n\t\t * If cp[i] is 0xff, sunkbd->reset will stay -1.\n\t\t * The keyboard sends 0xff 0xff 0xID on powerup.\n\t\t */\n\t\tsunkbd->reset = data;\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tgoto out;\n\t}\n\n\tif (sunkbd->layout == -1) {\n\t\tsunkbd->layout = data;\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tgoto out;\n\t}\n\n\tswitch (data) {\n\n\tcase SUNKBD_RET_RESET:\n\t\tif (sunkbd->enabled)\n\t\t\tschedule_work(&sunkbd->tq);\n\t\tsunkbd->reset = -1;\n\t\tbreak;\n\n\tcase SUNKBD_RET_LAYOUT:\n\t\tsunkbd->layout = -1;\n\t\tbreak;\n\n\tcase SUNKBD_RET_ALLUP: /* All keys released */\n\t\tbreak;\n\n\tdefault:\n\t\tif (!sunkbd->enabled)\n\t\t\tbreak;\n\n\t\tif (sunkbd->keycode[data & SUNKBD_KEY]) {\n\t\t\tinput_report_key(sunkbd->dev,\n\t\t\t\t\t sunkbd->keycode[data & SUNKBD_KEY],\n\t\t\t\t\t !(data & SUNKBD_RELEASE));\n\t\t\tinput_sync(sunkbd->dev);\n\t\t} else {\n\t\t\tprintk(KERN_WARNING\n\t\t\t\t\"sunkbd.c: Unknown key (scancode %#x) %s.\\n\",\n\t\t\t\tdata & SUNKBD_KEY,\n\t\t\t\tdata & SUNKBD_RELEASE ? \"released\" : \"pressed\");\n\t\t}\n\t}\nout:\n\treturn IRQ_HANDLED;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,8 @@\n \tswitch (data) {\n \n \tcase SUNKBD_RET_RESET:\n-\t\tschedule_work(&sunkbd->tq);\n+\t\tif (sunkbd->enabled)\n+\t\t\tschedule_work(&sunkbd->tq);\n \t\tsunkbd->reset = -1;\n \t\tbreak;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tif (sunkbd->enabled)",
                "\t\t\tschedule_work(&sunkbd->tq);"
            ],
            "deleted": [
                "\t\tschedule_work(&sunkbd->tq);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in the Linux Kernel where the function sunkbd_reinit having been scheduled by sunkbd_interrupt before sunkbd being freed. Though the dangling pointer is set to NULL in sunkbd_disconnect, there is still an alias in sunkbd_reinit causing Use After Free.",
        "id": 2596
    },
    {
        "cve_id": "CVE-2014-0203",
        "code_before_change": "static __always_inline int __do_follow_link(struct path *path, struct nameidata *nd)\n{\n\tint error;\n\tvoid *cookie;\n\tstruct dentry *dentry = path->dentry;\n\n\ttouch_atime(path->mnt, dentry);\n\tnd_set_link(nd, NULL);\n\n\tif (path->mnt != nd->path.mnt) {\n\t\tpath_to_nameidata(path, nd);\n\t\tdget(dentry);\n\t}\n\tmntget(path->mnt);\n\tcookie = dentry->d_inode->i_op->follow_link(dentry, nd);\n\terror = PTR_ERR(cookie);\n\tif (!IS_ERR(cookie)) {\n\t\tchar *s = nd_get_link(nd);\n\t\terror = 0;\n\t\tif (s)\n\t\t\terror = __vfs_follow_link(nd, s);\n\t\telse if (nd->last_type == LAST_BIND) {\n\t\t\terror = force_reval_path(&nd->path, nd);\n\t\t\tif (error)\n\t\t\t\tpath_put(&nd->path);\n\t\t}\n\t\tif (dentry->d_inode->i_op->put_link)\n\t\t\tdentry->d_inode->i_op->put_link(dentry, nd, cookie);\n\t}\n\treturn error;\n}",
        "code_after_change": "static __always_inline int __do_follow_link(struct path *path, struct nameidata *nd)\n{\n\tint error;\n\tvoid *cookie;\n\tstruct dentry *dentry = path->dentry;\n\n\ttouch_atime(path->mnt, dentry);\n\tnd_set_link(nd, NULL);\n\n\tif (path->mnt != nd->path.mnt) {\n\t\tpath_to_nameidata(path, nd);\n\t\tdget(dentry);\n\t}\n\tmntget(path->mnt);\n\tnd->last_type = LAST_BIND;\n\tcookie = dentry->d_inode->i_op->follow_link(dentry, nd);\n\terror = PTR_ERR(cookie);\n\tif (!IS_ERR(cookie)) {\n\t\tchar *s = nd_get_link(nd);\n\t\terror = 0;\n\t\tif (s)\n\t\t\terror = __vfs_follow_link(nd, s);\n\t\telse if (nd->last_type == LAST_BIND) {\n\t\t\terror = force_reval_path(&nd->path, nd);\n\t\t\tif (error)\n\t\t\t\tpath_put(&nd->path);\n\t\t}\n\t\tif (dentry->d_inode->i_op->put_link)\n\t\t\tdentry->d_inode->i_op->put_link(dentry, nd, cookie);\n\t}\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,7 @@\n \t\tdget(dentry);\n \t}\n \tmntget(path->mnt);\n+\tnd->last_type = LAST_BIND;\n \tcookie = dentry->d_inode->i_op->follow_link(dentry, nd);\n \terror = PTR_ERR(cookie);\n \tif (!IS_ERR(cookie)) {",
        "function_modified_lines": {
            "added": [
                "\tnd->last_type = LAST_BIND;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The __do_follow_link function in fs/namei.c in the Linux kernel before 2.6.33 does not properly handle the last pathname component during use of certain filesystems, which allows local users to cause a denial of service (incorrect free operations and system crash) via an open system call.",
        "id": 462
    },
    {
        "cve_id": "CVE-2022-28893",
        "code_before_change": "static int xs_local_send_request(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tstruct sock_xprt *transport =\n\t\t\t\tcontainer_of(xprt, struct sock_xprt, xprt);\n\tstruct xdr_buf *xdr = &req->rq_snd_buf;\n\trpc_fraghdr rm = xs_stream_record_marker(xdr);\n\tunsigned int msglen = rm ? req->rq_slen + sizeof(rm) : req->rq_slen;\n\tstruct msghdr msg = {\n\t\t.msg_flags\t= XS_SENDMSG_FLAGS,\n\t};\n\tbool vm_wait;\n\tunsigned int sent;\n\tint status;\n\n\t/* Close the stream if the previous transmission was incomplete */\n\tif (xs_send_request_was_aborted(transport, req)) {\n\t\txs_close(xprt);\n\t\treturn -ENOTCONN;\n\t}\n\n\txs_pktdump(\"packet data:\",\n\t\t\treq->rq_svec->iov_base, req->rq_svec->iov_len);\n\n\tvm_wait = sk_stream_is_writeable(transport->inet) ? true : false;\n\n\treq->rq_xtime = ktime_get();\n\tstatus = xprt_sock_sendmsg(transport->sock, &msg, xdr,\n\t\t\t\t   transport->xmit.offset, rm, &sent);\n\tdprintk(\"RPC:       %s(%u) = %d\\n\",\n\t\t\t__func__, xdr->len - transport->xmit.offset, status);\n\n\tif (likely(sent > 0) || status == 0) {\n\t\ttransport->xmit.offset += sent;\n\t\treq->rq_bytes_sent = transport->xmit.offset;\n\t\tif (likely(req->rq_bytes_sent >= msglen)) {\n\t\t\treq->rq_xmit_bytes_sent += transport->xmit.offset;\n\t\t\ttransport->xmit.offset = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tstatus = -EAGAIN;\n\t\tvm_wait = false;\n\t}\n\n\tswitch (status) {\n\tcase -EAGAIN:\n\t\tstatus = xs_stream_nospace(req, vm_wait);\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC:       sendmsg returned unrecognized error %d\\n\",\n\t\t\t-status);\n\t\tfallthrough;\n\tcase -EPIPE:\n\t\txs_close(xprt);\n\t\tstatus = -ENOTCONN;\n\t}\n\n\treturn status;\n}",
        "code_after_change": "static int xs_local_send_request(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tstruct sock_xprt *transport =\n\t\t\t\tcontainer_of(xprt, struct sock_xprt, xprt);\n\tstruct xdr_buf *xdr = &req->rq_snd_buf;\n\trpc_fraghdr rm = xs_stream_record_marker(xdr);\n\tunsigned int msglen = rm ? req->rq_slen + sizeof(rm) : req->rq_slen;\n\tstruct msghdr msg = {\n\t\t.msg_flags\t= XS_SENDMSG_FLAGS,\n\t};\n\tbool vm_wait;\n\tunsigned int sent;\n\tint status;\n\n\t/* Close the stream if the previous transmission was incomplete */\n\tif (xs_send_request_was_aborted(transport, req)) {\n\t\txprt_force_disconnect(xprt);\n\t\treturn -ENOTCONN;\n\t}\n\n\txs_pktdump(\"packet data:\",\n\t\t\treq->rq_svec->iov_base, req->rq_svec->iov_len);\n\n\tvm_wait = sk_stream_is_writeable(transport->inet) ? true : false;\n\n\treq->rq_xtime = ktime_get();\n\tstatus = xprt_sock_sendmsg(transport->sock, &msg, xdr,\n\t\t\t\t   transport->xmit.offset, rm, &sent);\n\tdprintk(\"RPC:       %s(%u) = %d\\n\",\n\t\t\t__func__, xdr->len - transport->xmit.offset, status);\n\n\tif (likely(sent > 0) || status == 0) {\n\t\ttransport->xmit.offset += sent;\n\t\treq->rq_bytes_sent = transport->xmit.offset;\n\t\tif (likely(req->rq_bytes_sent >= msglen)) {\n\t\t\treq->rq_xmit_bytes_sent += transport->xmit.offset;\n\t\t\ttransport->xmit.offset = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tstatus = -EAGAIN;\n\t\tvm_wait = false;\n\t}\n\n\tswitch (status) {\n\tcase -EAGAIN:\n\t\tstatus = xs_stream_nospace(req, vm_wait);\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC:       sendmsg returned unrecognized error %d\\n\",\n\t\t\t-status);\n\t\tfallthrough;\n\tcase -EPIPE:\n\t\txprt_force_disconnect(xprt);\n\t\tstatus = -ENOTCONN;\n\t}\n\n\treturn status;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,7 +15,7 @@\n \n \t/* Close the stream if the previous transmission was incomplete */\n \tif (xs_send_request_was_aborted(transport, req)) {\n-\t\txs_close(xprt);\n+\t\txprt_force_disconnect(xprt);\n \t\treturn -ENOTCONN;\n \t}\n \n@@ -51,7 +51,7 @@\n \t\t\t-status);\n \t\tfallthrough;\n \tcase -EPIPE:\n-\t\txs_close(xprt);\n+\t\txprt_force_disconnect(xprt);\n \t\tstatus = -ENOTCONN;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\txprt_force_disconnect(xprt);",
                "\t\txprt_force_disconnect(xprt);"
            ],
            "deleted": [
                "\t\txs_close(xprt);",
                "\t\txs_close(xprt);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The SUNRPC subsystem in the Linux kernel through 5.17.2 can call xs_xprt_free before ensuring that sockets are in the intended state.",
        "id": 3508
    },
    {
        "cve_id": "CVE-2020-0427",
        "code_before_change": "static void dt_free_map(struct pinctrl_dev *pctldev,\n\t\t     struct pinctrl_map *map, unsigned num_maps)\n{\n\tif (pctldev) {\n\t\tconst struct pinctrl_ops *ops = pctldev->desc->pctlops;\n\t\tif (ops->dt_free_map)\n\t\t\tops->dt_free_map(pctldev, map, num_maps);\n\t} else {\n\t\t/* There is no pctldev for PIN_MAP_TYPE_DUMMY_STATE */\n\t\tkfree(map);\n\t}\n}",
        "code_after_change": "static void dt_free_map(struct pinctrl_dev *pctldev,\n\t\t     struct pinctrl_map *map, unsigned num_maps)\n{\n\tint i;\n\n\tfor (i = 0; i < num_maps; ++i) {\n\t\tkfree_const(map[i].dev_name);\n\t\tmap[i].dev_name = NULL;\n\t}\n\n\tif (pctldev) {\n\t\tconst struct pinctrl_ops *ops = pctldev->desc->pctlops;\n\t\tif (ops->dt_free_map)\n\t\t\tops->dt_free_map(pctldev, map, num_maps);\n\t} else {\n\t\t/* There is no pctldev for PIN_MAP_TYPE_DUMMY_STATE */\n\t\tkfree(map);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,13 @@\n static void dt_free_map(struct pinctrl_dev *pctldev,\n \t\t     struct pinctrl_map *map, unsigned num_maps)\n {\n+\tint i;\n+\n+\tfor (i = 0; i < num_maps; ++i) {\n+\t\tkfree_const(map[i].dev_name);\n+\t\tmap[i].dev_name = NULL;\n+\t}\n+\n \tif (pctldev) {\n \t\tconst struct pinctrl_ops *ops = pctldev->desc->pctlops;\n \t\tif (ops->dt_free_map)",
        "function_modified_lines": {
            "added": [
                "\tint i;",
                "",
                "\tfor (i = 0; i < num_maps; ++i) {",
                "\t\tkfree_const(map[i].dev_name);",
                "\t\tmap[i].dev_name = NULL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-125",
            "CWE-416"
        ],
        "cve_description": "In create_pinctrl of core.c, there is a possible out of bounds read due to a use after free. This could lead to local information disclosure with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-140550171",
        "id": 2379
    },
    {
        "cve_id": "CVE-2023-39198",
        "code_before_change": "int qxl_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct qxl_bo *qobj;\n\tuint32_t handle;\n\tint r;\n\tstruct qxl_surface surf;\n\tuint32_t pitch, format;\n\n\tpitch = args->width * ((args->bpp + 1) / 8);\n\targs->size = pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\n\tswitch (args->bpp) {\n\tcase 16:\n\t\tformat = SPICE_SURFACE_FMT_16_565;\n\t\tbreak;\n\tcase 32:\n\t\tformat = SPICE_SURFACE_FMT_32_xRGB;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tsurf.width = args->width;\n\tsurf.height = args->height;\n\tsurf.stride = pitch;\n\tsurf.format = format;\n\tsurf.data = 0;\n\n\tr = qxl_gem_object_create_with_handle(qdev, file_priv,\n\t\t\t\t\t      QXL_GEM_DOMAIN_CPU,\n\t\t\t\t\t      args->size, &surf, &qobj,\n\t\t\t\t\t      &handle);\n\tif (r)\n\t\treturn r;\n\tqobj->is_dumb = true;\n\targs->pitch = pitch;\n\targs->handle = handle;\n\treturn 0;\n}",
        "code_after_change": "int qxl_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct qxl_bo *qobj;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tint r;\n\tstruct qxl_surface surf;\n\tuint32_t pitch, format;\n\n\tpitch = args->width * ((args->bpp + 1) / 8);\n\targs->size = pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\n\tswitch (args->bpp) {\n\tcase 16:\n\t\tformat = SPICE_SURFACE_FMT_16_565;\n\t\tbreak;\n\tcase 32:\n\t\tformat = SPICE_SURFACE_FMT_32_xRGB;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tsurf.width = args->width;\n\tsurf.height = args->height;\n\tsurf.stride = pitch;\n\tsurf.format = format;\n\tsurf.data = 0;\n\n\tr = qxl_gem_object_create_with_handle(qdev, file_priv,\n\t\t\t\t\t      QXL_GEM_DOMAIN_CPU,\n\t\t\t\t\t      args->size, &surf, &gobj,\n\t\t\t\t\t      &handle);\n\tif (r)\n\t\treturn r;\n\tqobj = gem_to_qxl_bo(gobj);\n\tqobj->is_dumb = true;\n\tdrm_gem_object_put(gobj);\n\targs->pitch = pitch;\n\targs->handle = handle;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n {\n \tstruct qxl_device *qdev = to_qxl(dev);\n \tstruct qxl_bo *qobj;\n+\tstruct drm_gem_object *gobj;\n \tuint32_t handle;\n \tint r;\n \tstruct qxl_surface surf;\n@@ -32,11 +33,13 @@\n \n \tr = qxl_gem_object_create_with_handle(qdev, file_priv,\n \t\t\t\t\t      QXL_GEM_DOMAIN_CPU,\n-\t\t\t\t\t      args->size, &surf, &qobj,\n+\t\t\t\t\t      args->size, &surf, &gobj,\n \t\t\t\t\t      &handle);\n \tif (r)\n \t\treturn r;\n+\tqobj = gem_to_qxl_bo(gobj);\n \tqobj->is_dumb = true;\n+\tdrm_gem_object_put(gobj);\n \targs->pitch = pitch;\n \targs->handle = handle;\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tstruct drm_gem_object *gobj;",
                "\t\t\t\t\t      args->size, &surf, &gobj,",
                "\tqobj = gem_to_qxl_bo(gobj);",
                "\tdrm_gem_object_put(gobj);"
            ],
            "deleted": [
                "\t\t\t\t\t      args->size, &surf, &qobj,"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A race condition was found in the QXL driver in the Linux kernel. The qxl_mode_dumb_create() function dereferences the qobj returned by the qxl_gem_object_create_with_handle(), but the handle is the only one holding a reference to it. This flaw allows an attacker to guess the returned handle value and trigger a use-after-free issue, potentially leading to a denial of service or privilege escalation.",
        "id": 4185
    },
    {
        "cve_id": "CVE-2023-21255",
        "code_before_change": "static void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "code_after_change": "static void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,6 +29,6 @@\n \t\tbinder_node_inner_unlock(buf_node);\n \t}\n \ttrace_binder_transaction_buffer_release(buffer);\n-\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\n+\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);\n \tbinder_alloc_free_buf(&proc->alloc, buffer);\n }",
        "function_modified_lines": {
            "added": [
                "\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);"
            ],
            "deleted": [
                "\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);"
            ]
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In multiple functions of binder.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.\n\n",
        "id": 3916
    },
    {
        "cve_id": "CVE-2016-4805",
        "code_before_change": "int ppp_register_net_channel(struct net *net, struct ppp_channel *chan)\n{\n\tstruct channel *pch;\n\tstruct ppp_net *pn;\n\n\tpch = kzalloc(sizeof(struct channel), GFP_KERNEL);\n\tif (!pch)\n\t\treturn -ENOMEM;\n\n\tpn = ppp_pernet(net);\n\n\tpch->ppp = NULL;\n\tpch->chan = chan;\n\tpch->chan_net = net;\n\tchan->ppp = pch;\n\tinit_ppp_file(&pch->file, CHANNEL);\n\tpch->file.hdrlen = chan->hdrlen;\n#ifdef CONFIG_PPP_MULTILINK\n\tpch->lastseq = -1;\n#endif /* CONFIG_PPP_MULTILINK */\n\tinit_rwsem(&pch->chan_sem);\n\tspin_lock_init(&pch->downl);\n\trwlock_init(&pch->upl);\n\n\tspin_lock_bh(&pn->all_channels_lock);\n\tpch->file.index = ++pn->last_channel_index;\n\tlist_add(&pch->list, &pn->new_channels);\n\tatomic_inc(&channel_count);\n\tspin_unlock_bh(&pn->all_channels_lock);\n\n\treturn 0;\n}",
        "code_after_change": "int ppp_register_net_channel(struct net *net, struct ppp_channel *chan)\n{\n\tstruct channel *pch;\n\tstruct ppp_net *pn;\n\n\tpch = kzalloc(sizeof(struct channel), GFP_KERNEL);\n\tif (!pch)\n\t\treturn -ENOMEM;\n\n\tpn = ppp_pernet(net);\n\n\tpch->ppp = NULL;\n\tpch->chan = chan;\n\tpch->chan_net = get_net(net);\n\tchan->ppp = pch;\n\tinit_ppp_file(&pch->file, CHANNEL);\n\tpch->file.hdrlen = chan->hdrlen;\n#ifdef CONFIG_PPP_MULTILINK\n\tpch->lastseq = -1;\n#endif /* CONFIG_PPP_MULTILINK */\n\tinit_rwsem(&pch->chan_sem);\n\tspin_lock_init(&pch->downl);\n\trwlock_init(&pch->upl);\n\n\tspin_lock_bh(&pn->all_channels_lock);\n\tpch->file.index = ++pn->last_channel_index;\n\tlist_add(&pch->list, &pn->new_channels);\n\tatomic_inc(&channel_count);\n\tspin_unlock_bh(&pn->all_channels_lock);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \n \tpch->ppp = NULL;\n \tpch->chan = chan;\n-\tpch->chan_net = net;\n+\tpch->chan_net = get_net(net);\n \tchan->ppp = pch;\n \tinit_ppp_file(&pch->file, CHANNEL);\n \tpch->file.hdrlen = chan->hdrlen;",
        "function_modified_lines": {
            "added": [
                "\tpch->chan_net = get_net(net);"
            ],
            "deleted": [
                "\tpch->chan_net = net;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in drivers/net/ppp/ppp_generic.c in the Linux kernel before 4.5.2 allows local users to cause a denial of service (memory corruption and system crash, or spinlock) or possibly have unspecified other impact by removing a network namespace, related to the ppp_register_net_channel and ppp_unregister_channel functions.",
        "id": 1036
    },
    {
        "cve_id": "CVE-2021-39801",
        "code_before_change": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\tstruct ion_handle *handle;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_buffer *buffer = NULL;\n\tstruct ion_heap *heap;\n\tint ret;\n\n\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n\t\t len, align, heap_id_mask, flags);\n\t/*\n\t * traverse the list of heaps available in this system in priority\n\t * order.  If the heap type is supported by the client, and matches the\n\t * request of the caller allocate from it.  Repeat until allocate has\n\t * succeeded or all heaps have been tried\n\t */\n\tlen = PAGE_ALIGN(len);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&dev->lock);\n\tplist_for_each_entry(heap, &dev->heaps, node) {\n\t\t/* if the caller didn't specify this heap id */\n\t\tif (!((1 << heap->id) & heap_id_mask))\n\t\t\tcontinue;\n\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n\t\tif (!IS_ERR(buffer))\n\t\t\tbreak;\n\t}\n\tup_read(&dev->lock);\n\n\tif (buffer == NULL)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (IS_ERR(buffer))\n\t\treturn ERR_CAST(buffer);\n\n\thandle = ion_handle_create(client, buffer);\n\n\t/*\n\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n\t * and ion_handle_create will take a second reference, drop one here\n\t */\n\tion_buffer_put(buffer);\n\n\tif (IS_ERR(handle))\n\t\treturn handle;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\n\treturn handle;\n}",
        "code_after_change": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,60 +2,5 @@\n \t\t\t     size_t align, unsigned int heap_id_mask,\n \t\t\t     unsigned int flags)\n {\n-\tstruct ion_handle *handle;\n-\tstruct ion_device *dev = client->dev;\n-\tstruct ion_buffer *buffer = NULL;\n-\tstruct ion_heap *heap;\n-\tint ret;\n-\n-\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n-\t\t len, align, heap_id_mask, flags);\n-\t/*\n-\t * traverse the list of heaps available in this system in priority\n-\t * order.  If the heap type is supported by the client, and matches the\n-\t * request of the caller allocate from it.  Repeat until allocate has\n-\t * succeeded or all heaps have been tried\n-\t */\n-\tlen = PAGE_ALIGN(len);\n-\n-\tif (!len)\n-\t\treturn ERR_PTR(-EINVAL);\n-\n-\tdown_read(&dev->lock);\n-\tplist_for_each_entry(heap, &dev->heaps, node) {\n-\t\t/* if the caller didn't specify this heap id */\n-\t\tif (!((1 << heap->id) & heap_id_mask))\n-\t\t\tcontinue;\n-\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n-\t\tif (!IS_ERR(buffer))\n-\t\t\tbreak;\n-\t}\n-\tup_read(&dev->lock);\n-\n-\tif (buffer == NULL)\n-\t\treturn ERR_PTR(-ENODEV);\n-\n-\tif (IS_ERR(buffer))\n-\t\treturn ERR_CAST(buffer);\n-\n-\thandle = ion_handle_create(client, buffer);\n-\n-\t/*\n-\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n-\t * and ion_handle_create will take a second reference, drop one here\n-\t */\n-\tion_buffer_put(buffer);\n-\n-\tif (IS_ERR(handle))\n-\t\treturn handle;\n-\n-\tmutex_lock(&client->lock);\n-\tret = ion_handle_add(client, handle);\n-\tmutex_unlock(&client->lock);\n-\tif (ret) {\n-\t\tion_handle_put(handle);\n-\t\thandle = ERR_PTR(ret);\n-\t}\n-\n-\treturn handle;\n+\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);"
            ],
            "deleted": [
                "\tstruct ion_handle *handle;",
                "\tstruct ion_device *dev = client->dev;",
                "\tstruct ion_buffer *buffer = NULL;",
                "\tstruct ion_heap *heap;",
                "\tint ret;",
                "",
                "\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,",
                "\t\t len, align, heap_id_mask, flags);",
                "\t/*",
                "\t * traverse the list of heaps available in this system in priority",
                "\t * order.  If the heap type is supported by the client, and matches the",
                "\t * request of the caller allocate from it.  Repeat until allocate has",
                "\t * succeeded or all heaps have been tried",
                "\t */",
                "\tlen = PAGE_ALIGN(len);",
                "",
                "\tif (!len)",
                "\t\treturn ERR_PTR(-EINVAL);",
                "",
                "\tdown_read(&dev->lock);",
                "\tplist_for_each_entry(heap, &dev->heaps, node) {",
                "\t\t/* if the caller didn't specify this heap id */",
                "\t\tif (!((1 << heap->id) & heap_id_mask))",
                "\t\t\tcontinue;",
                "\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);",
                "\t\tif (!IS_ERR(buffer))",
                "\t\t\tbreak;",
                "\t}",
                "\tup_read(&dev->lock);",
                "",
                "\tif (buffer == NULL)",
                "\t\treturn ERR_PTR(-ENODEV);",
                "",
                "\tif (IS_ERR(buffer))",
                "\t\treturn ERR_CAST(buffer);",
                "",
                "\thandle = ion_handle_create(client, buffer);",
                "",
                "\t/*",
                "\t * ion_buffer_create will create a buffer with a ref_cnt of 1,",
                "\t * and ion_handle_create will take a second reference, drop one here",
                "\t */",
                "\tion_buffer_put(buffer);",
                "",
                "\tif (IS_ERR(handle))",
                "\t\treturn handle;",
                "",
                "\tmutex_lock(&client->lock);",
                "\tret = ion_handle_add(client, handle);",
                "\tmutex_unlock(&client->lock);",
                "\tif (ret) {",
                "\t\tion_handle_put(handle);",
                "\t\thandle = ERR_PTR(ret);",
                "\t}",
                "",
                "\treturn handle;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In ion_ioctl of ion-ioctl.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-209791720References: Upstream kernel",
        "id": 3111
    },
    {
        "cve_id": "CVE-2016-10905",
        "code_before_change": "static int read_rindex_entry(struct gfs2_inode *ip)\n{\n\tstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\n\tconst unsigned bsize = sdp->sd_sb.sb_bsize;\n\tloff_t pos = sdp->sd_rgrps * sizeof(struct gfs2_rindex);\n\tstruct gfs2_rindex buf;\n\tint error;\n\tstruct gfs2_rgrpd *rgd;\n\n\tif (pos >= i_size_read(&ip->i_inode))\n\t\treturn 1;\n\n\terror = gfs2_internal_read(ip, (char *)&buf, &pos,\n\t\t\t\t   sizeof(struct gfs2_rindex));\n\n\tif (error != sizeof(struct gfs2_rindex))\n\t\treturn (error == 0) ? 1 : error;\n\n\trgd = kmem_cache_zalloc(gfs2_rgrpd_cachep, GFP_NOFS);\n\terror = -ENOMEM;\n\tif (!rgd)\n\t\treturn error;\n\n\trgd->rd_sbd = sdp;\n\trgd->rd_addr = be64_to_cpu(buf.ri_addr);\n\trgd->rd_length = be32_to_cpu(buf.ri_length);\n\trgd->rd_data0 = be64_to_cpu(buf.ri_data0);\n\trgd->rd_data = be32_to_cpu(buf.ri_data);\n\trgd->rd_bitbytes = be32_to_cpu(buf.ri_bitbytes);\n\tspin_lock_init(&rgd->rd_rsspin);\n\n\terror = compute_bitstructs(rgd);\n\tif (error)\n\t\tgoto fail;\n\n\terror = gfs2_glock_get(sdp, rgd->rd_addr,\n\t\t\t       &gfs2_rgrp_glops, CREATE, &rgd->rd_gl);\n\tif (error)\n\t\tgoto fail;\n\n\trgd->rd_gl->gl_object = rgd;\n\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;\n\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr + rgd->rd_length) * bsize) - 1;\n\trgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;\n\trgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);\n\tif (rgd->rd_data > sdp->sd_max_rg_data)\n\t\tsdp->sd_max_rg_data = rgd->rd_data;\n\tspin_lock(&sdp->sd_rindex_spin);\n\terror = rgd_insert(rgd);\n\tspin_unlock(&sdp->sd_rindex_spin);\n\tif (!error)\n\t\treturn 0;\n\n\terror = 0; /* someone else read in the rgrp; free it and ignore it */\n\tgfs2_glock_put(rgd->rd_gl);\n\nfail:\n\tkfree(rgd->rd_bits);\n\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n\treturn error;\n}",
        "code_after_change": "static int read_rindex_entry(struct gfs2_inode *ip)\n{\n\tstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\n\tconst unsigned bsize = sdp->sd_sb.sb_bsize;\n\tloff_t pos = sdp->sd_rgrps * sizeof(struct gfs2_rindex);\n\tstruct gfs2_rindex buf;\n\tint error;\n\tstruct gfs2_rgrpd *rgd;\n\n\tif (pos >= i_size_read(&ip->i_inode))\n\t\treturn 1;\n\n\terror = gfs2_internal_read(ip, (char *)&buf, &pos,\n\t\t\t\t   sizeof(struct gfs2_rindex));\n\n\tif (error != sizeof(struct gfs2_rindex))\n\t\treturn (error == 0) ? 1 : error;\n\n\trgd = kmem_cache_zalloc(gfs2_rgrpd_cachep, GFP_NOFS);\n\terror = -ENOMEM;\n\tif (!rgd)\n\t\treturn error;\n\n\trgd->rd_sbd = sdp;\n\trgd->rd_addr = be64_to_cpu(buf.ri_addr);\n\trgd->rd_length = be32_to_cpu(buf.ri_length);\n\trgd->rd_data0 = be64_to_cpu(buf.ri_data0);\n\trgd->rd_data = be32_to_cpu(buf.ri_data);\n\trgd->rd_bitbytes = be32_to_cpu(buf.ri_bitbytes);\n\tspin_lock_init(&rgd->rd_rsspin);\n\n\terror = compute_bitstructs(rgd);\n\tif (error)\n\t\tgoto fail;\n\n\terror = gfs2_glock_get(sdp, rgd->rd_addr,\n\t\t\t       &gfs2_rgrp_glops, CREATE, &rgd->rd_gl);\n\tif (error)\n\t\tgoto fail;\n\n\trgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;\n\trgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);\n\tif (rgd->rd_data > sdp->sd_max_rg_data)\n\t\tsdp->sd_max_rg_data = rgd->rd_data;\n\tspin_lock(&sdp->sd_rindex_spin);\n\terror = rgd_insert(rgd);\n\tspin_unlock(&sdp->sd_rindex_spin);\n\tif (!error) {\n\t\trgd->rd_gl->gl_object = rgd;\n\t\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;\n\t\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr +\n\t\t\t\t\t\t    rgd->rd_length) * bsize) - 1;\n\t\treturn 0;\n\t}\n\n\terror = 0; /* someone else read in the rgrp; free it and ignore it */\n\tgfs2_glock_put(rgd->rd_gl);\n\nfail:\n\tkfree(rgd->rd_bits);\n\trgd->rd_bits = NULL;\n\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,9 +38,6 @@\n \tif (error)\n \t\tgoto fail;\n \n-\trgd->rd_gl->gl_object = rgd;\n-\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;\n-\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr + rgd->rd_length) * bsize) - 1;\n \trgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;\n \trgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);\n \tif (rgd->rd_data > sdp->sd_max_rg_data)\n@@ -48,14 +45,20 @@\n \tspin_lock(&sdp->sd_rindex_spin);\n \terror = rgd_insert(rgd);\n \tspin_unlock(&sdp->sd_rindex_spin);\n-\tif (!error)\n+\tif (!error) {\n+\t\trgd->rd_gl->gl_object = rgd;\n+\t\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;\n+\t\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr +\n+\t\t\t\t\t\t    rgd->rd_length) * bsize) - 1;\n \t\treturn 0;\n+\t}\n \n \terror = 0; /* someone else read in the rgrp; free it and ignore it */\n \tgfs2_glock_put(rgd->rd_gl);\n \n fail:\n \tkfree(rgd->rd_bits);\n+\trgd->rd_bits = NULL;\n \tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n \treturn error;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!error) {",
                "\t\trgd->rd_gl->gl_object = rgd;",
                "\t\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;",
                "\t\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr +",
                "\t\t\t\t\t\t    rgd->rd_length) * bsize) - 1;",
                "\t}",
                "\trgd->rd_bits = NULL;"
            ],
            "deleted": [
                "\trgd->rd_gl->gl_object = rgd;",
                "\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;",
                "\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr + rgd->rd_length) * bsize) - 1;",
                "\tif (!error)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in fs/gfs2/rgrp.c in the Linux kernel before 4.8. A use-after-free is caused by the functions gfs2_clear_rgrpd and read_rindex_entry.",
        "id": 908
    },
    {
        "cve_id": "CVE-2022-45888",
        "code_before_change": "static int xillyusb_open(struct inode *inode, struct file *filp)\n{\n\tstruct xillyusb_dev *xdev;\n\tstruct xillyusb_channel *chan;\n\tstruct xillyfifo *in_fifo = NULL;\n\tstruct xillyusb_endpoint *out_ep = NULL;\n\tint rc;\n\tint index;\n\n\trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n\tif (rc)\n\t\treturn rc;\n\n\tchan = &xdev->channels[index];\n\tfilp->private_data = chan;\n\n\tmutex_lock(&chan->lock);\n\n\trc = -ENODEV;\n\n\tif (xdev->error)\n\t\tgoto unmutex_fail;\n\n\tif (((filp->f_mode & FMODE_READ) && !chan->readable) ||\n\t    ((filp->f_mode & FMODE_WRITE) && !chan->writable))\n\t\tgoto unmutex_fail;\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_READ) &&\n\t    chan->in_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for read on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_WRITE) &&\n\t    chan->out_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for write on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\trc = -EBUSY;\n\n\tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n\t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n\t\tgoto unmutex_fail;\n\n\tkref_get(&xdev->kref);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 1;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 1;\n\n\tmutex_unlock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tout_ep = endpoint_alloc(xdev,\n\t\t\t\t\t(chan->chan_idx + 2) | USB_DIR_OUT,\n\t\t\t\t\tbulk_out_work, BUF_SIZE_ORDER, BUFNUM);\n\n\t\tif (!out_ep) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto unopen;\n\t\t}\n\n\t\trc = fifo_init(&out_ep->fifo, chan->out_log2_fifo_size);\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\n\t\tout_ep->fill_mask = -(1 << chan->out_log2_element_size);\n\t\tchan->out_bytes = 0;\n\t\tchan->flushed = 0;\n\n\t\t/*\n\t\t * Sending a flush request to a previously closed stream\n\t\t * effectively opens it, and also waits until the command is\n\t\t * confirmed by the FPGA. The latter is necessary because the\n\t\t * data is sent through a separate BULK OUT endpoint, and the\n\t\t * xHCI controller is free to reorder transmissions.\n\t\t *\n\t\t * This can't go wrong unless there's a serious hardware error\n\t\t * (or the computer is stuck for 500 ms?)\n\t\t */\n\t\trc = flush_downstream(chan, XILLY_RESPONSE_TIMEOUT, false);\n\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\trc = -EIO;\n\t\t\treport_io_error(xdev, rc);\n\t\t}\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t}\n\n\tif (filp->f_mode & FMODE_READ) {\n\t\tin_fifo = kzalloc(sizeof(*in_fifo), GFP_KERNEL);\n\n\t\tif (!in_fifo) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto late_unopen;\n\t\t}\n\n\t\trc = fifo_init(in_fifo, chan->in_log2_fifo_size);\n\n\t\tif (rc) {\n\t\t\tkfree(in_fifo);\n\t\t\tgoto late_unopen;\n\t\t}\n\t}\n\n\tmutex_lock(&chan->lock);\n\tif (in_fifo) {\n\t\tchan->in_fifo = in_fifo;\n\t\tchan->read_data_ok = 1;\n\t}\n\tif (out_ep)\n\t\tchan->out_ep = out_ep;\n\tmutex_unlock(&chan->lock);\n\n\tif (in_fifo) {\n\t\tu32 in_checkpoint = 0;\n\n\t\tif (!chan->in_synchronous)\n\t\t\tin_checkpoint = in_fifo->size >>\n\t\t\t\tchan->in_log2_element_size;\n\n\t\tchan->in_consumed_bytes = 0;\n\t\tchan->poll_used = 0;\n\t\tchan->in_current_checkpoint = in_checkpoint;\n\t\trc = xillyusb_send_opcode(xdev, (chan->chan_idx << 1) | 1,\n\t\t\t\t\t  OPCODE_SET_CHECKPOINT,\n\t\t\t\t\t  in_checkpoint);\n\n\t\tif (rc) /* Failure guarantees that opcode wasn't sent */\n\t\t\tgoto unfifo;\n\n\t\t/*\n\t\t * In non-blocking mode, request the FPGA to send any data it\n\t\t * has right away. Otherwise, the first read() will always\n\t\t * return -EAGAIN, which is OK strictly speaking, but ugly.\n\t\t * Checking and unrolling if this fails isn't worth the\n\t\t * effort -- the error is propagated to the first read()\n\t\t * anyhow.\n\t\t */\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\trequest_read_anything(chan, OPCODE_SET_PUSH);\n\t}\n\n\treturn 0;\n\nunfifo:\n\tchan->read_data_ok = 0;\n\tsafely_assign_in_fifo(chan, NULL);\n\tfifo_mem_release(in_fifo);\n\tkfree(in_fifo);\n\n\tif (out_ep) {\n\t\tmutex_lock(&chan->lock);\n\t\tchan->out_ep = NULL;\n\t\tmutex_unlock(&chan->lock);\n\t}\n\nlate_unopen:\n\tif (out_ep)\n\t\tendpoint_dealloc(out_ep);\n\nunopen:\n\tmutex_lock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 0;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 0;\n\n\tmutex_unlock(&chan->lock);\n\n\tkref_put(&xdev->kref, cleanup_dev);\n\n\treturn rc;\n\nunmutex_fail:\n\tmutex_unlock(&chan->lock);\n\treturn rc;\n}",
        "code_after_change": "static int xillyusb_open(struct inode *inode, struct file *filp)\n{\n\tstruct xillyusb_dev *xdev;\n\tstruct xillyusb_channel *chan;\n\tstruct xillyfifo *in_fifo = NULL;\n\tstruct xillyusb_endpoint *out_ep = NULL;\n\tint rc;\n\tint index;\n\n\tmutex_lock(&kref_mutex);\n\n\trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n\tif (rc) {\n\t\tmutex_unlock(&kref_mutex);\n\t\treturn rc;\n\t}\n\n\tkref_get(&xdev->kref);\n\tmutex_unlock(&kref_mutex);\n\n\tchan = &xdev->channels[index];\n\tfilp->private_data = chan;\n\n\tmutex_lock(&chan->lock);\n\n\trc = -ENODEV;\n\n\tif (xdev->error)\n\t\tgoto unmutex_fail;\n\n\tif (((filp->f_mode & FMODE_READ) && !chan->readable) ||\n\t    ((filp->f_mode & FMODE_WRITE) && !chan->writable))\n\t\tgoto unmutex_fail;\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_READ) &&\n\t    chan->in_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for read on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_WRITE) &&\n\t    chan->out_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for write on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\trc = -EBUSY;\n\n\tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n\t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n\t\tgoto unmutex_fail;\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 1;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 1;\n\n\tmutex_unlock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tout_ep = endpoint_alloc(xdev,\n\t\t\t\t\t(chan->chan_idx + 2) | USB_DIR_OUT,\n\t\t\t\t\tbulk_out_work, BUF_SIZE_ORDER, BUFNUM);\n\n\t\tif (!out_ep) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto unopen;\n\t\t}\n\n\t\trc = fifo_init(&out_ep->fifo, chan->out_log2_fifo_size);\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\n\t\tout_ep->fill_mask = -(1 << chan->out_log2_element_size);\n\t\tchan->out_bytes = 0;\n\t\tchan->flushed = 0;\n\n\t\t/*\n\t\t * Sending a flush request to a previously closed stream\n\t\t * effectively opens it, and also waits until the command is\n\t\t * confirmed by the FPGA. The latter is necessary because the\n\t\t * data is sent through a separate BULK OUT endpoint, and the\n\t\t * xHCI controller is free to reorder transmissions.\n\t\t *\n\t\t * This can't go wrong unless there's a serious hardware error\n\t\t * (or the computer is stuck for 500 ms?)\n\t\t */\n\t\trc = flush_downstream(chan, XILLY_RESPONSE_TIMEOUT, false);\n\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\trc = -EIO;\n\t\t\treport_io_error(xdev, rc);\n\t\t}\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t}\n\n\tif (filp->f_mode & FMODE_READ) {\n\t\tin_fifo = kzalloc(sizeof(*in_fifo), GFP_KERNEL);\n\n\t\tif (!in_fifo) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto late_unopen;\n\t\t}\n\n\t\trc = fifo_init(in_fifo, chan->in_log2_fifo_size);\n\n\t\tif (rc) {\n\t\t\tkfree(in_fifo);\n\t\t\tgoto late_unopen;\n\t\t}\n\t}\n\n\tmutex_lock(&chan->lock);\n\tif (in_fifo) {\n\t\tchan->in_fifo = in_fifo;\n\t\tchan->read_data_ok = 1;\n\t}\n\tif (out_ep)\n\t\tchan->out_ep = out_ep;\n\tmutex_unlock(&chan->lock);\n\n\tif (in_fifo) {\n\t\tu32 in_checkpoint = 0;\n\n\t\tif (!chan->in_synchronous)\n\t\t\tin_checkpoint = in_fifo->size >>\n\t\t\t\tchan->in_log2_element_size;\n\n\t\tchan->in_consumed_bytes = 0;\n\t\tchan->poll_used = 0;\n\t\tchan->in_current_checkpoint = in_checkpoint;\n\t\trc = xillyusb_send_opcode(xdev, (chan->chan_idx << 1) | 1,\n\t\t\t\t\t  OPCODE_SET_CHECKPOINT,\n\t\t\t\t\t  in_checkpoint);\n\n\t\tif (rc) /* Failure guarantees that opcode wasn't sent */\n\t\t\tgoto unfifo;\n\n\t\t/*\n\t\t * In non-blocking mode, request the FPGA to send any data it\n\t\t * has right away. Otherwise, the first read() will always\n\t\t * return -EAGAIN, which is OK strictly speaking, but ugly.\n\t\t * Checking and unrolling if this fails isn't worth the\n\t\t * effort -- the error is propagated to the first read()\n\t\t * anyhow.\n\t\t */\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\trequest_read_anything(chan, OPCODE_SET_PUSH);\n\t}\n\n\treturn 0;\n\nunfifo:\n\tchan->read_data_ok = 0;\n\tsafely_assign_in_fifo(chan, NULL);\n\tfifo_mem_release(in_fifo);\n\tkfree(in_fifo);\n\n\tif (out_ep) {\n\t\tmutex_lock(&chan->lock);\n\t\tchan->out_ep = NULL;\n\t\tmutex_unlock(&chan->lock);\n\t}\n\nlate_unopen:\n\tif (out_ep)\n\t\tendpoint_dealloc(out_ep);\n\nunopen:\n\tmutex_lock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 0;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 0;\n\n\tmutex_unlock(&chan->lock);\n\n\tkref_put(&xdev->kref, cleanup_dev);\n\n\treturn rc;\n\nunmutex_fail:\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&chan->lock);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,9 +7,16 @@\n \tint rc;\n \tint index;\n \n+\tmutex_lock(&kref_mutex);\n+\n \trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n-\tif (rc)\n+\tif (rc) {\n+\t\tmutex_unlock(&kref_mutex);\n \t\treturn rc;\n+\t}\n+\n+\tkref_get(&xdev->kref);\n+\tmutex_unlock(&kref_mutex);\n \n \tchan = &xdev->channels[index];\n \tfilp->private_data = chan;\n@@ -44,8 +51,6 @@\n \tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n \t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n \t\tgoto unmutex_fail;\n-\n-\tkref_get(&xdev->kref);\n \n \tif (filp->f_mode & FMODE_READ)\n \t\tchan->open_for_read = 1;\n@@ -183,6 +188,7 @@\n \treturn rc;\n \n unmutex_fail:\n+\tkref_put(&xdev->kref, cleanup_dev);\n \tmutex_unlock(&chan->lock);\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&kref_mutex);",
                "",
                "\tif (rc) {",
                "\t\tmutex_unlock(&kref_mutex);",
                "\t}",
                "",
                "\tkref_get(&xdev->kref);",
                "\tmutex_unlock(&kref_mutex);",
                "\tkref_put(&xdev->kref, cleanup_dev);"
            ],
            "deleted": [
                "\tif (rc)",
                "",
                "\tkref_get(&xdev->kref);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/char/xillybus/xillyusb.c has a race condition and use-after-free during physical removal of a USB device.",
        "id": 3753
    },
    {
        "cve_id": "CVE-2016-10088",
        "code_before_change": "static ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tstatic char cmd[TASK_COMM_LEN];\n\t\tif (strcmp(current->comm, cmd)) {\n\t\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t\t   current->comm);\n\t\t\tstrcpy(cmd, current->comm);\n\t\t}\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}",
        "code_after_change": "static ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n\t\treturn -EINVAL;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tstatic char cmd[TASK_COMM_LEN];\n\t\tif (strcmp(current->comm, cmd)) {\n\t\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t\t   current->comm);\n\t\t\tstrcpy(cmd, current->comm);\n\t\t}\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,9 @@\n \tstruct sg_header old_hdr;\n \tsg_io_hdr_t *hp;\n \tunsigned char cmnd[SG_MAX_CDB_SIZE];\n+\n+\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n+\t\treturn -EINVAL;\n \n \tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n \t\treturn -ENXIO;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The sg implementation in the Linux kernel through 4.9 does not properly restrict write operations in situations where the KERNEL_DS option is set, which allows local users to read or write to arbitrary kernel memory locations or cause a denial of service (use-after-free) by leveraging access to a /dev/sg device, related to block/bsg.c and drivers/scsi/sg.c.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-9576.",
        "id": 894
    },
    {
        "cve_id": "CVE-2019-2213",
        "code_before_change": "static void binder_free_transaction(struct binder_transaction *t)\n{\n\tif (t->buffer)\n\t\tt->buffer->transaction = NULL;\n\tbinder_free_txn_fixups(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n}",
        "code_after_change": "static void binder_free_transaction(struct binder_transaction *t)\n{\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tif (target_proc) {\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (t->buffer)\n\t\t\tt->buffer->transaction = NULL;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t}\n\t/*\n\t * If the transaction has no target_proc, then\n\t * t->buffer->transaction has already been cleared.\n\t */\n\tbinder_free_txn_fixups(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,17 @@\n static void binder_free_transaction(struct binder_transaction *t)\n {\n-\tif (t->buffer)\n-\t\tt->buffer->transaction = NULL;\n+\tstruct binder_proc *target_proc = t->to_proc;\n+\n+\tif (target_proc) {\n+\t\tbinder_inner_proc_lock(target_proc);\n+\t\tif (t->buffer)\n+\t\t\tt->buffer->transaction = NULL;\n+\t\tbinder_inner_proc_unlock(target_proc);\n+\t}\n+\t/*\n+\t * If the transaction has no target_proc, then\n+\t * t->buffer->transaction has already been cleared.\n+\t */\n \tbinder_free_txn_fixups(t);\n \tkfree(t);\n \tbinder_stats_deleted(BINDER_STAT_TRANSACTION);",
        "function_modified_lines": {
            "added": [
                "\tstruct binder_proc *target_proc = t->to_proc;",
                "",
                "\tif (target_proc) {",
                "\t\tbinder_inner_proc_lock(target_proc);",
                "\t\tif (t->buffer)",
                "\t\t\tt->buffer->transaction = NULL;",
                "\t\tbinder_inner_proc_unlock(target_proc);",
                "\t}",
                "\t/*",
                "\t * If the transaction has no target_proc, then",
                "\t * t->buffer->transaction has already been cleared.",
                "\t */"
            ],
            "deleted": [
                "\tif (t->buffer)",
                "\t\tt->buffer->transaction = NULL;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In binder_free_transaction of binder.c, there is a possible use-after-free due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-133758011References: Upstream kernel",
        "id": 2294
    },
    {
        "cve_id": "CVE-2022-24122",
        "code_before_change": "struct ucounts *alloc_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\tbool wrapped;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 1);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tspin_unlock_irq(&ucounts_lock);\n\t\t\treturn new;\n\t\t}\n\t}\n\twrapped = !get_ucounts_or_wrap(ucounts);\n\tspin_unlock_irq(&ucounts_lock);\n\tif (wrapped) {\n\t\tput_ucounts(ucounts);\n\t\treturn NULL;\n\t}\n\treturn ucounts;\n}",
        "code_after_change": "struct ucounts *alloc_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\tbool wrapped;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 1);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tget_user_ns(new->ns);\n\t\t\tspin_unlock_irq(&ucounts_lock);\n\t\t\treturn new;\n\t\t}\n\t}\n\twrapped = !get_ucounts_or_wrap(ucounts);\n\tspin_unlock_irq(&ucounts_lock);\n\tif (wrapped) {\n\t\tput_ucounts(ucounts);\n\t\treturn NULL;\n\t}\n\treturn ucounts;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,6 +23,7 @@\n \t\t\tkfree(new);\n \t\t} else {\n \t\t\thlist_add_head(&new->node, hashent);\n+\t\t\tget_user_ns(new->ns);\n \t\t\tspin_unlock_irq(&ucounts_lock);\n \t\t\treturn new;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tget_user_ns(new->ns);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "kernel/ucount.c in the Linux kernel 5.14 through 5.16.4, when unprivileged user namespaces are enabled, allows a use-after-free and privilege escalation because a ucounts object can outlive its namespace.",
        "id": 3470
    },
    {
        "cve_id": "CVE-2017-2584",
        "code_before_change": "static int em_fxrstor(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tint rc;\n\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = segmented_read(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (fx_state.mxcsr >> 16)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tctxt->ops->get_fpu(ctxt);\n\n\tif (ctxt->mode < X86EMUL_MODE_PROT64)\n\t\trc = fxrstor_fixup(ctxt, &fx_state);\n\n\tif (rc == X86EMUL_CONTINUE)\n\t\trc = asm_safe(\"fxrstor %[fx]\", : [fx] \"m\"(fx_state));\n\n\tctxt->ops->put_fpu(ctxt);\n\n\treturn rc;\n}",
        "code_after_change": "static int em_fxrstor(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tint rc;\n\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = segmented_read_std(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (fx_state.mxcsr >> 16)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tctxt->ops->get_fpu(ctxt);\n\n\tif (ctxt->mode < X86EMUL_MODE_PROT64)\n\t\trc = fxrstor_fixup(ctxt, &fx_state);\n\n\tif (rc == X86EMUL_CONTINUE)\n\t\trc = asm_safe(\"fxrstor %[fx]\", : [fx] \"m\"(fx_state));\n\n\tctxt->ops->put_fpu(ctxt);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tif (rc != X86EMUL_CONTINUE)\n \t\treturn rc;\n \n-\trc = segmented_read(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n+\trc = segmented_read_std(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n \tif (rc != X86EMUL_CONTINUE)\n \t\treturn rc;\n ",
        "function_modified_lines": {
            "added": [
                "\trc = segmented_read_std(ctxt, ctxt->memop.addr.mem, &fx_state, 512);"
            ],
            "deleted": [
                "\trc = segmented_read(ctxt, ctxt->memop.addr.mem, &fx_state, 512);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-416"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel through 4.9.3 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free) via a crafted application that leverages instruction emulation for fxrstor, fxsave, sgdt, and sidt.",
        "id": 1444
    },
    {
        "cve_id": "CVE-2018-17182",
        "code_before_change": "static inline void vmacache_invalidate(struct mm_struct *mm)\n{\n\tmm->vmacache_seqnum++;\n\n\t/* deal with overflows */\n\tif (unlikely(mm->vmacache_seqnum == 0))\n\t\tvmacache_flush_all(mm);\n}",
        "code_after_change": "static inline void vmacache_invalidate(struct mm_struct *mm)\n{\n\tmm->vmacache_seqnum++;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,4 @@\n static inline void vmacache_invalidate(struct mm_struct *mm)\n {\n \tmm->vmacache_seqnum++;\n-\n-\t/* deal with overflows */\n-\tif (unlikely(mm->vmacache_seqnum == 0))\n-\t\tvmacache_flush_all(mm);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\t/* deal with overflows */",
                "\tif (unlikely(mm->vmacache_seqnum == 0))",
                "\t\tvmacache_flush_all(mm);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 4.18.8. The vmacache_flush_all function in mm/vmacache.c mishandles sequence number overflows. An attacker can trigger a use-after-free (and possibly gain privileges) via certain thread creation, map, unmap, invalidation, and dereference operations.",
        "id": 1726
    },
    {
        "cve_id": "CVE-2016-8655",
        "code_before_change": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n\tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n\t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n\t\tgoto out;\n\t}\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t/* Transmit path is not supported. We checked\n\t\t * it above but just being paranoid\n\t\t */\n\t\t\tif (!tx_ring)\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\trelease_sock(sk);\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\treturn err;\n}",
        "code_after_change": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\tlock_sock(sk);\n\t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n\tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n\t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n\t\tgoto out;\n\t}\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t/* Transmit path is not supported. We checked\n\t\t * it above but just being paranoid\n\t\t */\n\t\t\tif (!tx_ring)\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \t/* Added to avoid minimal code churn */\n \tstruct tpacket_req *req = &req_u->req;\n \n+\tlock_sock(sk);\n \t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n \tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n \t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n@@ -92,7 +93,6 @@\n \t\t\tgoto out;\n \t}\n \n-\tlock_sock(sk);\n \n \t/* Detach socket from network */\n \tspin_lock(&po->bind_lock);\n@@ -141,10 +141,10 @@\n \t\tif (!tx_ring)\n \t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n \t}\n-\trelease_sock(sk);\n \n \tif (pg_vec)\n \t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\n out:\n+\trelease_sock(sk);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tlock_sock(sk);",
                "\trelease_sock(sk);"
            ],
            "deleted": [
                "\tlock_sock(sk);",
                "\trelease_sock(sk);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel through 4.8.12 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging the CAP_NET_RAW capability to change a socket version, related to the packet_set_ring and packet_setsockopt functions.",
        "id": 1131
    },
    {
        "cve_id": "CVE-2020-15436",
        "code_before_change": "int blkdev_get(struct block_device *bdev, fmode_t mode, void *holder)\n{\n\tstruct block_device *whole = NULL;\n\tint res;\n\n\tWARN_ON_ONCE((mode & FMODE_EXCL) && !holder);\n\n\tif ((mode & FMODE_EXCL) && holder) {\n\t\twhole = bd_start_claiming(bdev, holder);\n\t\tif (IS_ERR(whole)) {\n\t\t\tbdput(bdev);\n\t\t\treturn PTR_ERR(whole);\n\t\t}\n\t}\n\n\tres = __blkdev_get(bdev, mode, 0);\n\n\tif (whole) {\n\t\tstruct gendisk *disk = whole->bd_disk;\n\n\t\t/* finish claiming */\n\t\tmutex_lock(&bdev->bd_mutex);\n\t\tif (!res)\n\t\t\tbd_finish_claiming(bdev, whole, holder);\n\t\telse\n\t\t\tbd_abort_claiming(bdev, whole, holder);\n\t\t/*\n\t\t * Block event polling for write claims if requested.  Any\n\t\t * write holder makes the write_holder state stick until\n\t\t * all are released.  This is good enough and tracking\n\t\t * individual writeable reference is too fragile given the\n\t\t * way @mode is used in blkdev_get/put().\n\t\t */\n\t\tif (!res && (mode & FMODE_WRITE) && !bdev->bd_write_holder &&\n\t\t    (disk->flags & GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE)) {\n\t\t\tbdev->bd_write_holder = true;\n\t\t\tdisk_block_events(disk);\n\t\t}\n\n\t\tmutex_unlock(&bdev->bd_mutex);\n\t\tbdput(whole);\n\t}\n\n\treturn res;\n}",
        "code_after_change": "int blkdev_get(struct block_device *bdev, fmode_t mode, void *holder)\n{\n\tstruct block_device *whole = NULL;\n\tint res;\n\n\tWARN_ON_ONCE((mode & FMODE_EXCL) && !holder);\n\n\tif ((mode & FMODE_EXCL) && holder) {\n\t\twhole = bd_start_claiming(bdev, holder);\n\t\tif (IS_ERR(whole)) {\n\t\t\tbdput(bdev);\n\t\t\treturn PTR_ERR(whole);\n\t\t}\n\t}\n\n\tres = __blkdev_get(bdev, mode, 0);\n\n\tif (whole) {\n\t\tstruct gendisk *disk = whole->bd_disk;\n\n\t\t/* finish claiming */\n\t\tmutex_lock(&bdev->bd_mutex);\n\t\tif (!res)\n\t\t\tbd_finish_claiming(bdev, whole, holder);\n\t\telse\n\t\t\tbd_abort_claiming(bdev, whole, holder);\n\t\t/*\n\t\t * Block event polling for write claims if requested.  Any\n\t\t * write holder makes the write_holder state stick until\n\t\t * all are released.  This is good enough and tracking\n\t\t * individual writeable reference is too fragile given the\n\t\t * way @mode is used in blkdev_get/put().\n\t\t */\n\t\tif (!res && (mode & FMODE_WRITE) && !bdev->bd_write_holder &&\n\t\t    (disk->flags & GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE)) {\n\t\t\tbdev->bd_write_holder = true;\n\t\t\tdisk_block_events(disk);\n\t\t}\n\n\t\tmutex_unlock(&bdev->bd_mutex);\n\t\tbdput(whole);\n\t}\n\n\tif (res)\n\t\tbdput(bdev);\n\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -41,5 +41,8 @@\n \t\tbdput(whole);\n \t}\n \n+\tif (res)\n+\t\tbdput(bdev);\n+\n \treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (res)",
                "\t\tbdput(bdev);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/block_dev.c in the Linux kernel before 5.8 allows local users to gain privileges or cause a denial of service by leveraging improper access to a certain error field.",
        "id": 2544
    },
    {
        "cve_id": "CVE-2017-10661",
        "code_before_change": "static void timerfd_setup_cancel(struct timerfd_ctx *ctx, int flags)\n{\n\tif ((ctx->clockid == CLOCK_REALTIME ||\n\t     ctx->clockid == CLOCK_REALTIME_ALARM) &&\n\t    (flags & TFD_TIMER_ABSTIME) && (flags & TFD_TIMER_CANCEL_ON_SET)) {\n\t\tif (!ctx->might_cancel) {\n\t\t\tctx->might_cancel = true;\n\t\t\tspin_lock(&cancel_lock);\n\t\t\tlist_add_rcu(&ctx->clist, &cancel_list);\n\t\t\tspin_unlock(&cancel_lock);\n\t\t}\n\t} else if (ctx->might_cancel) {\n\t\ttimerfd_remove_cancel(ctx);\n\t}\n}",
        "code_after_change": "static void timerfd_setup_cancel(struct timerfd_ctx *ctx, int flags)\n{\n\tspin_lock(&ctx->cancel_lock);\n\tif ((ctx->clockid == CLOCK_REALTIME ||\n\t     ctx->clockid == CLOCK_REALTIME_ALARM) &&\n\t    (flags & TFD_TIMER_ABSTIME) && (flags & TFD_TIMER_CANCEL_ON_SET)) {\n\t\tif (!ctx->might_cancel) {\n\t\t\tctx->might_cancel = true;\n\t\t\tspin_lock(&cancel_lock);\n\t\t\tlist_add_rcu(&ctx->clist, &cancel_list);\n\t\t\tspin_unlock(&cancel_lock);\n\t\t}\n\t} else {\n\t\t__timerfd_remove_cancel(ctx);\n\t}\n\tspin_unlock(&ctx->cancel_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static void timerfd_setup_cancel(struct timerfd_ctx *ctx, int flags)\n {\n+\tspin_lock(&ctx->cancel_lock);\n \tif ((ctx->clockid == CLOCK_REALTIME ||\n \t     ctx->clockid == CLOCK_REALTIME_ALARM) &&\n \t    (flags & TFD_TIMER_ABSTIME) && (flags & TFD_TIMER_CANCEL_ON_SET)) {\n@@ -9,7 +10,8 @@\n \t\t\tlist_add_rcu(&ctx->clist, &cancel_list);\n \t\t\tspin_unlock(&cancel_lock);\n \t\t}\n-\t} else if (ctx->might_cancel) {\n-\t\ttimerfd_remove_cancel(ctx);\n+\t} else {\n+\t\t__timerfd_remove_cancel(ctx);\n \t}\n+\tspin_unlock(&ctx->cancel_lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tspin_lock(&ctx->cancel_lock);",
                "\t} else {",
                "\t\t__timerfd_remove_cancel(ctx);",
                "\tspin_unlock(&ctx->cancel_lock);"
            ],
            "deleted": [
                "\t} else if (ctx->might_cancel) {",
                "\t\ttimerfd_remove_cancel(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Race condition in fs/timerfd.c in the Linux kernel before 4.10.15 allows local users to gain privileges or cause a denial of service (list corruption or use-after-free) via simultaneous file-descriptor operations that leverage improper might_cancel queueing.",
        "id": 1242
    },
    {
        "cve_id": "CVE-2023-0468",
        "code_before_change": "static inline bool io_poll_get_ownership(struct io_kiocb *req)\n{\n\treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n}",
        "code_after_change": "static inline bool io_poll_get_ownership(struct io_kiocb *req)\n{\n\tif (unlikely(atomic_read(&req->poll_refs) >= IO_POLL_REF_BIAS))\n\t\treturn io_poll_get_ownership_slowpath(req);\n\treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,6 @@\n static inline bool io_poll_get_ownership(struct io_kiocb *req)\n {\n+\tif (unlikely(atomic_read(&req->poll_refs) >= IO_POLL_REF_BIAS))\n+\t\treturn io_poll_get_ownership_slowpath(req);\n \treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(atomic_read(&req->poll_refs) >= IO_POLL_REF_BIAS))",
                "\t\treturn io_poll_get_ownership_slowpath(req);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in io_uring/poll.c in io_poll_check_events in the io_uring subcomponent in the Linux Kernel due to a race condition of poll_refs. This flaw may cause a NULL pointer dereference.",
        "id": 3831
    },
    {
        "cve_id": "CVE-2022-22942",
        "code_before_change": "void vmw_kms_helper_validation_finish(struct vmw_private *dev_priv,\n\t\t\t\t      struct drm_file *file_priv,\n\t\t\t\t      struct vmw_validation_context *ctx,\n\t\t\t\t      struct vmw_fence_obj **out_fence,\n\t\t\t\t      struct drm_vmw_fence_rep __user *\n\t\t\t\t      user_fence_rep)\n{\n\tstruct vmw_fence_obj *fence = NULL;\n\tuint32_t handle = 0;\n\tint ret = 0;\n\n\tif (file_priv || user_fence_rep || vmw_validation_has_bos(ctx) ||\n\t    out_fence)\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv, &fence,\n\t\t\t\t\t\t file_priv ? &handle : NULL);\n\tvmw_validation_done(ctx, fence);\n\tif (file_priv)\n\t\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv),\n\t\t\t\t\t    ret, user_fence_rep, fence,\n\t\t\t\t\t    handle, -1, NULL);\n\tif (out_fence)\n\t\t*out_fence = fence;\n\telse\n\t\tvmw_fence_obj_unreference(&fence);\n}",
        "code_after_change": "void vmw_kms_helper_validation_finish(struct vmw_private *dev_priv,\n\t\t\t\t      struct drm_file *file_priv,\n\t\t\t\t      struct vmw_validation_context *ctx,\n\t\t\t\t      struct vmw_fence_obj **out_fence,\n\t\t\t\t      struct drm_vmw_fence_rep __user *\n\t\t\t\t      user_fence_rep)\n{\n\tstruct vmw_fence_obj *fence = NULL;\n\tuint32_t handle = 0;\n\tint ret = 0;\n\n\tif (file_priv || user_fence_rep || vmw_validation_has_bos(ctx) ||\n\t    out_fence)\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv, &fence,\n\t\t\t\t\t\t file_priv ? &handle : NULL);\n\tvmw_validation_done(ctx, fence);\n\tif (file_priv)\n\t\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv),\n\t\t\t\t\t    ret, user_fence_rep, fence,\n\t\t\t\t\t    handle, -1);\n\tif (out_fence)\n\t\t*out_fence = fence;\n\telse\n\t\tvmw_fence_obj_unreference(&fence);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \tif (file_priv)\n \t\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv),\n \t\t\t\t\t    ret, user_fence_rep, fence,\n-\t\t\t\t\t    handle, -1, NULL);\n+\t\t\t\t\t    handle, -1);\n \tif (out_fence)\n \t\t*out_fence = fence;\n \telse",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t    handle, -1);"
            ],
            "deleted": [
                "\t\t\t\t\t    handle, -1, NULL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The vmwgfx driver contains a local privilege escalation vulnerability that allows unprivileged users to gain access to files opened by other processes on the system through a dangling 'file' pointer.",
        "id": 3418
    },
    {
        "cve_id": "CVE-2022-22942",
        "code_before_change": "int vmw_fence_event_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct drm_vmw_fence_event_arg *arg =\n\t\t(struct drm_vmw_fence_event_arg *) data;\n\tstruct vmw_fence_obj *fence = NULL;\n\tstruct vmw_fpriv *vmw_fp = vmw_fpriv(file_priv);\n\tstruct ttm_object_file *tfile = vmw_fp->tfile;\n\tstruct drm_vmw_fence_rep __user *user_fence_rep =\n\t\t(struct drm_vmw_fence_rep __user *)(unsigned long)\n\t\targ->fence_rep;\n\tuint32_t handle;\n\tint ret;\n\n\t/*\n\t * Look up an existing fence object,\n\t * and if user-space wants a new reference,\n\t * add one.\n\t */\n\tif (arg->handle) {\n\t\tstruct ttm_base_object *base =\n\t\t\tvmw_fence_obj_lookup(tfile, arg->handle);\n\n\t\tif (IS_ERR(base))\n\t\t\treturn PTR_ERR(base);\n\n\t\tfence = &(container_of(base, struct vmw_user_fence,\n\t\t\t\t       base)->fence);\n\t\t(void) vmw_fence_obj_reference(fence);\n\n\t\tif (user_fence_rep != NULL) {\n\t\t\tret = ttm_ref_object_add(vmw_fp->tfile, base,\n\t\t\t\t\t\t NULL, false);\n\t\t\tif (unlikely(ret != 0)) {\n\t\t\t\tDRM_ERROR(\"Failed to reference a fence \"\n\t\t\t\t\t  \"object.\\n\");\n\t\t\t\tgoto out_no_ref_obj;\n\t\t\t}\n\t\t\thandle = base->handle;\n\t\t}\n\t\tttm_base_object_unref(&base);\n\t}\n\n\t/*\n\t * Create a new fence object.\n\t */\n\tif (!fence) {\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv,\n\t\t\t\t\t\t &fence,\n\t\t\t\t\t\t (user_fence_rep) ?\n\t\t\t\t\t\t &handle : NULL);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Fence event failed to create fence.\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tBUG_ON(fence == NULL);\n\n\tret = vmw_event_fence_action_create(file_priv, fence,\n\t\t\t\t\t    arg->flags,\n\t\t\t\t\t    arg->user_data,\n\t\t\t\t\t    true);\n\tif (unlikely(ret != 0)) {\n\t\tif (ret != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"Failed to attach event to fence.\\n\");\n\t\tgoto out_no_create;\n\t}\n\n\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fp, 0, user_fence_rep, fence,\n\t\t\t\t    handle, -1, NULL);\n\tvmw_fence_obj_unreference(&fence);\n\treturn 0;\nout_no_create:\n\tif (user_fence_rep != NULL)\n\t\tttm_ref_object_base_unref(tfile, handle);\nout_no_ref_obj:\n\tvmw_fence_obj_unreference(&fence);\n\treturn ret;\n}",
        "code_after_change": "int vmw_fence_event_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct drm_vmw_fence_event_arg *arg =\n\t\t(struct drm_vmw_fence_event_arg *) data;\n\tstruct vmw_fence_obj *fence = NULL;\n\tstruct vmw_fpriv *vmw_fp = vmw_fpriv(file_priv);\n\tstruct ttm_object_file *tfile = vmw_fp->tfile;\n\tstruct drm_vmw_fence_rep __user *user_fence_rep =\n\t\t(struct drm_vmw_fence_rep __user *)(unsigned long)\n\t\targ->fence_rep;\n\tuint32_t handle;\n\tint ret;\n\n\t/*\n\t * Look up an existing fence object,\n\t * and if user-space wants a new reference,\n\t * add one.\n\t */\n\tif (arg->handle) {\n\t\tstruct ttm_base_object *base =\n\t\t\tvmw_fence_obj_lookup(tfile, arg->handle);\n\n\t\tif (IS_ERR(base))\n\t\t\treturn PTR_ERR(base);\n\n\t\tfence = &(container_of(base, struct vmw_user_fence,\n\t\t\t\t       base)->fence);\n\t\t(void) vmw_fence_obj_reference(fence);\n\n\t\tif (user_fence_rep != NULL) {\n\t\t\tret = ttm_ref_object_add(vmw_fp->tfile, base,\n\t\t\t\t\t\t NULL, false);\n\t\t\tif (unlikely(ret != 0)) {\n\t\t\t\tDRM_ERROR(\"Failed to reference a fence \"\n\t\t\t\t\t  \"object.\\n\");\n\t\t\t\tgoto out_no_ref_obj;\n\t\t\t}\n\t\t\thandle = base->handle;\n\t\t}\n\t\tttm_base_object_unref(&base);\n\t}\n\n\t/*\n\t * Create a new fence object.\n\t */\n\tif (!fence) {\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv,\n\t\t\t\t\t\t &fence,\n\t\t\t\t\t\t (user_fence_rep) ?\n\t\t\t\t\t\t &handle : NULL);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Fence event failed to create fence.\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tBUG_ON(fence == NULL);\n\n\tret = vmw_event_fence_action_create(file_priv, fence,\n\t\t\t\t\t    arg->flags,\n\t\t\t\t\t    arg->user_data,\n\t\t\t\t\t    true);\n\tif (unlikely(ret != 0)) {\n\t\tif (ret != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"Failed to attach event to fence.\\n\");\n\t\tgoto out_no_create;\n\t}\n\n\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fp, 0, user_fence_rep, fence,\n\t\t\t\t    handle, -1);\n\tvmw_fence_obj_unreference(&fence);\n\treturn 0;\nout_no_create:\n\tif (user_fence_rep != NULL)\n\t\tttm_ref_object_base_unref(tfile, handle);\nout_no_ref_obj:\n\tvmw_fence_obj_unreference(&fence);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -69,7 +69,7 @@\n \t}\n \n \tvmw_execbuf_copy_fence_user(dev_priv, vmw_fp, 0, user_fence_rep, fence,\n-\t\t\t\t    handle, -1, NULL);\n+\t\t\t\t    handle, -1);\n \tvmw_fence_obj_unreference(&fence);\n \treturn 0;\n out_no_create:",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t    handle, -1);"
            ],
            "deleted": [
                "\t\t\t\t    handle, -1, NULL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The vmwgfx driver contains a local privilege escalation vulnerability that allows unprivileged users to gain access to files opened by other processes on the system through a dangling 'file' pointer.",
        "id": 3417
    },
    {
        "cve_id": "CVE-2022-41849",
        "code_before_change": "static void ufx_usb_disconnect(struct usb_interface *interface)\n{\n\tstruct ufx_data *dev;\n\n\tdev = usb_get_intfdata(interface);\n\n\tpr_debug(\"USB disconnect starting\\n\");\n\n\t/* we virtualize until all fb clients release. Then we free */\n\tdev->virtualized = true;\n\n\t/* When non-active we'll update virtual framebuffer, but no new urbs */\n\tatomic_set(&dev->usb_active, 0);\n\n\tusb_set_intfdata(interface, NULL);\n\n\t/* if clients still have us open, will be freed on last close */\n\tif (dev->fb_count == 0)\n\t\tschedule_delayed_work(&dev->free_framebuffer_work, 0);\n\n\t/* release reference taken by kref_init in probe() */\n\tkref_put(&dev->kref, ufx_free);\n\n\t/* consider ufx_data freed */\n}",
        "code_after_change": "static void ufx_usb_disconnect(struct usb_interface *interface)\n{\n\tstruct ufx_data *dev;\n\n\tmutex_lock(&disconnect_mutex);\n\n\tdev = usb_get_intfdata(interface);\n\n\tpr_debug(\"USB disconnect starting\\n\");\n\n\t/* we virtualize until all fb clients release. Then we free */\n\tdev->virtualized = true;\n\n\t/* When non-active we'll update virtual framebuffer, but no new urbs */\n\tatomic_set(&dev->usb_active, 0);\n\n\tusb_set_intfdata(interface, NULL);\n\n\t/* if clients still have us open, will be freed on last close */\n\tif (dev->fb_count == 0)\n\t\tschedule_delayed_work(&dev->free_framebuffer_work, 0);\n\n\t/* release reference taken by kref_init in probe() */\n\tkref_put(&dev->kref, ufx_free);\n\n\t/* consider ufx_data freed */\n\n\tmutex_unlock(&disconnect_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n static void ufx_usb_disconnect(struct usb_interface *interface)\n {\n \tstruct ufx_data *dev;\n+\n+\tmutex_lock(&disconnect_mutex);\n \n \tdev = usb_get_intfdata(interface);\n \n@@ -22,4 +24,6 @@\n \tkref_put(&dev->kref, ufx_free);\n \n \t/* consider ufx_data freed */\n+\n+\tmutex_unlock(&disconnect_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tmutex_lock(&disconnect_mutex);",
                "",
                "\tmutex_unlock(&disconnect_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "drivers/video/fbdev/smscufx.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free if a physically proximate attacker removes a USB device while calling open(), aka a race condition between ufx_ops_open and ufx_usb_disconnect.",
        "id": 3720
    },
    {
        "cve_id": "CVE-2021-28691",
        "code_before_change": "static void xenvif_disconnect_queue(struct xenvif_queue *queue)\n{\n\tif (queue->task) {\n\t\tkthread_stop(queue->task);\n\t\tqueue->task = NULL;\n\t}\n\n\tif (queue->dealloc_task) {\n\t\tkthread_stop(queue->dealloc_task);\n\t\tqueue->dealloc_task = NULL;\n\t}\n\n\tif (queue->napi.poll) {\n\t\tnetif_napi_del(&queue->napi);\n\t\tqueue->napi.poll = NULL;\n\t}\n\n\tif (queue->tx_irq) {\n\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\tif (queue->tx_irq == queue->rx_irq)\n\t\t\tqueue->rx_irq = 0;\n\t\tqueue->tx_irq = 0;\n\t}\n\n\tif (queue->rx_irq) {\n\t\tunbind_from_irqhandler(queue->rx_irq, queue);\n\t\tqueue->rx_irq = 0;\n\t}\n\n\txenvif_unmap_frontend_data_rings(queue);\n}",
        "code_after_change": "static void xenvif_disconnect_queue(struct xenvif_queue *queue)\n{\n\tif (queue->task) {\n\t\tkthread_stop(queue->task);\n\t\tput_task_struct(queue->task);\n\t\tqueue->task = NULL;\n\t}\n\n\tif (queue->dealloc_task) {\n\t\tkthread_stop(queue->dealloc_task);\n\t\tqueue->dealloc_task = NULL;\n\t}\n\n\tif (queue->napi.poll) {\n\t\tnetif_napi_del(&queue->napi);\n\t\tqueue->napi.poll = NULL;\n\t}\n\n\tif (queue->tx_irq) {\n\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\tif (queue->tx_irq == queue->rx_irq)\n\t\t\tqueue->rx_irq = 0;\n\t\tqueue->tx_irq = 0;\n\t}\n\n\tif (queue->rx_irq) {\n\t\tunbind_from_irqhandler(queue->rx_irq, queue);\n\t\tqueue->rx_irq = 0;\n\t}\n\n\txenvif_unmap_frontend_data_rings(queue);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tif (queue->task) {\n \t\tkthread_stop(queue->task);\n+\t\tput_task_struct(queue->task);\n \t\tqueue->task = NULL;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tput_task_struct(queue->task);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Guest triggered use-after-free in Linux xen-netback A malicious or buggy network PV frontend can force Linux netback to disable the interface and terminate the receive kernel thread associated with queue 0 in response to the frontend sending a malformed packet. Such kernel thread termination will lead to a use-after-free in Linux netback when the backend is destroyed, as the kernel thread associated with queue 0 will have already exited and thus the call to kthread_stop will be performed against a stale pointer.",
        "id": 2917
    },
    {
        "cve_id": "CVE-2022-2602",
        "code_before_change": "int __io_scm_file_account(struct io_ring_ctx *ctx, struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sk->sk_receive_queue;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\n\tif (likely(!io_file_need_scm(file)))\n\t\treturn 0;\n\n\t/*\n\t * See if we can merge this file into an existing skb SCM_RIGHTS\n\t * file set. If there's no room, fall back to allocating a new skb\n\t * and filling it in.\n\t */\n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb && UNIXCB(skb).fp->count < SCM_MAX_FD)\n\t\t__skb_unlink(skb, head);\n\telse\n\t\tskb = NULL;\n\tspin_unlock_irq(&head->lock);\n\n\tif (!skb) {\n\t\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\t\tif (!fpl)\n\t\t\treturn -ENOMEM;\n\n\t\tskb = alloc_skb(0, GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\tkfree(fpl);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfpl->user = get_uid(current_user());\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = 0;\n\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->sk = sk;\n\t\tskb->destructor = unix_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t}\n\n\tfpl = UNIXCB(skb).fp;\n\tfpl->fp[fpl->count++] = get_file(file);\n\tunix_inflight(fpl->user, file);\n\tskb_queue_head(head, skb);\n\tfput(file);\n#endif\n\treturn 0;\n}",
        "code_after_change": "int __io_scm_file_account(struct io_ring_ctx *ctx, struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sk->sk_receive_queue;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\n\tif (likely(!io_file_need_scm(file)))\n\t\treturn 0;\n\n\t/*\n\t * See if we can merge this file into an existing skb SCM_RIGHTS\n\t * file set. If there's no room, fall back to allocating a new skb\n\t * and filling it in.\n\t */\n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb && UNIXCB(skb).fp->count < SCM_MAX_FD)\n\t\t__skb_unlink(skb, head);\n\telse\n\t\tskb = NULL;\n\tspin_unlock_irq(&head->lock);\n\n\tif (!skb) {\n\t\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\t\tif (!fpl)\n\t\t\treturn -ENOMEM;\n\n\t\tskb = alloc_skb(0, GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\tkfree(fpl);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfpl->user = get_uid(current_user());\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = 0;\n\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->sk = sk;\n\t\tskb->scm_io_uring = 1;\n\t\tskb->destructor = unix_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t}\n\n\tfpl = UNIXCB(skb).fp;\n\tfpl->fp[fpl->count++] = get_file(file);\n\tunix_inflight(fpl->user, file);\n\tskb_queue_head(head, skb);\n\tfput(file);\n#endif\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,6 +39,7 @@\n \n \t\tUNIXCB(skb).fp = fpl;\n \t\tskb->sk = sk;\n+\t\tskb->scm_io_uring = 1;\n \t\tskb->destructor = unix_destruct_scm;\n \t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tskb->scm_io_uring = 1;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "io_uring UAF, Unix SCM garbage collection",
        "id": 3483
    },
    {
        "cve_id": "CVE-2022-1055",
        "code_before_change": "static int tc_new_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tchar name[IFNAMSIZ];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tbool prio_allocate;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp;\n\tunsigned long cl;\n\tvoid *fh;\n\tint err;\n\tint tp_created;\n\tbool rtnl_held = false;\n\tu32 flags;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\ttp_created = 0;\n\n\terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n\t\t\t\t     rtm_tca_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tprio_allocate = false;\n\tparent = t->tcm_parent;\n\ttp = NULL;\n\tcl = 0;\n\tblock = NULL;\n\tflags = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is provided by the user,\n\t\t * we allocate one.\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_CREATE) {\n\t\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t\t\tprio_allocate = true;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\t/* Find head of filter chain. */\n\n\terr = __tcf_qdisc_find(net, &q, &parent, t->tcm_ifindex, false, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (tcf_proto_check_kind(tca[TCA_KIND], name)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified TC filter name too long\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\t/* Take rtnl mutex if rtnl_held was set to true on previous iteration,\n\t * block is shared (no qdisc found), qdisc is not unlocked, classifier\n\t * type is not specified, classifier is not unlocked.\n\t */\n\tif (rtnl_held ||\n\t    (q && !(q->ops->cl_ops->flags & QDISC_CLASS_OPS_DOIT_UNLOCKED)) ||\n\t    !tcf_proto_is_unlocked(name)) {\n\t\trtnl_held = true;\n\t\trtnl_lock();\n\t}\n\n\terr = __tcf_qdisc_cl_find(q, parent, &cl, t->tcm_ifindex, extack);\n\tif (err)\n\t\tgoto errout;\n\n\tblock = __tcf_block_find(net, q, cl, t->tcm_ifindex, t->tcm_block_index,\n\t\t\t\t extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\tblock->classid = parent;\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, true);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot create specified filter chain\");\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\tmutex_lock(&chain->filter_chain_lock);\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, prio_allocate);\n\tif (IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = PTR_ERR(tp);\n\t\tgoto errout_locked;\n\t}\n\n\tif (tp == NULL) {\n\t\tstruct tcf_proto *tp_new = NULL;\n\n\t\tif (chain->flushing) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Filter kind and protocol must be specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (prio_allocate)\n\t\t\tprio = tcf_auto_prio(tcf_chain_tp_prev(chain,\n\t\t\t\t\t\t\t       &chain_info));\n\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t\ttp_new = tcf_proto_create(name, protocol, prio, chain,\n\t\t\t\t\t  rtnl_held, extack);\n\t\tif (IS_ERR(tp_new)) {\n\t\t\terr = PTR_ERR(tp_new);\n\t\t\tgoto errout_tp;\n\t\t}\n\n\t\ttp_created = 1;\n\t\ttp = tcf_chain_tp_insert_unique(chain, tp_new, protocol, prio,\n\t\t\t\t\t\trtnl_held);\n\t\tif (IS_ERR(tp)) {\n\t\t\terr = PTR_ERR(tp);\n\t\t\tgoto errout_tp;\n\t\t}\n\t} else {\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\t} else if (n->nlmsg_flags & NLM_F_EXCL) {\n\t\ttfilter_put(tp, fh);\n\t\tNL_SET_ERR_MSG(extack, \"Filter already exists\");\n\t\terr = -EEXIST;\n\t\tgoto errout;\n\t}\n\n\tif (chain->tmplt_ops && chain->tmplt_ops != tp->ops) {\n\t\tNL_SET_ERR_MSG(extack, \"Chain template is set to a different filter kind\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\tflags |= TCA_ACT_FLAGS_REPLACE;\n\tif (!rtnl_held)\n\t\tflags |= TCA_ACT_FLAGS_NO_RTNL;\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh,\n\t\t\t      flags, extack);\n\tif (err == 0) {\n\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t       RTM_NEWTFILTER, false, rtnl_held);\n\t\ttfilter_put(tp, fh);\n\t\t/* q pointer is NULL for shared blocks */\n\t\tif (q)\n\t\t\tq->flags &= ~TCQ_F_CAN_BYPASS;\n\t}\n\nerrout:\n\tif (err && tp_created)\n\t\ttcf_chain_tp_delete_empty(chain, tp, rtnl_held, NULL);\nerrout_tp:\n\tif (chain) {\n\t\tif (tp && !IS_ERR(tp))\n\t\t\ttcf_proto_put(tp, rtnl_held, NULL);\n\t\tif (!tp_created)\n\t\t\ttcf_chain_put(chain);\n\t}\n\ttcf_block_release(q, block, rtnl_held);\n\n\tif (rtnl_held)\n\t\trtnl_unlock();\n\n\tif (err == -EAGAIN) {\n\t\t/* Take rtnl lock in case EAGAIN is caused by concurrent flush\n\t\t * of target chain.\n\t\t */\n\t\trtnl_held = true;\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\t}\n\treturn err;\n\nerrout_locked:\n\tmutex_unlock(&chain->filter_chain_lock);\n\tgoto errout;\n}",
        "code_after_change": "static int tc_new_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tchar name[IFNAMSIZ];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tbool prio_allocate;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp;\n\tunsigned long cl;\n\tvoid *fh;\n\tint err;\n\tint tp_created;\n\tbool rtnl_held = false;\n\tu32 flags;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\ttp_created = 0;\n\n\terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n\t\t\t\t     rtm_tca_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tprio_allocate = false;\n\tparent = t->tcm_parent;\n\ttp = NULL;\n\tcl = 0;\n\tblock = NULL;\n\tq = NULL;\n\tchain = NULL;\n\tflags = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is provided by the user,\n\t\t * we allocate one.\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_CREATE) {\n\t\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t\t\tprio_allocate = true;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\t/* Find head of filter chain. */\n\n\terr = __tcf_qdisc_find(net, &q, &parent, t->tcm_ifindex, false, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (tcf_proto_check_kind(tca[TCA_KIND], name)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified TC filter name too long\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\t/* Take rtnl mutex if rtnl_held was set to true on previous iteration,\n\t * block is shared (no qdisc found), qdisc is not unlocked, classifier\n\t * type is not specified, classifier is not unlocked.\n\t */\n\tif (rtnl_held ||\n\t    (q && !(q->ops->cl_ops->flags & QDISC_CLASS_OPS_DOIT_UNLOCKED)) ||\n\t    !tcf_proto_is_unlocked(name)) {\n\t\trtnl_held = true;\n\t\trtnl_lock();\n\t}\n\n\terr = __tcf_qdisc_cl_find(q, parent, &cl, t->tcm_ifindex, extack);\n\tif (err)\n\t\tgoto errout;\n\n\tblock = __tcf_block_find(net, q, cl, t->tcm_ifindex, t->tcm_block_index,\n\t\t\t\t extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\tblock->classid = parent;\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, true);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot create specified filter chain\");\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\tmutex_lock(&chain->filter_chain_lock);\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, prio_allocate);\n\tif (IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = PTR_ERR(tp);\n\t\tgoto errout_locked;\n\t}\n\n\tif (tp == NULL) {\n\t\tstruct tcf_proto *tp_new = NULL;\n\n\t\tif (chain->flushing) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Filter kind and protocol must be specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (prio_allocate)\n\t\t\tprio = tcf_auto_prio(tcf_chain_tp_prev(chain,\n\t\t\t\t\t\t\t       &chain_info));\n\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t\ttp_new = tcf_proto_create(name, protocol, prio, chain,\n\t\t\t\t\t  rtnl_held, extack);\n\t\tif (IS_ERR(tp_new)) {\n\t\t\terr = PTR_ERR(tp_new);\n\t\t\tgoto errout_tp;\n\t\t}\n\n\t\ttp_created = 1;\n\t\ttp = tcf_chain_tp_insert_unique(chain, tp_new, protocol, prio,\n\t\t\t\t\t\trtnl_held);\n\t\tif (IS_ERR(tp)) {\n\t\t\terr = PTR_ERR(tp);\n\t\t\tgoto errout_tp;\n\t\t}\n\t} else {\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\t} else if (n->nlmsg_flags & NLM_F_EXCL) {\n\t\ttfilter_put(tp, fh);\n\t\tNL_SET_ERR_MSG(extack, \"Filter already exists\");\n\t\terr = -EEXIST;\n\t\tgoto errout;\n\t}\n\n\tif (chain->tmplt_ops && chain->tmplt_ops != tp->ops) {\n\t\tNL_SET_ERR_MSG(extack, \"Chain template is set to a different filter kind\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\tflags |= TCA_ACT_FLAGS_REPLACE;\n\tif (!rtnl_held)\n\t\tflags |= TCA_ACT_FLAGS_NO_RTNL;\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh,\n\t\t\t      flags, extack);\n\tif (err == 0) {\n\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t       RTM_NEWTFILTER, false, rtnl_held);\n\t\ttfilter_put(tp, fh);\n\t\t/* q pointer is NULL for shared blocks */\n\t\tif (q)\n\t\t\tq->flags &= ~TCQ_F_CAN_BYPASS;\n\t}\n\nerrout:\n\tif (err && tp_created)\n\t\ttcf_chain_tp_delete_empty(chain, tp, rtnl_held, NULL);\nerrout_tp:\n\tif (chain) {\n\t\tif (tp && !IS_ERR(tp))\n\t\t\ttcf_proto_put(tp, rtnl_held, NULL);\n\t\tif (!tp_created)\n\t\t\ttcf_chain_put(chain);\n\t}\n\ttcf_block_release(q, block, rtnl_held);\n\n\tif (rtnl_held)\n\t\trtnl_unlock();\n\n\tif (err == -EAGAIN) {\n\t\t/* Take rtnl lock in case EAGAIN is caused by concurrent flush\n\t\t * of target chain.\n\t\t */\n\t\trtnl_held = true;\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\t}\n\treturn err;\n\nerrout_locked:\n\tmutex_unlock(&chain->filter_chain_lock);\n\tgoto errout;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,9 +10,9 @@\n \tbool prio_allocate;\n \tu32 parent;\n \tu32 chain_index;\n-\tstruct Qdisc *q = NULL;\n+\tstruct Qdisc *q;\n \tstruct tcf_chain_info chain_info;\n-\tstruct tcf_chain *chain = NULL;\n+\tstruct tcf_chain *chain;\n \tstruct tcf_block *block;\n \tstruct tcf_proto *tp;\n \tunsigned long cl;\n@@ -41,6 +41,8 @@\n \ttp = NULL;\n \tcl = 0;\n \tblock = NULL;\n+\tq = NULL;\n+\tchain = NULL;\n \tflags = 0;\n \n \tif (prio == 0) {",
        "function_modified_lines": {
            "added": [
                "\tstruct Qdisc *q;",
                "\tstruct tcf_chain *chain;",
                "\tq = NULL;",
                "\tchain = NULL;"
            ],
            "deleted": [
                "\tstruct Qdisc *q = NULL;",
                "\tstruct tcf_chain *chain = NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free exists in the Linux Kernel in tc_new_tfilter that could allow a local attacker to gain privilege escalation. The exploit requires unprivileged user namespaces. We recommend upgrading past commit 04c2a47ffb13c29778e2a14e414ad4cb5a5db4b5",
        "id": 3247
    },
    {
        "cve_id": "CVE-2016-10906",
        "code_before_change": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
        "code_after_change": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\t\ttx_buff->skb = NULL;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \t\tstruct sk_buff *skb = tx_buff->skb;\n \t\tunsigned int info = le32_to_cpu(txbd->info);\n \n-\t\tif ((info & FOR_EMAC) || !txbd->data)\n+\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n \t\t\tbreak;\n \n \t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n@@ -39,6 +39,7 @@\n \n \t\ttxbd->data = 0;\n \t\ttxbd->info = 0;\n+\t\ttx_buff->skb = NULL;\n \n \t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)",
                "\t\ttx_buff->skb = NULL;"
            ],
            "deleted": [
                "\t\tif ((info & FOR_EMAC) || !txbd->data)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/net/ethernet/arc/emac_main.c in the Linux kernel before 4.5. A use-after-free is caused by a race condition between the functions arc_emac_tx and arc_emac_tx_clean.",
        "id": 910
    },
    {
        "cve_id": "CVE-2020-14416",
        "code_before_change": "static void slcan_close(struct tty_struct *tty)\n{\n\tstruct slcan *sl = (struct slcan *) tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLCAN_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\ttty->disc_data = NULL;\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tflush_work(&sl->tx_work);\n\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "code_after_change": "static void slcan_close(struct tty_struct *tty)\n{\n\tstruct slcan *sl = (struct slcan *) tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLCAN_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\trcu_assign_pointer(tty->disc_data, NULL);\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tsynchronize_rcu();\n\tflush_work(&sl->tx_work);\n\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,10 +7,11 @@\n \t\treturn;\n \n \tspin_lock_bh(&sl->lock);\n-\ttty->disc_data = NULL;\n+\trcu_assign_pointer(tty->disc_data, NULL);\n \tsl->tty = NULL;\n \tspin_unlock_bh(&sl->lock);\n \n+\tsynchronize_rcu();\n \tflush_work(&sl->tx_work);\n \n \t/* Flush network side */",
        "function_modified_lines": {
            "added": [
                "\trcu_assign_pointer(tty->disc_data, NULL);",
                "\tsynchronize_rcu();"
            ],
            "deleted": [
                "\ttty->disc_data = NULL;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.4.16, a race condition in tty->disc_data handling in the slip and slcan line discipline could lead to a use-after-free, aka CID-0ace17d56824. This affects drivers/net/slip/slip.c and drivers/net/can/slcan.c.",
        "id": 2540
    },
    {
        "cve_id": "CVE-2022-3534",
        "code_before_change": "static size_t btf_dump_name_dups(struct btf_dump *d, struct hashmap *name_map,\n\t\t\t\t const char *orig_name)\n{\n\tsize_t dup_cnt = 0;\n\n\thashmap__find(name_map, orig_name, (void **)&dup_cnt);\n\tdup_cnt++;\n\thashmap__set(name_map, orig_name, (void *)dup_cnt, NULL, NULL);\n\n\treturn dup_cnt;\n}",
        "code_after_change": "static size_t btf_dump_name_dups(struct btf_dump *d, struct hashmap *name_map,\n\t\t\t\t const char *orig_name)\n{\n\tchar *old_name, *new_name;\n\tsize_t dup_cnt = 0;\n\tint err;\n\n\tnew_name = strdup(orig_name);\n\tif (!new_name)\n\t\treturn 1;\n\n\thashmap__find(name_map, orig_name, (void **)&dup_cnt);\n\tdup_cnt++;\n\n\terr = hashmap__set(name_map, new_name, (void *)dup_cnt,\n\t\t\t   (const void **)&old_name, NULL);\n\tif (err)\n\t\tfree(new_name);\n\n\tfree(old_name);\n\n\treturn dup_cnt;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,23 @@\n static size_t btf_dump_name_dups(struct btf_dump *d, struct hashmap *name_map,\n \t\t\t\t const char *orig_name)\n {\n+\tchar *old_name, *new_name;\n \tsize_t dup_cnt = 0;\n+\tint err;\n+\n+\tnew_name = strdup(orig_name);\n+\tif (!new_name)\n+\t\treturn 1;\n \n \thashmap__find(name_map, orig_name, (void **)&dup_cnt);\n \tdup_cnt++;\n-\thashmap__set(name_map, orig_name, (void *)dup_cnt, NULL, NULL);\n+\n+\terr = hashmap__set(name_map, new_name, (void *)dup_cnt,\n+\t\t\t   (const void **)&old_name, NULL);\n+\tif (err)\n+\t\tfree(new_name);\n+\n+\tfree(old_name);\n \n \treturn dup_cnt;\n }",
        "function_modified_lines": {
            "added": [
                "\tchar *old_name, *new_name;",
                "\tint err;",
                "",
                "\tnew_name = strdup(orig_name);",
                "\tif (!new_name)",
                "\t\treturn 1;",
                "",
                "\terr = hashmap__set(name_map, new_name, (void *)dup_cnt,",
                "\t\t\t   (const void **)&old_name, NULL);",
                "\tif (err)",
                "\t\tfree(new_name);",
                "",
                "\tfree(old_name);"
            ],
            "deleted": [
                "\thashmap__set(name_map, orig_name, (void *)dup_cnt, NULL, NULL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability classified as critical has been found in Linux Kernel. Affected is the function btf_dump_name_dups of the file tools/lib/bpf/btf_dump.c of the component libbpf. The manipulation leads to use after free. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211032.",
        "id": 3628
    },
    {
        "cve_id": "CVE-2022-42896",
        "code_before_change": "static inline int l2cap_ecred_conn_req(struct l2cap_conn *conn,\n\t\t\t\t       struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t       u8 *data)\n{\n\tstruct l2cap_ecred_conn_req *req = (void *) data;\n\tstruct {\n\t\tstruct l2cap_ecred_conn_rsp rsp;\n\t\t__le16 dcid[L2CAP_ECRED_MAX_CID];\n\t} __packed pdu;\n\tstruct l2cap_chan *chan, *pchan;\n\tu16 mtu, mps;\n\t__le16 psm;\n\tu8 result, len = 0;\n\tint i, num_scid;\n\tbool defer = false;\n\n\tif (!enable_ecred)\n\t\treturn -EINVAL;\n\n\tif (cmd_len < sizeof(*req) || (cmd_len - sizeof(*req)) % sizeof(u16)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tcmd_len -= sizeof(*req);\n\tnum_scid = cmd_len / sizeof(u16);\n\n\tif (num_scid > ARRAY_SIZE(pdu.dcid)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tmtu  = __le16_to_cpu(req->mtu);\n\tmps  = __le16_to_cpu(req->mps);\n\n\tif (mtu < L2CAP_ECRED_MIN_MTU || mps < L2CAP_ECRED_MIN_MPS) {\n\t\tresult = L2CAP_CR_LE_UNACCEPT_PARAMS;\n\t\tgoto response;\n\t}\n\n\tpsm  = req->psm;\n\n\tBT_DBG(\"psm 0x%2.2x mtu %u mps %u\", __le16_to_cpu(psm), mtu, mps);\n\n\tmemset(&pdu, 0, sizeof(pdu));\n\n\t/* Check if we have socket listening on psm */\n\tpchan = l2cap_global_chan_by_psm(BT_LISTEN, psm, &conn->hcon->src,\n\t\t\t\t\t &conn->hcon->dst, LE_LINK);\n\tif (!pchan) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tmutex_lock(&conn->chan_lock);\n\tl2cap_chan_lock(pchan);\n\n\tif (!smp_sufficient_security(conn->hcon, pchan->sec_level,\n\t\t\t\t     SMP_ALLOW_STK)) {\n\t\tresult = L2CAP_CR_LE_AUTHENTICATION;\n\t\tgoto unlock;\n\t}\n\n\tresult = L2CAP_CR_LE_SUCCESS;\n\n\tfor (i = 0; i < num_scid; i++) {\n\t\tu16 scid = __le16_to_cpu(req->scid[i]);\n\n\t\tBT_DBG(\"scid[%d] 0x%4.4x\", i, scid);\n\n\t\tpdu.dcid[i] = 0x0000;\n\t\tlen += sizeof(*pdu.dcid);\n\n\t\t/* Check for valid dynamic CID range */\n\t\tif (scid < L2CAP_CID_DYN_START || scid > L2CAP_CID_LE_DYN_END) {\n\t\t\tresult = L2CAP_CR_LE_INVALID_SCID;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Check if we already have channel with that dcid */\n\t\tif (__l2cap_get_chan_by_dcid(conn, scid)) {\n\t\t\tresult = L2CAP_CR_LE_SCID_IN_USE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tchan = pchan->ops->new_connection(pchan);\n\t\tif (!chan) {\n\t\t\tresult = L2CAP_CR_LE_NO_MEM;\n\t\t\tcontinue;\n\t\t}\n\n\t\tbacpy(&chan->src, &conn->hcon->src);\n\t\tbacpy(&chan->dst, &conn->hcon->dst);\n\t\tchan->src_type = bdaddr_src_type(conn->hcon);\n\t\tchan->dst_type = bdaddr_dst_type(conn->hcon);\n\t\tchan->psm  = psm;\n\t\tchan->dcid = scid;\n\t\tchan->omtu = mtu;\n\t\tchan->remote_mps = mps;\n\n\t\t__l2cap_chan_add(conn, chan);\n\n\t\tl2cap_ecred_init(chan, __le16_to_cpu(req->credits));\n\n\t\t/* Init response */\n\t\tif (!pdu.rsp.credits) {\n\t\t\tpdu.rsp.mtu = cpu_to_le16(chan->imtu);\n\t\t\tpdu.rsp.mps = cpu_to_le16(chan->mps);\n\t\t\tpdu.rsp.credits = cpu_to_le16(chan->rx_credits);\n\t\t}\n\n\t\tpdu.dcid[i] = cpu_to_le16(chan->scid);\n\n\t\t__set_chan_timer(chan, chan->ops->get_sndtimeo(chan));\n\n\t\tchan->ident = cmd->ident;\n\n\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\tl2cap_state_change(chan, BT_CONNECT2);\n\t\t\tdefer = true;\n\t\t\tchan->ops->defer(chan);\n\t\t} else {\n\t\t\tl2cap_chan_ready(chan);\n\t\t}\n\t}\n\nunlock:\n\tl2cap_chan_unlock(pchan);\n\tmutex_unlock(&conn->chan_lock);\n\tl2cap_chan_put(pchan);\n\nresponse:\n\tpdu.rsp.result = cpu_to_le16(result);\n\n\tif (defer)\n\t\treturn 0;\n\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_ECRED_CONN_RSP,\n\t\t       sizeof(pdu.rsp) + len, &pdu);\n\n\treturn 0;\n}",
        "code_after_change": "static inline int l2cap_ecred_conn_req(struct l2cap_conn *conn,\n\t\t\t\t       struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t       u8 *data)\n{\n\tstruct l2cap_ecred_conn_req *req = (void *) data;\n\tstruct {\n\t\tstruct l2cap_ecred_conn_rsp rsp;\n\t\t__le16 dcid[L2CAP_ECRED_MAX_CID];\n\t} __packed pdu;\n\tstruct l2cap_chan *chan, *pchan;\n\tu16 mtu, mps;\n\t__le16 psm;\n\tu8 result, len = 0;\n\tint i, num_scid;\n\tbool defer = false;\n\n\tif (!enable_ecred)\n\t\treturn -EINVAL;\n\n\tif (cmd_len < sizeof(*req) || (cmd_len - sizeof(*req)) % sizeof(u16)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tcmd_len -= sizeof(*req);\n\tnum_scid = cmd_len / sizeof(u16);\n\n\tif (num_scid > ARRAY_SIZE(pdu.dcid)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tmtu  = __le16_to_cpu(req->mtu);\n\tmps  = __le16_to_cpu(req->mps);\n\n\tif (mtu < L2CAP_ECRED_MIN_MTU || mps < L2CAP_ECRED_MIN_MPS) {\n\t\tresult = L2CAP_CR_LE_UNACCEPT_PARAMS;\n\t\tgoto response;\n\t}\n\n\tpsm  = req->psm;\n\n\t/* BLUETOOTH CORE SPECIFICATION Version 5.3 | Vol 3, Part A\n\t * page 1059:\n\t *\n\t * Valid range: 0x0001-0x00ff\n\t *\n\t * Table 4.15: L2CAP_LE_CREDIT_BASED_CONNECTION_REQ SPSM ranges\n\t */\n\tif (!psm || __le16_to_cpu(psm) > L2CAP_PSM_LE_DYN_END) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tBT_DBG(\"psm 0x%2.2x mtu %u mps %u\", __le16_to_cpu(psm), mtu, mps);\n\n\tmemset(&pdu, 0, sizeof(pdu));\n\n\t/* Check if we have socket listening on psm */\n\tpchan = l2cap_global_chan_by_psm(BT_LISTEN, psm, &conn->hcon->src,\n\t\t\t\t\t &conn->hcon->dst, LE_LINK);\n\tif (!pchan) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tmutex_lock(&conn->chan_lock);\n\tl2cap_chan_lock(pchan);\n\n\tif (!smp_sufficient_security(conn->hcon, pchan->sec_level,\n\t\t\t\t     SMP_ALLOW_STK)) {\n\t\tresult = L2CAP_CR_LE_AUTHENTICATION;\n\t\tgoto unlock;\n\t}\n\n\tresult = L2CAP_CR_LE_SUCCESS;\n\n\tfor (i = 0; i < num_scid; i++) {\n\t\tu16 scid = __le16_to_cpu(req->scid[i]);\n\n\t\tBT_DBG(\"scid[%d] 0x%4.4x\", i, scid);\n\n\t\tpdu.dcid[i] = 0x0000;\n\t\tlen += sizeof(*pdu.dcid);\n\n\t\t/* Check for valid dynamic CID range */\n\t\tif (scid < L2CAP_CID_DYN_START || scid > L2CAP_CID_LE_DYN_END) {\n\t\t\tresult = L2CAP_CR_LE_INVALID_SCID;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Check if we already have channel with that dcid */\n\t\tif (__l2cap_get_chan_by_dcid(conn, scid)) {\n\t\t\tresult = L2CAP_CR_LE_SCID_IN_USE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tchan = pchan->ops->new_connection(pchan);\n\t\tif (!chan) {\n\t\t\tresult = L2CAP_CR_LE_NO_MEM;\n\t\t\tcontinue;\n\t\t}\n\n\t\tbacpy(&chan->src, &conn->hcon->src);\n\t\tbacpy(&chan->dst, &conn->hcon->dst);\n\t\tchan->src_type = bdaddr_src_type(conn->hcon);\n\t\tchan->dst_type = bdaddr_dst_type(conn->hcon);\n\t\tchan->psm  = psm;\n\t\tchan->dcid = scid;\n\t\tchan->omtu = mtu;\n\t\tchan->remote_mps = mps;\n\n\t\t__l2cap_chan_add(conn, chan);\n\n\t\tl2cap_ecred_init(chan, __le16_to_cpu(req->credits));\n\n\t\t/* Init response */\n\t\tif (!pdu.rsp.credits) {\n\t\t\tpdu.rsp.mtu = cpu_to_le16(chan->imtu);\n\t\t\tpdu.rsp.mps = cpu_to_le16(chan->mps);\n\t\t\tpdu.rsp.credits = cpu_to_le16(chan->rx_credits);\n\t\t}\n\n\t\tpdu.dcid[i] = cpu_to_le16(chan->scid);\n\n\t\t__set_chan_timer(chan, chan->ops->get_sndtimeo(chan));\n\n\t\tchan->ident = cmd->ident;\n\n\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\tl2cap_state_change(chan, BT_CONNECT2);\n\t\t\tdefer = true;\n\t\t\tchan->ops->defer(chan);\n\t\t} else {\n\t\t\tl2cap_chan_ready(chan);\n\t\t}\n\t}\n\nunlock:\n\tl2cap_chan_unlock(pchan);\n\tmutex_unlock(&conn->chan_lock);\n\tl2cap_chan_put(pchan);\n\nresponse:\n\tpdu.rsp.result = cpu_to_le16(result);\n\n\tif (defer)\n\t\treturn 0;\n\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_ECRED_CONN_RSP,\n\t\t       sizeof(pdu.rsp) + len, &pdu);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,6 +39,18 @@\n \t}\n \n \tpsm  = req->psm;\n+\n+\t/* BLUETOOTH CORE SPECIFICATION Version 5.3 | Vol 3, Part A\n+\t * page 1059:\n+\t *\n+\t * Valid range: 0x0001-0x00ff\n+\t *\n+\t * Table 4.15: L2CAP_LE_CREDIT_BASED_CONNECTION_REQ SPSM ranges\n+\t */\n+\tif (!psm || __le16_to_cpu(psm) > L2CAP_PSM_LE_DYN_END) {\n+\t\tresult = L2CAP_CR_LE_BAD_PSM;\n+\t\tgoto response;\n+\t}\n \n \tBT_DBG(\"psm 0x%2.2x mtu %u mps %u\", __le16_to_cpu(psm), mtu, mps);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* BLUETOOTH CORE SPECIFICATION Version 5.3 | Vol 3, Part A",
                "\t * page 1059:",
                "\t *",
                "\t * Valid range: 0x0001-0x00ff",
                "\t *",
                "\t * Table 4.15: L2CAP_LE_CREDIT_BASED_CONNECTION_REQ SPSM ranges",
                "\t */",
                "\tif (!psm || __le16_to_cpu(psm) > L2CAP_PSM_LE_DYN_END) {",
                "\t\tresult = L2CAP_CR_LE_BAD_PSM;",
                "\t\tgoto response;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities in the Linux kernel's net/bluetooth/l2cap_core.c's l2cap_connect and l2cap_le_connect_req functions which may allow code execution and leaking kernel memory (respectively) remotely via Bluetooth.\u00a0A remote attacker could execute code leaking kernel memory via Bluetooth if within proximity of the victim.\n\nWe recommend upgrading past commit\u00a0  https://www.google.com/url  https://github.com/torvalds/linux/commit/711f8c3fb3db61897080468586b970c87c61d9e4 https://www.google.com/url \n\n",
        "id": 3740
    },
    {
        "cve_id": "CVE-2022-4382",
        "code_before_change": "static int\ngadgetfs_fill_super (struct super_block *sb, struct fs_context *fc)\n{\n\tstruct inode\t*inode;\n\tstruct dev_data\t*dev;\n\n\tif (the_device)\n\t\treturn -ESRCH;\n\n\tCHIP = usb_get_gadget_udc_name();\n\tif (!CHIP)\n\t\treturn -ENODEV;\n\n\t/* superblock */\n\tsb->s_blocksize = PAGE_SIZE;\n\tsb->s_blocksize_bits = PAGE_SHIFT;\n\tsb->s_magic = GADGETFS_MAGIC;\n\tsb->s_op = &gadget_fs_operations;\n\tsb->s_time_gran = 1;\n\n\t/* root inode */\n\tinode = gadgetfs_make_inode (sb,\n\t\t\tNULL, &simple_dir_operations,\n\t\t\tS_IFDIR | S_IRUGO | S_IXUGO);\n\tif (!inode)\n\t\tgoto Enomem;\n\tinode->i_op = &simple_dir_inode_operations;\n\tif (!(sb->s_root = d_make_root (inode)))\n\t\tgoto Enomem;\n\n\t/* the ep0 file is named after the controller we expect;\n\t * user mode code can use it for sanity checks, like we do.\n\t */\n\tdev = dev_new ();\n\tif (!dev)\n\t\tgoto Enomem;\n\n\tdev->sb = sb;\n\tdev->dentry = gadgetfs_create_file(sb, CHIP, dev, &ep0_operations);\n\tif (!dev->dentry) {\n\t\tput_dev(dev);\n\t\tgoto Enomem;\n\t}\n\n\t/* other endpoint files are available after hardware setup,\n\t * from binding to a controller.\n\t */\n\tthe_device = dev;\n\treturn 0;\n\nEnomem:\n\tkfree(CHIP);\n\tCHIP = NULL;\n\n\treturn -ENOMEM;\n}",
        "code_after_change": "static int\ngadgetfs_fill_super (struct super_block *sb, struct fs_context *fc)\n{\n\tstruct inode\t*inode;\n\tstruct dev_data\t*dev;\n\tint\t\trc;\n\n\tmutex_lock(&sb_mutex);\n\n\tif (the_device) {\n\t\trc = -ESRCH;\n\t\tgoto Done;\n\t}\n\n\tCHIP = usb_get_gadget_udc_name();\n\tif (!CHIP) {\n\t\trc = -ENODEV;\n\t\tgoto Done;\n\t}\n\n\t/* superblock */\n\tsb->s_blocksize = PAGE_SIZE;\n\tsb->s_blocksize_bits = PAGE_SHIFT;\n\tsb->s_magic = GADGETFS_MAGIC;\n\tsb->s_op = &gadget_fs_operations;\n\tsb->s_time_gran = 1;\n\n\t/* root inode */\n\tinode = gadgetfs_make_inode (sb,\n\t\t\tNULL, &simple_dir_operations,\n\t\t\tS_IFDIR | S_IRUGO | S_IXUGO);\n\tif (!inode)\n\t\tgoto Enomem;\n\tinode->i_op = &simple_dir_inode_operations;\n\tif (!(sb->s_root = d_make_root (inode)))\n\t\tgoto Enomem;\n\n\t/* the ep0 file is named after the controller we expect;\n\t * user mode code can use it for sanity checks, like we do.\n\t */\n\tdev = dev_new ();\n\tif (!dev)\n\t\tgoto Enomem;\n\n\tdev->sb = sb;\n\tdev->dentry = gadgetfs_create_file(sb, CHIP, dev, &ep0_operations);\n\tif (!dev->dentry) {\n\t\tput_dev(dev);\n\t\tgoto Enomem;\n\t}\n\n\t/* other endpoint files are available after hardware setup,\n\t * from binding to a controller.\n\t */\n\tthe_device = dev;\n\trc = 0;\n\tgoto Done;\n\n Enomem:\n\tkfree(CHIP);\n\tCHIP = NULL;\n\trc = -ENOMEM;\n\n Done:\n\tmutex_unlock(&sb_mutex);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,13 +3,20 @@\n {\n \tstruct inode\t*inode;\n \tstruct dev_data\t*dev;\n+\tint\t\trc;\n \n-\tif (the_device)\n-\t\treturn -ESRCH;\n+\tmutex_lock(&sb_mutex);\n+\n+\tif (the_device) {\n+\t\trc = -ESRCH;\n+\t\tgoto Done;\n+\t}\n \n \tCHIP = usb_get_gadget_udc_name();\n-\tif (!CHIP)\n-\t\treturn -ENODEV;\n+\tif (!CHIP) {\n+\t\trc = -ENODEV;\n+\t\tgoto Done;\n+\t}\n \n \t/* superblock */\n \tsb->s_blocksize = PAGE_SIZE;\n@@ -46,11 +53,15 @@\n \t * from binding to a controller.\n \t */\n \tthe_device = dev;\n-\treturn 0;\n+\trc = 0;\n+\tgoto Done;\n \n-Enomem:\n+ Enomem:\n \tkfree(CHIP);\n \tCHIP = NULL;\n+\trc = -ENOMEM;\n \n-\treturn -ENOMEM;\n+ Done:\n+\tmutex_unlock(&sb_mutex);\n+\treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tint\t\trc;",
                "\tmutex_lock(&sb_mutex);",
                "",
                "\tif (the_device) {",
                "\t\trc = -ESRCH;",
                "\t\tgoto Done;",
                "\t}",
                "\tif (!CHIP) {",
                "\t\trc = -ENODEV;",
                "\t\tgoto Done;",
                "\t}",
                "\trc = 0;",
                "\tgoto Done;",
                " Enomem:",
                "\trc = -ENOMEM;",
                " Done:",
                "\tmutex_unlock(&sb_mutex);",
                "\treturn rc;"
            ],
            "deleted": [
                "\tif (the_device)",
                "\t\treturn -ESRCH;",
                "\tif (!CHIP)",
                "\t\treturn -ENODEV;",
                "\treturn 0;",
                "Enomem:",
                "\treturn -ENOMEM;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw caused by a race among the superblock operations in the gadgetfs Linux driver was found. It could be triggered by yanking out a device that is running the gadgetfs side.",
        "id": 3747
    },
    {
        "cve_id": "CVE-2020-36313",
        "code_before_change": "static int kvm_s390_get_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,\n\t\t\t     u8 *res, unsigned long bufsize)\n{\n\tunsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *ms;\n\n\tcur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);\n\tms = gfn_to_memslot(kvm, cur_gfn);\n\targs->count = 0;\n\targs->start_gfn = cur_gfn;\n\tif (!ms)\n\t\treturn 0;\n\tnext_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);\n\tmem_end = slots->memslots[0].base_gfn + slots->memslots[0].npages;\n\n\twhile (args->count < bufsize) {\n\t\thva = gfn_to_hva(kvm, cur_gfn);\n\t\tif (kvm_is_error_hva(hva))\n\t\t\treturn 0;\n\t\t/* Decrement only if we actually flipped the bit to 0 */\n\t\tif (test_and_clear_bit(cur_gfn - ms->base_gfn, kvm_second_dirty_bitmap(ms)))\n\t\t\tatomic64_dec(&kvm->arch.cmma_dirty_pages);\n\t\tif (get_pgste(kvm->mm, hva, &pgstev) < 0)\n\t\t\tpgstev = 0;\n\t\t/* Save the value */\n\t\tres[args->count++] = (pgstev >> 24) & 0x43;\n\t\t/* If the next bit is too far away, stop. */\n\t\tif (next_gfn > cur_gfn + KVM_S390_MAX_BIT_DISTANCE)\n\t\t\treturn 0;\n\t\t/* If we reached the previous \"next\", find the next one */\n\t\tif (cur_gfn == next_gfn)\n\t\t\tnext_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);\n\t\t/* Reached the end of memory or of the buffer, stop */\n\t\tif ((next_gfn >= mem_end) ||\n\t\t    (next_gfn - args->start_gfn >= bufsize))\n\t\t\treturn 0;\n\t\tcur_gfn++;\n\t\t/* Reached the end of the current memslot, take the next one. */\n\t\tif (cur_gfn - ms->base_gfn >= ms->npages) {\n\t\t\tms = gfn_to_memslot(kvm, cur_gfn);\n\t\t\tif (!ms)\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int kvm_s390_get_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,\n\t\t\t     u8 *res, unsigned long bufsize)\n{\n\tunsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *ms;\n\n\tif (unlikely(!slots->used_slots))\n\t\treturn 0;\n\n\tcur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);\n\tms = gfn_to_memslot(kvm, cur_gfn);\n\targs->count = 0;\n\targs->start_gfn = cur_gfn;\n\tif (!ms)\n\t\treturn 0;\n\tnext_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);\n\tmem_end = slots->memslots[0].base_gfn + slots->memslots[0].npages;\n\n\twhile (args->count < bufsize) {\n\t\thva = gfn_to_hva(kvm, cur_gfn);\n\t\tif (kvm_is_error_hva(hva))\n\t\t\treturn 0;\n\t\t/* Decrement only if we actually flipped the bit to 0 */\n\t\tif (test_and_clear_bit(cur_gfn - ms->base_gfn, kvm_second_dirty_bitmap(ms)))\n\t\t\tatomic64_dec(&kvm->arch.cmma_dirty_pages);\n\t\tif (get_pgste(kvm->mm, hva, &pgstev) < 0)\n\t\t\tpgstev = 0;\n\t\t/* Save the value */\n\t\tres[args->count++] = (pgstev >> 24) & 0x43;\n\t\t/* If the next bit is too far away, stop. */\n\t\tif (next_gfn > cur_gfn + KVM_S390_MAX_BIT_DISTANCE)\n\t\t\treturn 0;\n\t\t/* If we reached the previous \"next\", find the next one */\n\t\tif (cur_gfn == next_gfn)\n\t\t\tnext_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);\n\t\t/* Reached the end of memory or of the buffer, stop */\n\t\tif ((next_gfn >= mem_end) ||\n\t\t    (next_gfn - args->start_gfn >= bufsize))\n\t\t\treturn 0;\n\t\tcur_gfn++;\n\t\t/* Reached the end of the current memslot, take the next one. */\n\t\tif (cur_gfn - ms->base_gfn >= ms->npages) {\n\t\t\tms = gfn_to_memslot(kvm, cur_gfn);\n\t\t\tif (!ms)\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \tunsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;\n \tstruct kvm_memslots *slots = kvm_memslots(kvm);\n \tstruct kvm_memory_slot *ms;\n+\n+\tif (unlikely(!slots->used_slots))\n+\t\treturn 0;\n \n \tcur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);\n \tms = gfn_to_memslot(kvm, cur_gfn);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (unlikely(!slots->used_slots))",
                "\t\treturn 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7. The KVM subsystem allows out-of-range access to memslots after a deletion, aka CID-0774a964ef56. This affects arch/s390/kvm/kvm-s390.c, include/linux/kvm_host.h, and virt/kvm/kvm_main.c.",
        "id": 2717
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\n\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n\t\ttty_port_destroy(&vc->port);\n\t\tkfree(vc);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\n\tif (vc && vc_num >= MIN_NR_CONSOLES)\n\t\ttty_port_put(&vc->port);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,10 +10,8 @@\n \t\tvc = vc_deallocate(vc_num);\n \tconsole_unlock();\n \n-\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n-\t\ttty_port_destroy(&vc->port);\n-\t\tkfree(vc);\n-\t}\n+\tif (vc && vc_num >= MIN_NR_CONSOLES)\n+\t\ttty_port_put(&vc->port);\n \n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (vc && vc_num >= MIN_NR_CONSOLES)",
                "\t\ttty_port_put(&vc->port);"
            ],
            "deleted": [
                "\tif (vc && vc_num >= MIN_NR_CONSOLES) {",
                "\t\ttty_port_destroy(&vc->port);",
                "\t\tkfree(vc);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "id": 2765
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "static int con_install(struct tty_driver *driver, struct tty_struct *tty)\n{\n\tunsigned int currcons = tty->index;\n\tstruct vc_data *vc;\n\tint ret;\n\n\tconsole_lock();\n\tret = vc_allocate(currcons);\n\tif (ret)\n\t\tgoto unlock;\n\n\tvc = vc_cons[currcons].d;\n\n\t/* Still being freed */\n\tif (vc->port.tty) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto unlock;\n\t}\n\n\tret = tty_port_install(&vc->port, driver, tty);\n\tif (ret)\n\t\tgoto unlock;\n\n\ttty->driver_data = vc;\n\tvc->port.tty = tty;\n\n\tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n\t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;\n\t\ttty->winsize.ws_col = vc_cons[currcons].d->vc_cols;\n\t}\n\tif (vc->vc_utf)\n\t\ttty->termios.c_iflag |= IUTF8;\n\telse\n\t\ttty->termios.c_iflag &= ~IUTF8;\nunlock:\n\tconsole_unlock();\n\treturn ret;\n}",
        "code_after_change": "static int con_install(struct tty_driver *driver, struct tty_struct *tty)\n{\n\tunsigned int currcons = tty->index;\n\tstruct vc_data *vc;\n\tint ret;\n\n\tconsole_lock();\n\tret = vc_allocate(currcons);\n\tif (ret)\n\t\tgoto unlock;\n\n\tvc = vc_cons[currcons].d;\n\n\t/* Still being freed */\n\tif (vc->port.tty) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto unlock;\n\t}\n\n\tret = tty_port_install(&vc->port, driver, tty);\n\tif (ret)\n\t\tgoto unlock;\n\n\ttty->driver_data = vc;\n\tvc->port.tty = tty;\n\ttty_port_get(&vc->port);\n\n\tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n\t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;\n\t\ttty->winsize.ws_col = vc_cons[currcons].d->vc_cols;\n\t}\n\tif (vc->vc_utf)\n\t\ttty->termios.c_iflag |= IUTF8;\n\telse\n\t\ttty->termios.c_iflag &= ~IUTF8;\nunlock:\n\tconsole_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,6 +23,7 @@\n \n \ttty->driver_data = vc;\n \tvc->port.tty = tty;\n+\ttty_port_get(&vc->port);\n \n \tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n \t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;",
        "function_modified_lines": {
            "added": [
                "\ttty_port_get(&vc->port);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "id": 2763
    },
    {
        "cve_id": "CVE-2021-45868",
        "code_before_change": "static int remove_tree(struct qtree_mem_dqinfo *info, struct dquot *dquot,\n\t\t       uint *blk, int depth)\n{\n\tchar *buf = kmalloc(info->dqi_usable_bs, GFP_NOFS);\n\tint ret = 0;\n\tuint newblk;\n\t__le32 *ref = (__le32 *)buf;\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tret = read_blk(info, *blk, buf);\n\tif (ret < 0) {\n\t\tquota_error(dquot->dq_sb, \"Can't read quota data block %u\",\n\t\t\t    *blk);\n\t\tgoto out_buf;\n\t}\n\tnewblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n\tif (depth == info->dqi_qtree_depth - 1) {\n\t\tret = free_dqentry(info, dquot, newblk);\n\t\tnewblk = 0;\n\t} else {\n\t\tret = remove_tree(info, dquot, &newblk, depth+1);\n\t}\n\tif (ret >= 0 && !newblk) {\n\t\tint i;\n\t\tref[get_index(info, dquot->dq_id, depth)] = cpu_to_le32(0);\n\t\t/* Block got empty? */\n\t\tfor (i = 0; i < (info->dqi_usable_bs >> 2) && !ref[i]; i++)\n\t\t\t;\n\t\t/* Don't put the root block into the free block list */\n\t\tif (i == (info->dqi_usable_bs >> 2)\n\t\t    && *blk != QT_TREEOFF) {\n\t\t\tput_free_dqblk(info, buf, *blk);\n\t\t\t*blk = 0;\n\t\t} else {\n\t\t\tret = write_blk(info, *blk, buf);\n\t\t\tif (ret < 0)\n\t\t\t\tquota_error(dquot->dq_sb,\n\t\t\t\t\t    \"Can't write quota tree block %u\",\n\t\t\t\t\t    *blk);\n\t\t}\n\t}\nout_buf:\n\tkfree(buf);\n\treturn ret;\n}",
        "code_after_change": "static int remove_tree(struct qtree_mem_dqinfo *info, struct dquot *dquot,\n\t\t       uint *blk, int depth)\n{\n\tchar *buf = kmalloc(info->dqi_usable_bs, GFP_NOFS);\n\tint ret = 0;\n\tuint newblk;\n\t__le32 *ref = (__le32 *)buf;\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tret = read_blk(info, *blk, buf);\n\tif (ret < 0) {\n\t\tquota_error(dquot->dq_sb, \"Can't read quota data block %u\",\n\t\t\t    *blk);\n\t\tgoto out_buf;\n\t}\n\tnewblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n\tif (newblk < QT_TREEOFF || newblk >= info->dqi_blocks) {\n\t\tquota_error(dquot->dq_sb, \"Getting block too big (%u >= %u)\",\n\t\t\t    newblk, info->dqi_blocks);\n\t\tret = -EUCLEAN;\n\t\tgoto out_buf;\n\t}\n\n\tif (depth == info->dqi_qtree_depth - 1) {\n\t\tret = free_dqentry(info, dquot, newblk);\n\t\tnewblk = 0;\n\t} else {\n\t\tret = remove_tree(info, dquot, &newblk, depth+1);\n\t}\n\tif (ret >= 0 && !newblk) {\n\t\tint i;\n\t\tref[get_index(info, dquot->dq_id, depth)] = cpu_to_le32(0);\n\t\t/* Block got empty? */\n\t\tfor (i = 0; i < (info->dqi_usable_bs >> 2) && !ref[i]; i++)\n\t\t\t;\n\t\t/* Don't put the root block into the free block list */\n\t\tif (i == (info->dqi_usable_bs >> 2)\n\t\t    && *blk != QT_TREEOFF) {\n\t\t\tput_free_dqblk(info, buf, *blk);\n\t\t\t*blk = 0;\n\t\t} else {\n\t\t\tret = write_blk(info, *blk, buf);\n\t\t\tif (ret < 0)\n\t\t\t\tquota_error(dquot->dq_sb,\n\t\t\t\t\t    \"Can't write quota tree block %u\",\n\t\t\t\t\t    *blk);\n\t\t}\n\t}\nout_buf:\n\tkfree(buf);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,13 @@\n \t\tgoto out_buf;\n \t}\n \tnewblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n+\tif (newblk < QT_TREEOFF || newblk >= info->dqi_blocks) {\n+\t\tquota_error(dquot->dq_sb, \"Getting block too big (%u >= %u)\",\n+\t\t\t    newblk, info->dqi_blocks);\n+\t\tret = -EUCLEAN;\n+\t\tgoto out_buf;\n+\t}\n+\n \tif (depth == info->dqi_qtree_depth - 1) {\n \t\tret = free_dqentry(info, dquot, newblk);\n \t\tnewblk = 0;",
        "function_modified_lines": {
            "added": [
                "\tif (newblk < QT_TREEOFF || newblk >= info->dqi_blocks) {",
                "\t\tquota_error(dquot->dq_sb, \"Getting block too big (%u >= %u)\",",
                "\t\t\t    newblk, info->dqi_blocks);",
                "\t\tret = -EUCLEAN;",
                "\t\tgoto out_buf;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.15.3, fs/quota/quota_tree.c does not validate the block number in the quota tree (on disk). This can, for example, lead to a kernel/locking/rwsem.c use-after-free if there is a corrupted quota file.",
        "id": 3183
    },
    {
        "cve_id": "CVE-2022-2977",
        "code_before_change": "void tpm_chip_unregister(struct tpm_chip *chip)\n{\n\ttpm_del_legacy_sysfs(chip);\n\tif (IS_ENABLED(CONFIG_HW_RANDOM_TPM) && !tpm_is_firmware_upgrade(chip))\n\t\thwrng_unregister(&chip->hwrng);\n\ttpm_bios_log_teardown(chip);\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip))\n\t\tcdev_device_del(&chip->cdevs, &chip->devs);\n\ttpm_del_char_device(chip);\n}",
        "code_after_change": "void tpm_chip_unregister(struct tpm_chip *chip)\n{\n\ttpm_del_legacy_sysfs(chip);\n\tif (IS_ENABLED(CONFIG_HW_RANDOM_TPM) && !tpm_is_firmware_upgrade(chip))\n\t\thwrng_unregister(&chip->hwrng);\n\ttpm_bios_log_teardown(chip);\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip))\n\t\ttpm_devs_remove(chip);\n\ttpm_del_char_device(chip);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,6 @@\n \t\thwrng_unregister(&chip->hwrng);\n \ttpm_bios_log_teardown(chip);\n \tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip))\n-\t\tcdev_device_del(&chip->cdevs, &chip->devs);\n+\t\ttpm_devs_remove(chip);\n \ttpm_del_char_device(chip);\n }",
        "function_modified_lines": {
            "added": [
                "\t\ttpm_devs_remove(chip);"
            ],
            "deleted": [
                "\t\tcdev_device_del(&chip->cdevs, &chip->devs);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel implementation of proxied virtualized TPM devices. On a system where virtualized TPM devices are configured (this is not the default) a local attacker can create a use-after-free and create a situation where it may be possible to escalate privileges on the system.",
        "id": 3526
    },
    {
        "cve_id": "CVE-2022-1973",
        "code_before_change": "static int log_read_rst(struct ntfs_log *log, u32 l_size, bool first,\n\t\t\tstruct restart_info *info)\n{\n\tu32 skip, vbo;\n\tstruct RESTART_HDR *r_page = kmalloc(DefaultLogPageSize, GFP_NOFS);\n\n\tif (!r_page)\n\t\treturn -ENOMEM;\n\n\tmemset(info, 0, sizeof(struct restart_info));\n\n\t/* Determine which restart area we are looking for. */\n\tif (first) {\n\t\tvbo = 0;\n\t\tskip = 512;\n\t} else {\n\t\tvbo = 512;\n\t\tskip = 0;\n\t}\n\n\t/* Loop continuously until we succeed. */\n\tfor (; vbo < l_size; vbo = 2 * vbo + skip, skip = 0) {\n\t\tbool usa_error;\n\t\tu32 sys_page_size;\n\t\tbool brst, bchk;\n\t\tstruct RESTART_AREA *ra;\n\n\t\t/* Read a page header at the current offset. */\n\t\tif (read_log_page(log, vbo, (struct RECORD_PAGE_HDR **)&r_page,\n\t\t\t\t  &usa_error)) {\n\t\t\t/* Ignore any errors. */\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Exit if the signature is a log record page. */\n\t\tif (r_page->rhdr.sign == NTFS_RCRD_SIGNATURE) {\n\t\t\tinfo->initialized = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tbrst = r_page->rhdr.sign == NTFS_RSTR_SIGNATURE;\n\t\tbchk = r_page->rhdr.sign == NTFS_CHKD_SIGNATURE;\n\n\t\tif (!bchk && !brst) {\n\t\t\tif (r_page->rhdr.sign != NTFS_FFFF_SIGNATURE) {\n\t\t\t\t/*\n\t\t\t\t * Remember if the signature does not\n\t\t\t\t * indicate uninitialized file.\n\t\t\t\t */\n\t\t\t\tinfo->initialized = true;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tra = NULL;\n\t\tinfo->valid_page = false;\n\t\tinfo->initialized = true;\n\t\tinfo->vbo = vbo;\n\n\t\t/* Let's check the restart area if this is a valid page. */\n\t\tif (!is_rst_page_hdr_valid(vbo, r_page))\n\t\t\tgoto check_result;\n\t\tra = Add2Ptr(r_page, le16_to_cpu(r_page->ra_off));\n\n\t\tif (!is_rst_area_valid(r_page))\n\t\t\tgoto check_result;\n\n\t\t/*\n\t\t * We have a valid restart page header and restart area.\n\t\t * If chkdsk was run or we have no clients then we have\n\t\t * no more checking to do.\n\t\t */\n\t\tif (bchk || ra->client_idx[1] == LFS_NO_CLIENT_LE) {\n\t\t\tinfo->valid_page = true;\n\t\t\tgoto check_result;\n\t\t}\n\n\t\t/* Read the entire restart area. */\n\t\tsys_page_size = le32_to_cpu(r_page->sys_page_size);\n\t\tif (DefaultLogPageSize != sys_page_size) {\n\t\t\tkfree(r_page);\n\t\t\tr_page = kzalloc(sys_page_size, GFP_NOFS);\n\t\t\tif (!r_page)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (read_log_page(log, vbo,\n\t\t\t\t\t  (struct RECORD_PAGE_HDR **)&r_page,\n\t\t\t\t\t  &usa_error)) {\n\t\t\t\t/* Ignore any errors. */\n\t\t\t\tkfree(r_page);\n\t\t\t\tr_page = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (is_client_area_valid(r_page, usa_error)) {\n\t\t\tinfo->valid_page = true;\n\t\t\tra = Add2Ptr(r_page, le16_to_cpu(r_page->ra_off));\n\t\t}\n\ncheck_result:\n\t\t/*\n\t\t * If chkdsk was run then update the caller's\n\t\t * values and return.\n\t\t */\n\t\tif (r_page->rhdr.sign == NTFS_CHKD_SIGNATURE) {\n\t\t\tinfo->chkdsk_was_run = true;\n\t\t\tinfo->last_lsn = le64_to_cpu(r_page->rhdr.lsn);\n\t\t\tinfo->restart = true;\n\t\t\tinfo->r_page = r_page;\n\t\t\treturn 0;\n\t\t}\n\n\t\t/*\n\t\t * If we have a valid page then copy the values\n\t\t * we need from it.\n\t\t */\n\t\tif (info->valid_page) {\n\t\t\tinfo->last_lsn = le64_to_cpu(ra->current_lsn);\n\t\t\tinfo->restart = true;\n\t\t\tinfo->r_page = r_page;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tkfree(r_page);\n\n\treturn 0;\n}",
        "code_after_change": "static int log_read_rst(struct ntfs_log *log, u32 l_size, bool first,\n\t\t\tstruct restart_info *info)\n{\n\tu32 skip, vbo;\n\tstruct RESTART_HDR *r_page = kmalloc(DefaultLogPageSize, GFP_NOFS);\n\n\tif (!r_page)\n\t\treturn -ENOMEM;\n\n\t/* Determine which restart area we are looking for. */\n\tif (first) {\n\t\tvbo = 0;\n\t\tskip = 512;\n\t} else {\n\t\tvbo = 512;\n\t\tskip = 0;\n\t}\n\n\t/* Loop continuously until we succeed. */\n\tfor (; vbo < l_size; vbo = 2 * vbo + skip, skip = 0) {\n\t\tbool usa_error;\n\t\tu32 sys_page_size;\n\t\tbool brst, bchk;\n\t\tstruct RESTART_AREA *ra;\n\n\t\t/* Read a page header at the current offset. */\n\t\tif (read_log_page(log, vbo, (struct RECORD_PAGE_HDR **)&r_page,\n\t\t\t\t  &usa_error)) {\n\t\t\t/* Ignore any errors. */\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Exit if the signature is a log record page. */\n\t\tif (r_page->rhdr.sign == NTFS_RCRD_SIGNATURE) {\n\t\t\tinfo->initialized = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tbrst = r_page->rhdr.sign == NTFS_RSTR_SIGNATURE;\n\t\tbchk = r_page->rhdr.sign == NTFS_CHKD_SIGNATURE;\n\n\t\tif (!bchk && !brst) {\n\t\t\tif (r_page->rhdr.sign != NTFS_FFFF_SIGNATURE) {\n\t\t\t\t/*\n\t\t\t\t * Remember if the signature does not\n\t\t\t\t * indicate uninitialized file.\n\t\t\t\t */\n\t\t\t\tinfo->initialized = true;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tra = NULL;\n\t\tinfo->valid_page = false;\n\t\tinfo->initialized = true;\n\t\tinfo->vbo = vbo;\n\n\t\t/* Let's check the restart area if this is a valid page. */\n\t\tif (!is_rst_page_hdr_valid(vbo, r_page))\n\t\t\tgoto check_result;\n\t\tra = Add2Ptr(r_page, le16_to_cpu(r_page->ra_off));\n\n\t\tif (!is_rst_area_valid(r_page))\n\t\t\tgoto check_result;\n\n\t\t/*\n\t\t * We have a valid restart page header and restart area.\n\t\t * If chkdsk was run or we have no clients then we have\n\t\t * no more checking to do.\n\t\t */\n\t\tif (bchk || ra->client_idx[1] == LFS_NO_CLIENT_LE) {\n\t\t\tinfo->valid_page = true;\n\t\t\tgoto check_result;\n\t\t}\n\n\t\t/* Read the entire restart area. */\n\t\tsys_page_size = le32_to_cpu(r_page->sys_page_size);\n\t\tif (DefaultLogPageSize != sys_page_size) {\n\t\t\tkfree(r_page);\n\t\t\tr_page = kzalloc(sys_page_size, GFP_NOFS);\n\t\t\tif (!r_page)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (read_log_page(log, vbo,\n\t\t\t\t\t  (struct RECORD_PAGE_HDR **)&r_page,\n\t\t\t\t\t  &usa_error)) {\n\t\t\t\t/* Ignore any errors. */\n\t\t\t\tkfree(r_page);\n\t\t\t\tr_page = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (is_client_area_valid(r_page, usa_error)) {\n\t\t\tinfo->valid_page = true;\n\t\t\tra = Add2Ptr(r_page, le16_to_cpu(r_page->ra_off));\n\t\t}\n\ncheck_result:\n\t\t/*\n\t\t * If chkdsk was run then update the caller's\n\t\t * values and return.\n\t\t */\n\t\tif (r_page->rhdr.sign == NTFS_CHKD_SIGNATURE) {\n\t\t\tinfo->chkdsk_was_run = true;\n\t\t\tinfo->last_lsn = le64_to_cpu(r_page->rhdr.lsn);\n\t\t\tinfo->restart = true;\n\t\t\tinfo->r_page = r_page;\n\t\t\treturn 0;\n\t\t}\n\n\t\t/*\n\t\t * If we have a valid page then copy the values\n\t\t * we need from it.\n\t\t */\n\t\tif (info->valid_page) {\n\t\t\tinfo->last_lsn = le64_to_cpu(ra->current_lsn);\n\t\t\tinfo->restart = true;\n\t\t\tinfo->r_page = r_page;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tkfree(r_page);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,6 @@\n \n \tif (!r_page)\n \t\treturn -ENOMEM;\n-\n-\tmemset(info, 0, sizeof(struct restart_info));\n \n \t/* Determine which restart area we are looking for. */\n \tif (first) {",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmemset(info, 0, sizeof(struct restart_info));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel in log_replay in fs/ntfs3/fslog.c in the NTFS journal. This flaw allows a local attacker to crash the system and leads to a kernel information leak problem.",
        "id": 3305
    },
    {
        "cve_id": "CVE-2023-4921",
        "code_before_change": "static void agg_dequeue(struct qfq_aggregate *agg,\n\t\t\tstruct qfq_class *cl, unsigned int len)\n{\n\tqdisc_dequeue_peeked(cl->qdisc);\n\n\tcl->deficit -= (int) len;\n\n\tif (cl->qdisc->q.qlen == 0) /* no more packets, remove from list */\n\t\tlist_del(&cl->alist);\n\telse if (cl->deficit < qdisc_pkt_len(cl->qdisc->ops->peek(cl->qdisc))) {\n\t\tcl->deficit += agg->lmax;\n\t\tlist_move_tail(&cl->alist, &agg->active);\n\t}\n}",
        "code_after_change": "static struct sk_buff *agg_dequeue(struct qfq_aggregate *agg,\n\t\t\t\t   struct qfq_class *cl, unsigned int len)\n{\n\tstruct sk_buff *skb = qdisc_dequeue_peeked(cl->qdisc);\n\n\tif (!skb)\n\t\treturn NULL;\n\n\tcl->deficit -= (int) len;\n\n\tif (cl->qdisc->q.qlen == 0) /* no more packets, remove from list */\n\t\tlist_del(&cl->alist);\n\telse if (cl->deficit < qdisc_pkt_len(cl->qdisc->ops->peek(cl->qdisc))) {\n\t\tcl->deficit += agg->lmax;\n\t\tlist_move_tail(&cl->alist, &agg->active);\n\t}\n\n\treturn skb;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,10 @@\n-static void agg_dequeue(struct qfq_aggregate *agg,\n-\t\t\tstruct qfq_class *cl, unsigned int len)\n+static struct sk_buff *agg_dequeue(struct qfq_aggregate *agg,\n+\t\t\t\t   struct qfq_class *cl, unsigned int len)\n {\n-\tqdisc_dequeue_peeked(cl->qdisc);\n+\tstruct sk_buff *skb = qdisc_dequeue_peeked(cl->qdisc);\n+\n+\tif (!skb)\n+\t\treturn NULL;\n \n \tcl->deficit -= (int) len;\n \n@@ -11,4 +14,6 @@\n \t\tcl->deficit += agg->lmax;\n \t\tlist_move_tail(&cl->alist, &agg->active);\n \t}\n+\n+\treturn skb;\n }",
        "function_modified_lines": {
            "added": [
                "static struct sk_buff *agg_dequeue(struct qfq_aggregate *agg,",
                "\t\t\t\t   struct qfq_class *cl, unsigned int len)",
                "\tstruct sk_buff *skb = qdisc_dequeue_peeked(cl->qdisc);",
                "",
                "\tif (!skb)",
                "\t\treturn NULL;",
                "",
                "\treturn skb;"
            ],
            "deleted": [
                "static void agg_dequeue(struct qfq_aggregate *agg,",
                "\t\t\tstruct qfq_class *cl, unsigned int len)",
                "\tqdisc_dequeue_peeked(cl->qdisc);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's net/sched: sch_qfq component can be exploited to achieve local privilege escalation.\n\nWhen the plug qdisc is used as a class of the qfq qdisc, sending network packets triggers use-after-free in qfq_dequeue() due to the incorrect .peek handler of sch_plug and lack of error checking in agg_dequeue().\n\nWe recommend upgrading past commit 8fc134fee27f2263988ae38920bc03da416b03d8.\n\n",
        "id": 4253
    },
    {
        "cve_id": "CVE-2019-13233",
        "code_before_change": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc->type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc->l << 1) | desc->d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "code_after_change": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc.type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc.l << 1) | desc.d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n int insn_get_code_seg_params(struct pt_regs *regs)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tshort sel;\n \n \tif (v8086_mode(regs))\n@@ -11,8 +11,7 @@\n \tif (sel < 0)\n \t\treturn sel;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn -EINVAL;\n \n \t/*\n@@ -20,10 +19,10 @@\n \t * determines whether a segment contains data or code. If this is a data\n \t * segment, return error.\n \t */\n-\tif (!(desc->type & BIT(3)))\n+\tif (!(desc.type & BIT(3)))\n \t\treturn -EINVAL;\n \n-\tswitch ((desc->l << 1) | desc->d) {\n+\tswitch ((desc.l << 1) | desc.d) {\n \tcase 0: /*\n \t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n \t\t * both 16-bit.",
        "function_modified_lines": {
            "added": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\tif (!(desc.type & BIT(3)))",
                "\tswitch ((desc.l << 1) | desc.d) {"
            ],
            "deleted": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\tif (!(desc->type & BIT(3)))",
                "\tswitch ((desc->l << 1) | desc->d) {"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation.",
        "id": 1958
    },
    {
        "cve_id": "CVE-2022-1011",
        "code_before_change": "static int fuse_get_user_pages(struct fuse_args_pages *ap, struct iov_iter *ii,\n\t\t\t       size_t *nbytesp, int write,\n\t\t\t       unsigned int max_pages)\n{\n\tsize_t nbytes = 0;  /* # bytes already packed in req */\n\tssize_t ret = 0;\n\n\t/* Special case for kernel I/O: can copy directly into the buffer */\n\tif (iov_iter_is_kvec(ii)) {\n\t\tunsigned long user_addr = fuse_get_user_addr(ii);\n\t\tsize_t frag_size = fuse_get_frag_size(ii, *nbytesp);\n\n\t\tif (write)\n\t\t\tap->args.in_args[1].value = (void *) user_addr;\n\t\telse\n\t\t\tap->args.out_args[0].value = (void *) user_addr;\n\n\t\tiov_iter_advance(ii, frag_size);\n\t\t*nbytesp = frag_size;\n\t\treturn 0;\n\t}\n\n\twhile (nbytes < *nbytesp && ap->num_pages < max_pages) {\n\t\tunsigned npages;\n\t\tsize_t start;\n\t\tret = iov_iter_get_pages(ii, &ap->pages[ap->num_pages],\n\t\t\t\t\t*nbytesp - nbytes,\n\t\t\t\t\tmax_pages - ap->num_pages,\n\t\t\t\t\t&start);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tiov_iter_advance(ii, ret);\n\t\tnbytes += ret;\n\n\t\tret += start;\n\t\tnpages = DIV_ROUND_UP(ret, PAGE_SIZE);\n\n\t\tap->descs[ap->num_pages].offset = start;\n\t\tfuse_page_descs_length_init(ap->descs, ap->num_pages, npages);\n\n\t\tap->num_pages += npages;\n\t\tap->descs[ap->num_pages - 1].length -=\n\t\t\t(PAGE_SIZE - ret) & (PAGE_SIZE - 1);\n\t}\n\n\tif (write)\n\t\tap->args.in_pages = true;\n\telse\n\t\tap->args.out_pages = true;\n\n\t*nbytesp = nbytes;\n\n\treturn ret < 0 ? ret : 0;\n}",
        "code_after_change": "static int fuse_get_user_pages(struct fuse_args_pages *ap, struct iov_iter *ii,\n\t\t\t       size_t *nbytesp, int write,\n\t\t\t       unsigned int max_pages)\n{\n\tsize_t nbytes = 0;  /* # bytes already packed in req */\n\tssize_t ret = 0;\n\n\t/* Special case for kernel I/O: can copy directly into the buffer */\n\tif (iov_iter_is_kvec(ii)) {\n\t\tunsigned long user_addr = fuse_get_user_addr(ii);\n\t\tsize_t frag_size = fuse_get_frag_size(ii, *nbytesp);\n\n\t\tif (write)\n\t\t\tap->args.in_args[1].value = (void *) user_addr;\n\t\telse\n\t\t\tap->args.out_args[0].value = (void *) user_addr;\n\n\t\tiov_iter_advance(ii, frag_size);\n\t\t*nbytesp = frag_size;\n\t\treturn 0;\n\t}\n\n\twhile (nbytes < *nbytesp && ap->num_pages < max_pages) {\n\t\tunsigned npages;\n\t\tsize_t start;\n\t\tret = iov_iter_get_pages(ii, &ap->pages[ap->num_pages],\n\t\t\t\t\t*nbytesp - nbytes,\n\t\t\t\t\tmax_pages - ap->num_pages,\n\t\t\t\t\t&start);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tiov_iter_advance(ii, ret);\n\t\tnbytes += ret;\n\n\t\tret += start;\n\t\tnpages = DIV_ROUND_UP(ret, PAGE_SIZE);\n\n\t\tap->descs[ap->num_pages].offset = start;\n\t\tfuse_page_descs_length_init(ap->descs, ap->num_pages, npages);\n\n\t\tap->num_pages += npages;\n\t\tap->descs[ap->num_pages - 1].length -=\n\t\t\t(PAGE_SIZE - ret) & (PAGE_SIZE - 1);\n\t}\n\n\tap->args.user_pages = true;\n\tif (write)\n\t\tap->args.in_pages = true;\n\telse\n\t\tap->args.out_pages = true;\n\n\t*nbytesp = nbytes;\n\n\treturn ret < 0 ? ret : 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -44,6 +44,7 @@\n \t\t\t(PAGE_SIZE - ret) & (PAGE_SIZE - 1);\n \t}\n \n+\tap->args.user_pages = true;\n \tif (write)\n \t\tap->args.in_pages = true;\n \telse",
        "function_modified_lines": {
            "added": [
                "\tap->args.user_pages = true;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s FUSE filesystem in the way a user triggers write(). This flaw allows a local user to gain unauthorized access to data from the FUSE filesystem, resulting in privilege escalation.",
        "id": 3231
    }
]