[
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
        "code_after_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \t\treturn;\n \n \t/* Can only change if privileged. */\n-\tif (!capable(CAP_NET_ADMIN)) {\n+\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {\n \t\terr = EPERM;\n \t\tgoto out;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {"
            ],
            "deleted": [
                "\tif (!capable(CAP_NET_ADMIN)) {"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 434
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int packet_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tint num = 0, s_num = cb->args[0];\n\tstruct packet_diag_req *req;\n\tstruct net *net;\n\tstruct sock *sk;\n\tbool may_report_filterinfo;\n\n\tnet = sock_net(skb->sk);\n\treq = nlmsg_data(cb->nlh);\n\tmay_report_filterinfo = ns_capable(net->user_ns, CAP_NET_ADMIN);\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_for_each(sk, &net->packet.sklist) {\n\t\tif (!net_eq(sock_net(sk), net))\n\t\t\tcontinue;\n\t\tif (num < s_num)\n\t\t\tgoto next;\n\n\t\tif (sk_diag_fill(sk, skb, req,\n\t\t\t\t may_report_filterinfo,\n\t\t\t\t sk_user_ns(NETLINK_CB(cb->skb).sk),\n\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t sock_i_ino(sk)) < 0)\n\t\t\tgoto done;\nnext:\n\t\tnum++;\n\t}\ndone:\n\tmutex_unlock(&net->packet.sklist_lock);\n\tcb->args[0] = num;\n\n\treturn skb->len;\n}",
        "code_after_change": "static int packet_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tint num = 0, s_num = cb->args[0];\n\tstruct packet_diag_req *req;\n\tstruct net *net;\n\tstruct sock *sk;\n\tbool may_report_filterinfo;\n\n\tnet = sock_net(skb->sk);\n\treq = nlmsg_data(cb->nlh);\n\tmay_report_filterinfo = netlink_net_capable(cb->skb, CAP_NET_ADMIN);\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_for_each(sk, &net->packet.sklist) {\n\t\tif (!net_eq(sock_net(sk), net))\n\t\t\tcontinue;\n\t\tif (num < s_num)\n\t\t\tgoto next;\n\n\t\tif (sk_diag_fill(sk, skb, req,\n\t\t\t\t may_report_filterinfo,\n\t\t\t\t sk_user_ns(NETLINK_CB(cb->skb).sk),\n\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t sock_i_ino(sk)) < 0)\n\t\t\tgoto done;\nnext:\n\t\tnum++;\n\t}\ndone:\n\tmutex_unlock(&net->packet.sklist_lock);\n\tcb->args[0] = num;\n\n\treturn skb->len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \n \tnet = sock_net(skb->sk);\n \treq = nlmsg_data(cb->nlh);\n-\tmay_report_filterinfo = ns_capable(net->user_ns, CAP_NET_ADMIN);\n+\tmay_report_filterinfo = netlink_net_capable(cb->skb, CAP_NET_ADMIN);\n \n \tmutex_lock(&net->packet.sklist_lock);\n \tsk_for_each(sk, &net->packet.sklist) {",
        "function_modified_lines": {
            "added": [
                "\tmay_report_filterinfo = netlink_net_capable(cb->skb, CAP_NET_ADMIN);"
            ],
            "deleted": [
                "\tmay_report_filterinfo = ns_capable(net->user_ns, CAP_NET_ADMIN);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 451
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tstruct nlattr *attrs[RTA_MAX+1];\n \tint err;\n \n-\tif (!capable(CAP_NET_ADMIN))\n+\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \tif (!net_eq(net, &init_net))",
        "function_modified_lines": {
            "added": [
                "\tif (!netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif (!capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 446
    },
    {
        "cve_id": "CVE-2015-8709",
        "code_before_change": "struct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current);\n}",
        "code_after_change": "struct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current, current_user_ns());\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,5 +7,5 @@\n \t\treturn NULL;\n \n \tmemset(mm, 0, sizeof(*mm));\n-\treturn mm_init(mm, current);\n+\treturn mm_init(mm, current, current_user_ns());\n }",
        "function_modified_lines": {
            "added": [
                "\treturn mm_init(mm, current, current_user_ns());"
            ],
            "deleted": [
                "\treturn mm_init(mm, current);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/ptrace.c in the Linux kernel through 4.4.1 mishandles uid and gid mappings, which allows local users to gain privileges by establishing a user namespace, waiting for a root process to enter that namespace with an unsafe uid or gid, and then using the ptrace system call.  NOTE: the vendor states \"there is no kernel bug here.",
        "id": 836
    },
    {
        "cve_id": "CVE-2015-8709",
        "code_before_change": "static struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}",
        "code_after_change": "static struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk, mm->user_ns))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \n \tmemcpy(mm, oldmm, sizeof(*mm));\n \n-\tif (!mm_init(mm, tsk))\n+\tif (!mm_init(mm, tsk, mm->user_ns))\n \t\tgoto fail_nomem;\n \n \terr = dup_mmap(mm, oldmm);",
        "function_modified_lines": {
            "added": [
                "\tif (!mm_init(mm, tsk, mm->user_ns))"
            ],
            "deleted": [
                "\tif (!mm_init(mm, tsk))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/ptrace.c in the Linux kernel through 4.4.1 mishandles uid and gid mappings, which allows local users to gain privileges by establishing a user namespace, waiting for a root process to enter that namespace with an unsafe uid or gid, and then using the ptrace system call.  NOTE: the vendor states \"there is no kernel bug here.",
        "id": 837
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "void perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Enable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_enable, event);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tgoto out;\n\n\t/*\n\t * If the event is in error state, clear that first.\n\t * That way, if we see the event in error state below, we\n\t * know that it has gone back into error state, as distinct\n\t * from the task having been scheduled away before the\n\t * cross-call arrived.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\nretry:\n\tif (!ctx->is_active) {\n\t\t__perf_event_mark_enabled(event);\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tif (!task_function_call(task, __perf_event_enable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\n\t/*\n\t * If the context is active and the event is still off,\n\t * we need to retry the cross-call.\n\t */\n\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {\n\t\t/*\n\t\t * task could have been flipped by a concurrent\n\t\t * perf_event_context_sched_out()\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\nout:\n\traw_spin_unlock_irq(&ctx->lock);\n}",
        "code_after_change": "void perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = perf_event_ctx_lock(event);\n\t_perf_event_enable(event);\n\tperf_event_ctx_unlock(event, ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,56 +1,8 @@\n void perf_event_enable(struct perf_event *event)\n {\n-\tstruct perf_event_context *ctx = event->ctx;\n-\tstruct task_struct *task = ctx->task;\n+\tstruct perf_event_context *ctx;\n \n-\tif (!task) {\n-\t\t/*\n-\t\t * Enable the event on the cpu that it's on\n-\t\t */\n-\t\tcpu_function_call(event->cpu, __perf_event_enable, event);\n-\t\treturn;\n-\t}\n-\n-\traw_spin_lock_irq(&ctx->lock);\n-\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n-\t\tgoto out;\n-\n-\t/*\n-\t * If the event is in error state, clear that first.\n-\t * That way, if we see the event in error state below, we\n-\t * know that it has gone back into error state, as distinct\n-\t * from the task having been scheduled away before the\n-\t * cross-call arrived.\n-\t */\n-\tif (event->state == PERF_EVENT_STATE_ERROR)\n-\t\tevent->state = PERF_EVENT_STATE_OFF;\n-\n-retry:\n-\tif (!ctx->is_active) {\n-\t\t__perf_event_mark_enabled(event);\n-\t\tgoto out;\n-\t}\n-\n-\traw_spin_unlock_irq(&ctx->lock);\n-\n-\tif (!task_function_call(task, __perf_event_enable, event))\n-\t\treturn;\n-\n-\traw_spin_lock_irq(&ctx->lock);\n-\n-\t/*\n-\t * If the context is active and the event is still off,\n-\t * we need to retry the cross-call.\n-\t */\n-\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {\n-\t\t/*\n-\t\t * task could have been flipped by a concurrent\n-\t\t * perf_event_context_sched_out()\n-\t\t */\n-\t\ttask = ctx->task;\n-\t\tgoto retry;\n-\t}\n-\n-out:\n-\traw_spin_unlock_irq(&ctx->lock);\n+\tctx = perf_event_ctx_lock(event);\n+\t_perf_event_enable(event);\n+\tperf_event_ctx_unlock(event, ctx);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct perf_event_context *ctx;",
                "\tctx = perf_event_ctx_lock(event);",
                "\t_perf_event_enable(event);",
                "\tperf_event_ctx_unlock(event, ctx);"
            ],
            "deleted": [
                "\tstruct perf_event_context *ctx = event->ctx;",
                "\tstruct task_struct *task = ctx->task;",
                "\tif (!task) {",
                "\t\t/*",
                "\t\t * Enable the event on the cpu that it's on",
                "\t\t */",
                "\t\tcpu_function_call(event->cpu, __perf_event_enable, event);",
                "\t\treturn;",
                "\t}",
                "",
                "\traw_spin_lock_irq(&ctx->lock);",
                "\tif (event->state >= PERF_EVENT_STATE_INACTIVE)",
                "\t\tgoto out;",
                "",
                "\t/*",
                "\t * If the event is in error state, clear that first.",
                "\t * That way, if we see the event in error state below, we",
                "\t * know that it has gone back into error state, as distinct",
                "\t * from the task having been scheduled away before the",
                "\t * cross-call arrived.",
                "\t */",
                "\tif (event->state == PERF_EVENT_STATE_ERROR)",
                "\t\tevent->state = PERF_EVENT_STATE_OFF;",
                "",
                "retry:",
                "\tif (!ctx->is_active) {",
                "\t\t__perf_event_mark_enabled(event);",
                "\t\tgoto out;",
                "\t}",
                "",
                "\traw_spin_unlock_irq(&ctx->lock);",
                "",
                "\tif (!task_function_call(task, __perf_event_enable, event))",
                "\t\treturn;",
                "",
                "\traw_spin_lock_irq(&ctx->lock);",
                "",
                "\t/*",
                "\t * If the context is active and the event is still off,",
                "\t * we need to retry the cross-call.",
                "\t */",
                "\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {",
                "\t\t/*",
                "\t\t * task could have been flipped by a concurrent",
                "\t\t * perf_event_context_sched_out()",
                "\t\t */",
                "\t\ttask = ctx->task;",
                "\t\tgoto retry;",
                "\t}",
                "",
                "out:",
                "\traw_spin_unlock_irq(&ctx->lock);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1086
    },
    {
        "cve_id": "CVE-2016-3857",
        "code_before_change": "asmlinkage long sys_oabi_epoll_wait(int epfd,\n\t\t\t\t    struct oabi_epoll_event __user *events,\n\t\t\t\t    int maxevents, int timeout)\n{\n\tstruct epoll_event *kbuf;\n\tmm_segment_t fs;\n\tlong ret, err, i;\n\n\tif (maxevents <= 0 || maxevents > (INT_MAX/sizeof(struct epoll_event)))\n\t\treturn -EINVAL;\n\tkbuf = kmalloc(sizeof(*kbuf) * maxevents, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\tret = sys_epoll_wait(epfd, kbuf, maxevents, timeout);\n\tset_fs(fs);\n\terr = 0;\n\tfor (i = 0; i < ret; i++) {\n\t\t__put_user_error(kbuf[i].events, &events->events, err);\n\t\t__put_user_error(kbuf[i].data,   &events->data,   err);\n\t\tevents++;\n\t}\n\tkfree(kbuf);\n\treturn err ? -EFAULT : ret;\n}",
        "code_after_change": "asmlinkage long sys_oabi_epoll_wait(int epfd,\n\t\t\t\t    struct oabi_epoll_event __user *events,\n\t\t\t\t    int maxevents, int timeout)\n{\n\tstruct epoll_event *kbuf;\n\tmm_segment_t fs;\n\tlong ret, err, i;\n\n\tif (maxevents <= 0 ||\n\t\t\tmaxevents > (INT_MAX/sizeof(*kbuf)) ||\n\t\t\tmaxevents > (INT_MAX/sizeof(*events)))\n\t\treturn -EINVAL;\n\tif (!access_ok(VERIFY_WRITE, events, sizeof(*events) * maxevents))\n\t\treturn -EFAULT;\n\tkbuf = kmalloc(sizeof(*kbuf) * maxevents, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\tret = sys_epoll_wait(epfd, kbuf, maxevents, timeout);\n\tset_fs(fs);\n\terr = 0;\n\tfor (i = 0; i < ret; i++) {\n\t\t__put_user_error(kbuf[i].events, &events->events, err);\n\t\t__put_user_error(kbuf[i].data,   &events->data,   err);\n\t\tevents++;\n\t}\n\tkfree(kbuf);\n\treturn err ? -EFAULT : ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,12 @@\n \tmm_segment_t fs;\n \tlong ret, err, i;\n \n-\tif (maxevents <= 0 || maxevents > (INT_MAX/sizeof(struct epoll_event)))\n+\tif (maxevents <= 0 ||\n+\t\t\tmaxevents > (INT_MAX/sizeof(*kbuf)) ||\n+\t\t\tmaxevents > (INT_MAX/sizeof(*events)))\n \t\treturn -EINVAL;\n+\tif (!access_ok(VERIFY_WRITE, events, sizeof(*events) * maxevents))\n+\t\treturn -EFAULT;\n \tkbuf = kmalloc(sizeof(*kbuf) * maxevents, GFP_KERNEL);\n \tif (!kbuf)\n \t\treturn -ENOMEM;",
        "function_modified_lines": {
            "added": [
                "\tif (maxevents <= 0 ||",
                "\t\t\tmaxevents > (INT_MAX/sizeof(*kbuf)) ||",
                "\t\t\tmaxevents > (INT_MAX/sizeof(*events)))",
                "\tif (!access_ok(VERIFY_WRITE, events, sizeof(*events) * maxevents))",
                "\t\treturn -EFAULT;"
            ],
            "deleted": [
                "\tif (maxevents <= 0 || maxevents > (INT_MAX/sizeof(struct epoll_event)))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The kernel in Android before 2016-08-05 on Nexus 7 (2013) devices allows attackers to gain privileges via a crafted application, aka internal bug 28522518.",
        "id": 1010
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\n\t\tlock_sock(sk);\n\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n\t\t\t\t\t     optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tlock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -141,10 +141,11 @@\n \tcase IPV6_RTHDR:\n \tcase IPV6_DSTOPTS:\n \t{\n+\t\tstruct ipv6_txoptions *opt;\n \n \t\tlock_sock(sk);\n-\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n-\t\t\t\t\t     optname, optval, len);\n+\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n \t\trelease_sock(sk);\n \t\t/* check if ipv6_getsockopt_sticky() returns err code */\n \t\tif (len < 0)",
        "function_modified_lines": {
            "added": [
                "\t\tstruct ipv6_txoptions *opt;",
                "\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);"
            ],
            "deleted": [
                "\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,",
                "\t\t\t\t\t     optname, optval, len);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1002
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "code_after_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n \tstruct dccp_sock *dp = dccp_sk(sk);\n \tstruct in6_addr *saddr = NULL, *final_p, final;\n+\tstruct ipv6_txoptions *opt;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n \tint addr_type;\n@@ -106,7 +107,8 @@\n \tfl6.fl6_sport = inet->inet_sport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \tif (IS_ERR(dst)) {\n@@ -126,9 +128,8 @@\n \t__ip6_dst_store(sk, dst, NULL, NULL);\n \n \ticsk->icsk_ext_hdr_len = 0;\n-\tif (np->opt != NULL)\n-\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n-\t\t\t\t\t  np->opt->opt_nflen);\n+\tif (opt)\n+\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n \n \tinet->inet_dport = usin->sin6_port;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\tfinal_p = fl6_update_dst(&fl6, opt, &final);",
                "\tif (opt)",
                "\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);",
                "\tif (np->opt != NULL)",
                "\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +",
                "\t\t\t\t\t  np->opt->opt_nflen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 990
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
        "code_after_change": "int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n \tstruct ipv6_txoptions *opt = NULL;\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ip6_flowlabel *flowlabel = NULL;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n@@ -160,8 +161,10 @@\n \t\t\topt = NULL;\n \t\tconnected = 0;\n \t}\n-\tif (!opt)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -270,6 +273,7 @@\n out:\n \tdst_release(dst);\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \tif (!err)\n \t\treturn len;\n \t/*",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt_to_free = NULL;",
                "\tif (!opt) {",
                "\t\topt = txopt_get(np);",
                "\t\topt_to_free = opt;",
                "\t}",
                "\ttxopt_put(opt_to_free);"
            ],
            "deleted": [
                "\tif (!opt)",
                "\t\topt = np->opt;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1008
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}",
        "code_after_change": "struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -87,7 +87,7 @@\n \t\tmemset(&fl6, 0, sizeof(fl6));\n \t\tfl6.flowi6_proto = IPPROTO_TCP;\n \t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n-\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n \t\tfl6.saddr = ireq->ir_v6_loc_addr;\n \t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n \t\tfl6.flowi6_mark = ireq->ir_mark;",
        "function_modified_lines": {
            "added": [
                "\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);"
            ],
            "deleted": [
                "\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1004
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg(&np->opt, NULL);\n\t\t\tif (opt)\n\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = ipv6_renew_options(sk, np->opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "code_after_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -93,9 +93,12 @@\n \t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n \t\t\t\tsk->sk_family = PF_INET;\n \t\t\t}\n-\t\t\topt = xchg(&np->opt, NULL);\n-\t\t\tif (opt)\n-\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n+\t\t\t\t   NULL);\n+\t\t\tif (opt) {\n+\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\t\ttxopt_put(opt);\n+\t\t\t}\n \t\t\tpktopt = xchg(&np->pktoptions, NULL);\n \t\t\tkfree_skb(pktopt);\n \n@@ -265,7 +268,8 @@\n \t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n \t\t\tbreak;\n \n-\t\topt = ipv6_renew_options(sk, np->opt, optname,\n+\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\t\topt = ipv6_renew_options(sk, opt, optname,\n \t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n \t\t\t\t\t optlen);\n \t\tif (IS_ERR(opt)) {\n@@ -294,8 +298,10 @@\n \t\tretv = 0;\n \t\topt = ipv6_update_options(sk, opt);\n sticky_done:\n-\t\tif (opt)\n-\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\tif (opt) {\n+\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\ttxopt_put(opt);\n+\t\t}\n \t\tbreak;\n \t}\n \n@@ -348,6 +354,7 @@\n \t\t\tbreak;\n \n \t\tmemset(opt, 0, sizeof(*opt));\n+\t\tatomic_set(&opt->refcnt, 1);\n \t\topt->tot_len = sizeof(*opt) + optlen;\n \t\tretv = -EFAULT;\n \t\tif (copy_from_user(opt+1, optval, optlen))\n@@ -364,8 +371,10 @@\n \t\tretv = 0;\n \t\topt = ipv6_update_options(sk, opt);\n done:\n-\t\tif (opt)\n-\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\tif (opt) {\n+\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\ttxopt_put(opt);\n+\t\t}\n \t\tbreak;\n \t}\n \tcase IPV6_UNICAST_HOPS:",
        "function_modified_lines": {
            "added": [
                "\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,",
                "\t\t\t\t   NULL);",
                "\t\t\tif (opt) {",
                "\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\t\ttxopt_put(opt);",
                "\t\t\t}",
                "\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\t\topt = ipv6_renew_options(sk, opt, optname,",
                "\t\tif (opt) {",
                "\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\ttxopt_put(opt);",
                "\t\t}",
                "\t\tatomic_set(&opt->refcnt, 1);",
                "\t\tif (opt) {",
                "\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\ttxopt_put(opt);",
                "\t\t}"
            ],
            "deleted": [
                "\t\t\topt = xchg(&np->opt, NULL);",
                "\t\t\tif (opt)",
                "\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);",
                "\t\topt = ipv6_renew_options(sk, np->opt, optname,",
                "\t\tif (opt)",
                "\t\t\tsock_kfree_s(sk, opt, opt->tot_len);",
                "\t\tif (opt)",
                "\t\t\tsock_kfree_s(sk, opt, opt->tot_len);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1001
    },
    {
        "cve_id": "CVE-2014-4014",
        "code_before_change": "int generic_permission(struct inode *inode, int mask)\n{\n\tint ret;\n\n\t/*\n\t * Do the basic permission checks.\n\t */\n\tret = acl_permission_check(inode, mask);\n\tif (ret != -EACCES)\n\t\treturn ret;\n\n\tif (S_ISDIR(inode->i_mode)) {\n\t\t/* DACs are overridable for directories */\n\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\t\tif (!(mask & MAY_WRITE))\n\t\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))\n\t\t\t\treturn 0;\n\t\treturn -EACCES;\n\t}\n\t/*\n\t * Read/write DACs are always overridable.\n\t * Executable DACs are overridable when there is\n\t * at least one exec bit set.\n\t */\n\tif (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))\n\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\n\t/*\n\t * Searching includes executable on directories, else just read.\n\t */\n\tmask &= MAY_READ | MAY_WRITE | MAY_EXEC;\n\tif (mask == MAY_READ)\n\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))\n\t\t\treturn 0;\n\n\treturn -EACCES;\n}",
        "code_after_change": "int generic_permission(struct inode *inode, int mask)\n{\n\tint ret;\n\n\t/*\n\t * Do the basic permission checks.\n\t */\n\tret = acl_permission_check(inode, mask);\n\tif (ret != -EACCES)\n\t\treturn ret;\n\n\tif (S_ISDIR(inode->i_mode)) {\n\t\t/* DACs are overridable for directories */\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\t\tif (!(mask & MAY_WRITE))\n\t\t\tif (capable_wrt_inode_uidgid(inode,\n\t\t\t\t\t\t     CAP_DAC_READ_SEARCH))\n\t\t\t\treturn 0;\n\t\treturn -EACCES;\n\t}\n\t/*\n\t * Read/write DACs are always overridable.\n\t * Executable DACs are overridable when there is\n\t * at least one exec bit set.\n\t */\n\tif (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\n\t/*\n\t * Searching includes executable on directories, else just read.\n\t */\n\tmask &= MAY_READ | MAY_WRITE | MAY_EXEC;\n\tif (mask == MAY_READ)\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_READ_SEARCH))\n\t\t\treturn 0;\n\n\treturn -EACCES;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,10 +11,11 @@\n \n \tif (S_ISDIR(inode->i_mode)) {\n \t\t/* DACs are overridable for directories */\n-\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))\n+\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n \t\t\treturn 0;\n \t\tif (!(mask & MAY_WRITE))\n-\t\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))\n+\t\t\tif (capable_wrt_inode_uidgid(inode,\n+\t\t\t\t\t\t     CAP_DAC_READ_SEARCH))\n \t\t\t\treturn 0;\n \t\treturn -EACCES;\n \t}\n@@ -24,7 +25,7 @@\n \t * at least one exec bit set.\n \t */\n \tif (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))\n-\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))\n+\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n \t\t\treturn 0;\n \n \t/*\n@@ -32,7 +33,7 @@\n \t */\n \tmask &= MAY_READ | MAY_WRITE | MAY_EXEC;\n \tif (mask == MAY_READ)\n-\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))\n+\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_READ_SEARCH))\n \t\t\treturn 0;\n \n \treturn -EACCES;",
        "function_modified_lines": {
            "added": [
                "\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))",
                "\t\t\tif (capable_wrt_inode_uidgid(inode,",
                "\t\t\t\t\t\t     CAP_DAC_READ_SEARCH))",
                "\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))",
                "\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_READ_SEARCH))"
            ],
            "deleted": [
                "\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))",
                "\t\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))",
                "\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))",
                "\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The capabilities implementation in the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local users to bypass intended chmod restrictions by first creating a user namespace, as demonstrated by setting the setgid bit on a file with group ownership of root.",
        "id": 554
    },
    {
        "cve_id": "CVE-2014-4014",
        "code_before_change": "void setattr_copy(struct inode *inode, const struct iattr *attr)\n{\n\tunsigned int ia_valid = attr->ia_valid;\n\n\tif (ia_valid & ATTR_UID)\n\t\tinode->i_uid = attr->ia_uid;\n\tif (ia_valid & ATTR_GID)\n\t\tinode->i_gid = attr->ia_gid;\n\tif (ia_valid & ATTR_ATIME)\n\t\tinode->i_atime = timespec_trunc(attr->ia_atime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_MTIME)\n\t\tinode->i_mtime = timespec_trunc(attr->ia_mtime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_CTIME)\n\t\tinode->i_ctime = timespec_trunc(attr->ia_ctime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_MODE) {\n\t\tumode_t mode = attr->ia_mode;\n\n\t\tif (!in_group_p(inode->i_gid) &&\n\t\t    !inode_capable(inode, CAP_FSETID))\n\t\t\tmode &= ~S_ISGID;\n\t\tinode->i_mode = mode;\n\t}\n}",
        "code_after_change": "void setattr_copy(struct inode *inode, const struct iattr *attr)\n{\n\tunsigned int ia_valid = attr->ia_valid;\n\n\tif (ia_valid & ATTR_UID)\n\t\tinode->i_uid = attr->ia_uid;\n\tif (ia_valid & ATTR_GID)\n\t\tinode->i_gid = attr->ia_gid;\n\tif (ia_valid & ATTR_ATIME)\n\t\tinode->i_atime = timespec_trunc(attr->ia_atime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_MTIME)\n\t\tinode->i_mtime = timespec_trunc(attr->ia_mtime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_CTIME)\n\t\tinode->i_ctime = timespec_trunc(attr->ia_ctime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_MODE) {\n\t\tumode_t mode = attr->ia_mode;\n\n\t\tif (!in_group_p(inode->i_gid) &&\n\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))\n\t\t\tmode &= ~S_ISGID;\n\t\tinode->i_mode = mode;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,7 @@\n \t\tumode_t mode = attr->ia_mode;\n \n \t\tif (!in_group_p(inode->i_gid) &&\n-\t\t    !inode_capable(inode, CAP_FSETID))\n+\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))\n \t\t\tmode &= ~S_ISGID;\n \t\tinode->i_mode = mode;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))"
            ],
            "deleted": [
                "\t\t    !inode_capable(inode, CAP_FSETID))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The capabilities implementation in the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local users to bypass intended chmod restrictions by first creating a user namespace, as demonstrated by setting the setgid bit on a file with group ownership of root.",
        "id": 550
    },
    {
        "cve_id": "CVE-2015-1593",
        "code_before_change": "static unsigned long randomize_stack_top(unsigned long stack_top)\n{\n\tunsigned int random_variable = 0;\n\n\tif ((current->flags & PF_RANDOMIZE) &&\n\t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n\t\trandom_variable = get_random_int() & STACK_RND_MASK;\n\t\trandom_variable <<= PAGE_SHIFT;\n\t}\n#ifdef CONFIG_STACK_GROWSUP\n\treturn PAGE_ALIGN(stack_top) + random_variable;\n#else\n\treturn PAGE_ALIGN(stack_top) - random_variable;\n#endif\n}",
        "code_after_change": "static unsigned long randomize_stack_top(unsigned long stack_top)\n{\n\tunsigned long random_variable = 0;\n\n\tif ((current->flags & PF_RANDOMIZE) &&\n\t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n\t\trandom_variable = (unsigned long) get_random_int();\n\t\trandom_variable &= STACK_RND_MASK;\n\t\trandom_variable <<= PAGE_SHIFT;\n\t}\n#ifdef CONFIG_STACK_GROWSUP\n\treturn PAGE_ALIGN(stack_top) + random_variable;\n#else\n\treturn PAGE_ALIGN(stack_top) - random_variable;\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,11 @@\n static unsigned long randomize_stack_top(unsigned long stack_top)\n {\n-\tunsigned int random_variable = 0;\n+\tunsigned long random_variable = 0;\n \n \tif ((current->flags & PF_RANDOMIZE) &&\n \t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n-\t\trandom_variable = get_random_int() & STACK_RND_MASK;\n+\t\trandom_variable = (unsigned long) get_random_int();\n+\t\trandom_variable &= STACK_RND_MASK;\n \t\trandom_variable <<= PAGE_SHIFT;\n \t}\n #ifdef CONFIG_STACK_GROWSUP",
        "function_modified_lines": {
            "added": [
                "\tunsigned long random_variable = 0;",
                "\t\trandom_variable = (unsigned long) get_random_int();",
                "\t\trandom_variable &= STACK_RND_MASK;"
            ],
            "deleted": [
                "\tunsigned int random_variable = 0;",
                "\t\trandom_variable = get_random_int() & STACK_RND_MASK;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The stack randomization feature in the Linux kernel before 3.19.1 on 64-bit platforms uses incorrect data types for the results of bitwise left-shift operations, which makes it easier for attackers to bypass the ASLR protection mechanism by predicting the address of the top of the stack, related to the randomize_stack_top function in fs/binfmt_elf.c and the stack_maxrandom_size function in arch/x86/mm/mmap.c.",
        "id": 739
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}",
        "code_after_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,4 @@\n struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n {\n-\tstruct request *rq = tags->rqs[tag];\n-\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n-\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n-\n-\tif (!is_flush_request(rq, fq, tag))\n-\t\treturn rq;\n-\n-\treturn fq->flush_rq;\n+\treturn tags->rqs[tag];\n }",
        "function_modified_lines": {
            "added": [
                "\treturn tags->rqs[tag];"
            ],
            "deleted": [
                "\tstruct request *rq = tags->rqs[tag];",
                "\t/* mq_ctx of flush rq is always cloned from the corresponding req */",
                "\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);",
                "",
                "\tif (!is_flush_request(rq, fq, tag))",
                "\t\treturn rq;",
                "",
                "\treturn fq->flush_rq;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "id": 885
    },
    {
        "cve_id": "CVE-2016-9644",
        "code_before_change": "void sort_extable(struct exception_table_entry *start,\n\t\t  struct exception_table_entry *finish)\n{\n\tstruct exception_table_entry *p;\n\tint i;\n\n\t/* Convert all entries to being relative to the start of the section */\n\ti = 0;\n\tfor (p = start; p < finish; p++) {\n\t\tp->insn += i;\n\t\ti += 4;\n\t\tp->fixup += i;\n\t\ti += 4;\n\t}\n\n\tsort(start, finish - start, sizeof(struct exception_table_entry),\n\t     cmp_ex, NULL);\n\n\t/* Denormalize all entries */\n\ti = 0;\n\tfor (p = start; p < finish; p++) {\n\t\tp->insn -= i;\n\t\ti += 4;\n\t\tp->fixup -= i;\n\t\ti += 4;\n\t}\n}",
        "code_after_change": "void sort_extable(struct exception_table_entry *start,\n\t\t  struct exception_table_entry *finish)\n{\n\tstruct exception_table_entry *p;\n\tint i;\n\n\t/* Convert all entries to being relative to the start of the section */\n\ti = 0;\n\tfor (p = start; p < finish; p++) {\n\t\tp->insn += i;\n\t\ti += 4;\n\t\tp->fixup += i;\n\t\ti += 4;\n\t\tp->handler += i;\n\t\ti += 4;\n\t}\n\n\tsort(start, finish - start, sizeof(struct exception_table_entry),\n\t     cmp_ex, NULL);\n\n\t/* Denormalize all entries */\n\ti = 0;\n\tfor (p = start; p < finish; p++) {\n\t\tp->insn -= i;\n\t\ti += 4;\n\t\tp->fixup -= i;\n\t\ti += 4;\n\t\tp->handler -= i;\n\t\ti += 4;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,8 @@\n \t\ti += 4;\n \t\tp->fixup += i;\n \t\ti += 4;\n+\t\tp->handler += i;\n+\t\ti += 4;\n \t}\n \n \tsort(start, finish - start, sizeof(struct exception_table_entry),\n@@ -23,5 +25,7 @@\n \t\ti += 4;\n \t\tp->fixup -= i;\n \t\ti += 4;\n+\t\tp->handler -= i;\n+\t\ti += 4;\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tp->handler += i;",
                "\t\ti += 4;",
                "\t\tp->handler -= i;",
                "\t\ti += 4;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel 4.4.22 through 4.4.28 contains extended asm statements that are incompatible with the exception table, which allows local users to obtain root access on non-SMEP platforms via a crafted application.  NOTE: this vulnerability exists because of incorrect backporting of the CVE-2016-9178 patch to older kernels.",
        "id": 1156
    },
    {
        "cve_id": "CVE-2014-8133",
        "code_before_change": "int do_set_thread_area(struct task_struct *p, int idx,\n\t\t       struct user_desc __user *u_info,\n\t\t       int can_allocate)\n{\n\tstruct user_desc info;\n\n\tif (copy_from_user(&info, u_info, sizeof(info)))\n\t\treturn -EFAULT;\n\n\tif (idx == -1)\n\t\tidx = info.entry_number;\n\n\t/*\n\t * index -1 means the kernel should try to find and\n\t * allocate an empty descriptor:\n\t */\n\tif (idx == -1 && can_allocate) {\n\t\tidx = get_free_idx();\n\t\tif (idx < 0)\n\t\t\treturn idx;\n\t\tif (put_user(idx, &u_info->entry_number))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)\n\t\treturn -EINVAL;\n\n\tset_tls_desc(p, idx, &info, 1);\n\n\treturn 0;\n}",
        "code_after_change": "int do_set_thread_area(struct task_struct *p, int idx,\n\t\t       struct user_desc __user *u_info,\n\t\t       int can_allocate)\n{\n\tstruct user_desc info;\n\n\tif (copy_from_user(&info, u_info, sizeof(info)))\n\t\treturn -EFAULT;\n\n\tif (!tls_desc_okay(&info))\n\t\treturn -EINVAL;\n\n\tif (idx == -1)\n\t\tidx = info.entry_number;\n\n\t/*\n\t * index -1 means the kernel should try to find and\n\t * allocate an empty descriptor:\n\t */\n\tif (idx == -1 && can_allocate) {\n\t\tidx = get_free_idx();\n\t\tif (idx < 0)\n\t\t\treturn idx;\n\t\tif (put_user(idx, &u_info->entry_number))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)\n\t\treturn -EINVAL;\n\n\tset_tls_desc(p, idx, &info, 1);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,9 @@\n \n \tif (copy_from_user(&info, u_info, sizeof(info)))\n \t\treturn -EFAULT;\n+\n+\tif (!tls_desc_okay(&info))\n+\t\treturn -EINVAL;\n \n \tif (idx == -1)\n \t\tidx = info.entry_number;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!tls_desc_okay(&info))",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/x86/kernel/tls.c in the Thread Local Storage (TLS) implementation in the Linux kernel through 3.18.1 allows local users to bypass the espfix protection mechanism, and consequently makes it easier for local users to bypass the ASLR protection mechanism, via a crafted application that makes a set_thread_area system call and later reads a 16-bit value.",
        "id": 602
    },
    {
        "cve_id": "CVE-2016-3157",
        "code_before_change": "static void xen_set_iopl_mask(unsigned mask)\n{\n\tstruct physdev_set_iopl set_iopl;\n\n\t/* Force the change at ring 0. */\n\tset_iopl.iopl = (mask == 0) ? 1 : (mask >> 12) & 3;\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);\n}",
        "code_after_change": "void xen_set_iopl_mask(unsigned mask)\n{\n\tstruct physdev_set_iopl set_iopl;\n\n\t/* Force the change at ring 0. */\n\tset_iopl.iopl = (mask == 0) ? 1 : (mask >> 12) & 3;\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static void xen_set_iopl_mask(unsigned mask)\n+void xen_set_iopl_mask(unsigned mask)\n {\n \tstruct physdev_set_iopl set_iopl;\n ",
        "function_modified_lines": {
            "added": [
                "void xen_set_iopl_mask(unsigned mask)"
            ],
            "deleted": [
                "static void xen_set_iopl_mask(unsigned mask)"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __switch_to function in arch/x86/kernel/process_64.c in the Linux kernel does not properly context-switch IOPL on 64-bit PV Xen guests, which allows local guest OS users to gain privileges, cause a denial of service (guest OS crash), or obtain sensitive information by leveraging I/O port access.",
        "id": 986
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "code_after_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tint addr_type;\n \tint err;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET6)\n \t\treturn -EINVAL;\n \tif (addr_len < sizeof(*addr))\n@@ -35,6 +33,9 @@\n \tlock_sock(sk);\n \n \terr = -EINVAL;\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out_unlock;\n+\n \tif (sk->sk_state != TCP_CLOSE)\n \t\tgoto out_unlock;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out_unlock;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "id": 899
    },
    {
        "cve_id": "CVE-2016-4997",
        "code_before_change": "static int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
        "code_after_change": "static int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,7 +27,8 @@\n \tif (!ip6_checkentry(&e->ipv6))\n \t\treturn -EINVAL;\n \n-\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n+\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n+\t\t\t\t     e->next_offset);\n \tif (err)\n \t\treturn err;\n ",
        "function_modified_lines": {
            "added": [
                "\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,",
                "\t\t\t\t     e->next_offset);"
            ],
            "deleted": [
                "\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The compat IPT_SO_SET_REPLACE and IP6T_SO_SET_REPLACE setsockopt implementations in the netfilter subsystem in the Linux kernel before 4.6.3 allow local users to gain privileges or cause a denial of service (memory corruption) by leveraging in-container root access to provide a crafted offset value that triggers an unintended decrement.",
        "id": 1043
    },
    {
        "cve_id": "CVE-2016-4997",
        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,7 @@\n \tif (!ip_checkentry(&e->ip))\n \t\treturn -EINVAL;\n \n-\tret = xt_compat_check_entry_offsets(e,\n+\tret = xt_compat_check_entry_offsets(e, e->elems,\n \t\t\t\t\t    e->target_offset, e->next_offset);\n \tif (ret)\n \t\treturn ret;",
        "function_modified_lines": {
            "added": [
                "\tret = xt_compat_check_entry_offsets(e, e->elems,"
            ],
            "deleted": [
                "\tret = xt_compat_check_entry_offsets(e,"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The compat IPT_SO_SET_REPLACE and IP6T_SO_SET_REPLACE setsockopt implementations in the netfilter subsystem in the Linux kernel before 4.6.3 allow local users to gain privileges or cause a denial of service (memory corruption) by leveraging in-container root access to provide a crafted offset value that triggers an unintended decrement.",
        "id": 1042
    },
    {
        "cve_id": "CVE-2016-8632",
        "code_before_change": "static int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}",
        "code_after_change": "static int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +\n\t\t\t\t      sizeof(struct udphdr))) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -55,6 +55,11 @@\n \t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n \t\tudp_conf.use_udp_checksums = false;\n \t\tub->ifindex = dev->ifindex;\n+\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +\n+\t\t\t\t      sizeof(struct udphdr))) {\n+\t\t\terr = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n \t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n \t\t\t- sizeof(struct udphdr);\n #if IS_ENABLED(CONFIG_IPV6)",
        "function_modified_lines": {
            "added": [
                "\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +",
                "\t\t\t\t      sizeof(struct udphdr))) {",
                "\t\t\terr = -EINVAL;",
                "\t\t\tgoto err;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264",
            "CWE-119"
        ],
        "cve_description": "The tipc_msg_build function in net/tipc/msg.c in the Linux kernel through 4.8.11 does not validate the relationship between the minimum fragment length and the maximum packet size, which allows local users to gain privileges or cause a denial of service (heap-based buffer overflow) by leveraging the CAP_NET_ADMIN capability.",
        "id": 1122
    },
    {
        "cve_id": "CVE-2013-4299",
        "code_before_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\treturn 0;\n}",
        "code_after_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\tskip_metadata(ps);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,5 +21,7 @@\n \n \tps->current_area--;\n \n+\tskip_metadata(ps);\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tskip_metadata(ps);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264",
            "CWE-200"
        ],
        "cve_description": "Interpretation conflict in drivers/md/dm-snap-persistent.c in the Linux kernel through 3.11.6 allows remote authenticated users to obtain sensitive information or modify data via a crafted mapping to a snapshot block device.",
        "id": 292
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "code_after_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,11 +43,15 @@\n \t{\n \t\tstruct ion_handle *handle;\n \n-\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n-\t\tif (IS_ERR(handle))\n+\t\tmutex_lock(&client->lock);\n+\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n+\t\tif (IS_ERR(handle)) {\n+\t\t\tmutex_unlock(&client->lock);\n \t\t\treturn PTR_ERR(handle);\n-\t\tion_free(client, handle);\n-\t\tion_handle_put(handle);\n+\t\t}\n+\t\tion_free_nolock(client, handle);\n+\t\tion_handle_put_nolock(handle);\n+\t\tmutex_unlock(&client->lock);\n \t\tbreak;\n \t}\n \tcase ION_IOC_SHARE:",
        "function_modified_lines": {
            "added": [
                "\t\tmutex_lock(&client->lock);",
                "\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);",
                "\t\tif (IS_ERR(handle)) {",
                "\t\t\tmutex_unlock(&client->lock);",
                "\t\t}",
                "\t\tion_free_nolock(client, handle);",
                "\t\tion_handle_put_nolock(handle);",
                "\t\tmutex_unlock(&client->lock);"
            ],
            "deleted": [
                "\t\thandle = ion_handle_get_by_id(client, data.handle.handle);",
                "\t\tif (IS_ERR(handle))",
                "\t\tion_free(client, handle);",
                "\t\tion_handle_put(handle);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1140
    },
    {
        "cve_id": "CVE-2014-9870",
        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
        "code_after_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -51,7 +51,7 @@\n \t\treturn regs->ARM_r0;\n \n \tcase NR(set_tls):\n-\t\tthread->tp_value = regs->ARM_r0;\n+\t\tthread->tp_value[0] = regs->ARM_r0;\n \t\tif (tls_emu)\n \t\t\treturn 0;\n \t\tif (has_tls_reg) {",
        "function_modified_lines": {
            "added": [
                "\t\tthread->tp_value[0] = regs->ARM_r0;"
            ],
            "deleted": [
                "\t\tthread->tp_value = regs->ARM_r0;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Linux kernel before 3.11 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly consider user-space access to the TPIDRURW register, which allows local users to gain privileges via a crafted application, aka Android internal bug 28749743 and Qualcomm internal bug CR561044.",
        "id": 704
    },
    {
        "cve_id": "CVE-2014-9888",
        "code_before_change": "void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t    gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, false,\n\t\t\t   __builtin_return_address(0));\n}",
        "code_after_change": "void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t    gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, false,\n\t\t\t   __builtin_return_address(0));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n \t\t    gfp_t gfp, struct dma_attrs *attrs)\n {\n-\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n+\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n \tvoid *memory;\n \n \tif (dma_alloc_from_coherent(dev, size, handle, &memory))",
        "function_modified_lines": {
            "added": [
                "\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);"
            ],
            "deleted": [
                "\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/arm/mm/dma-mapping.c in the Linux kernel before 3.13 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not prevent executable DMA mappings, which might allow local users to gain privileges via a crafted application, aka Android internal bug 28803642 and Qualcomm internal bug CR642735.",
        "id": 707
    },
    {
        "cve_id": "CVE-2015-2686",
        "code_before_change": "\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\tif (size > INT_MAX)\n\t\tsize = INT_MAX;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tiov.iov_len = size;\n\tiov.iov_base = ubuf;\n\tiov_iter_init(&msg.msg_iter, READ, &iov, 1, size);\n\t/* Save some cycles and don't copy the address if not needed */\n\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg.msg_namelen = 0;\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, size, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "code_after_change": "\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\tif (size > INT_MAX)\n\t\tsize = INT_MAX;\n\tif (unlikely(!access_ok(VERIFY_WRITE, ubuf, size)))\n\t\treturn -EFAULT;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tiov.iov_len = size;\n\tiov.iov_base = ubuf;\n\tiov_iter_init(&msg.msg_iter, READ, &iov, 1, size);\n\t/* Save some cycles and don't copy the address if not needed */\n\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg.msg_namelen = 0;\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, size, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,8 @@\n \n \tif (size > INT_MAX)\n \t\tsize = INT_MAX;\n+\tif (unlikely(!access_ok(VERIFY_WRITE, ubuf, size)))\n+\t\treturn -EFAULT;\n \tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n \tif (!sock)\n \t\tgoto out;",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(!access_ok(VERIFY_WRITE, ubuf, size)))",
                "\t\treturn -EFAULT;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "net/socket.c in the Linux kernel 3.19 before 3.19.3 does not validate certain range data for (1) sendto and (2) recvfrom system calls, which allows local users to gain privileges by leveraging a subsystem that uses the copy_from_iter function in the iov_iter interface, as demonstrated by the Bluetooth subsystem.",
        "id": 745
    },
    {
        "cve_id": "CVE-2016-4440",
        "code_before_change": "static __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tif (enable_apicv) {\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t\t/* According SDM, in x2apic mode, the whole id reg is used.\n\t\t * But in KVM, it only use the highest eight bits. Need to\n\t\t * intercept it */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t\t/* TMCCT */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t\t/* TPR */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t\t/* EOI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t\t/* SELF-IPI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\t}\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}",
        "code_after_change": "static __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t/* According SDM, in x2apic mode, the whole id reg is used.  But in\n\t * KVM, it only use the highest eight bits. Need to intercept it */\n\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t/* TMCCT */\n\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t/* TPR */\n\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t/* EOI */\n\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t/* SELF-IPI */\n\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -137,23 +137,20 @@\n \n \tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n \n-\tif (enable_apicv) {\n-\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n-\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n-\n-\t\t/* According SDM, in x2apic mode, the whole id reg is used.\n-\t\t * But in KVM, it only use the highest eight bits. Need to\n-\t\t * intercept it */\n-\t\tvmx_enable_intercept_msr_read_x2apic(0x802);\n-\t\t/* TMCCT */\n-\t\tvmx_enable_intercept_msr_read_x2apic(0x839);\n-\t\t/* TPR */\n-\t\tvmx_disable_intercept_msr_write_x2apic(0x808);\n-\t\t/* EOI */\n-\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n-\t\t/* SELF-IPI */\n-\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n-\t}\n+\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n+\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n+\n+\t/* According SDM, in x2apic mode, the whole id reg is used.  But in\n+\t * KVM, it only use the highest eight bits. Need to intercept it */\n+\tvmx_enable_intercept_msr_read_x2apic(0x802);\n+\t/* TMCCT */\n+\tvmx_enable_intercept_msr_read_x2apic(0x839);\n+\t/* TPR */\n+\tvmx_disable_intercept_msr_write_x2apic(0x808);\n+\t/* EOI */\n+\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n+\t/* SELF-IPI */\n+\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n \n \tif (enable_ept) {\n \t\tkvm_mmu_set_mask_ptes(0ull,",
        "function_modified_lines": {
            "added": [
                "\tfor (msr = 0x800; msr <= 0x8ff; msr++)",
                "\t\tvmx_disable_intercept_msr_read_x2apic(msr);",
                "",
                "\t/* According SDM, in x2apic mode, the whole id reg is used.  But in",
                "\t * KVM, it only use the highest eight bits. Need to intercept it */",
                "\tvmx_enable_intercept_msr_read_x2apic(0x802);",
                "\t/* TMCCT */",
                "\tvmx_enable_intercept_msr_read_x2apic(0x839);",
                "\t/* TPR */",
                "\tvmx_disable_intercept_msr_write_x2apic(0x808);",
                "\t/* EOI */",
                "\tvmx_disable_intercept_msr_write_x2apic(0x80b);",
                "\t/* SELF-IPI */",
                "\tvmx_disable_intercept_msr_write_x2apic(0x83f);"
            ],
            "deleted": [
                "\tif (enable_apicv) {",
                "\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)",
                "\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);",
                "",
                "\t\t/* According SDM, in x2apic mode, the whole id reg is used.",
                "\t\t * But in KVM, it only use the highest eight bits. Need to",
                "\t\t * intercept it */",
                "\t\tvmx_enable_intercept_msr_read_x2apic(0x802);",
                "\t\t/* TMCCT */",
                "\t\tvmx_enable_intercept_msr_read_x2apic(0x839);",
                "\t\t/* TPR */",
                "\t\tvmx_disable_intercept_msr_write_x2apic(0x808);",
                "\t\t/* EOI */",
                "\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);",
                "\t\t/* SELF-IPI */",
                "\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/x86/kvm/vmx.c in the Linux kernel through 4.6.3 mishandles the APICv on/off state, which allows guest OS users to obtain direct APIC MSR access on the host OS, and consequently cause a denial of service (host OS crash) or possibly execute arbitrary code on the host OS, via x2APIC mode.",
        "id": 1016
    },
    {
        "cve_id": "CVE-2013-1858",
        "code_before_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
        "code_after_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,9 @@\n \tstruct task_struct *p;\n \n \tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n+\t\treturn ERR_PTR(-EINVAL);\n+\n+\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n \t\treturn ERR_PTR(-EINVAL);\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\t\treturn ERR_PTR(-EINVAL);",
                "",
                "\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The clone system-call implementation in the Linux kernel before 3.8.3 does not properly handle a combination of the CLONE_NEWUSER and CLONE_FS flags, which allows local users to gain privileges by calling chroot and leveraging the sharing of the / directory between a parent process and a child process.",
        "id": 198
    },
    {
        "cve_id": "CVE-2012-2319",
        "code_before_change": "int hfsplus_rename_cat(u32 cnid,\n\t\t       struct inode *src_dir, struct qstr *src_name,\n\t\t       struct inode *dst_dir, struct qstr *dst_name)\n{\n\tstruct super_block *sb = src_dir->i_sb;\n\tstruct hfs_find_data src_fd, dst_fd;\n\thfsplus_cat_entry entry;\n\tint entry_size, type;\n\tint err;\n\n\tdprint(DBG_CAT_MOD, \"rename_cat: %u - %lu,%s - %lu,%s\\n\",\n\t\tcnid, src_dir->i_ino, src_name->name,\n\t\tdst_dir->i_ino, dst_name->name);\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &src_fd);\n\tif (err)\n\t\treturn err;\n\tdst_fd = src_fd;\n\n\t/* find the old dir entry and read the data */\n\thfsplus_cat_build_key(sb, src_fd.search_key, src_dir->i_ino, src_name);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\n\thfs_bnode_read(src_fd.bnode, &entry, src_fd.entryoffset,\n\t\t\t\tsrc_fd.entrylength);\n\n\t/* create new dir entry with the data from the old entry */\n\thfsplus_cat_build_key(sb, dst_fd.search_key, dst_dir->i_ino, dst_name);\n\terr = hfs_brec_find(&dst_fd);\n\tif (err != -ENOENT) {\n\t\tif (!err)\n\t\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\n\terr = hfs_brec_insert(&dst_fd, &entry, src_fd.entrylength);\n\tif (err)\n\t\tgoto out;\n\tdst_dir->i_size++;\n\tdst_dir->i_mtime = dst_dir->i_ctime = CURRENT_TIME_SEC;\n\n\t/* finally remove the old entry */\n\thfsplus_cat_build_key(sb, src_fd.search_key, src_dir->i_ino, src_name);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\terr = hfs_brec_remove(&src_fd);\n\tif (err)\n\t\tgoto out;\n\tsrc_dir->i_size--;\n\tsrc_dir->i_mtime = src_dir->i_ctime = CURRENT_TIME_SEC;\n\n\t/* remove old thread entry */\n\thfsplus_cat_build_key(sb, src_fd.search_key, cnid, NULL);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\ttype = hfs_bnode_read_u16(src_fd.bnode, src_fd.entryoffset);\n\terr = hfs_brec_remove(&src_fd);\n\tif (err)\n\t\tgoto out;\n\n\t/* create new thread entry */\n\thfsplus_cat_build_key(sb, dst_fd.search_key, cnid, NULL);\n\tentry_size = hfsplus_fill_cat_thread(sb, &entry, type,\n\t\tdst_dir->i_ino, dst_name);\n\terr = hfs_brec_find(&dst_fd);\n\tif (err != -ENOENT) {\n\t\tif (!err)\n\t\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\terr = hfs_brec_insert(&dst_fd, &entry, entry_size);\n\n\thfsplus_mark_inode_dirty(dst_dir, HFSPLUS_I_CAT_DIRTY);\n\thfsplus_mark_inode_dirty(src_dir, HFSPLUS_I_CAT_DIRTY);\nout:\n\thfs_bnode_put(dst_fd.bnode);\n\thfs_find_exit(&src_fd);\n\treturn err;\n}",
        "code_after_change": "int hfsplus_rename_cat(u32 cnid,\n\t\t       struct inode *src_dir, struct qstr *src_name,\n\t\t       struct inode *dst_dir, struct qstr *dst_name)\n{\n\tstruct super_block *sb = src_dir->i_sb;\n\tstruct hfs_find_data src_fd, dst_fd;\n\thfsplus_cat_entry entry;\n\tint entry_size, type;\n\tint err;\n\n\tdprint(DBG_CAT_MOD, \"rename_cat: %u - %lu,%s - %lu,%s\\n\",\n\t\tcnid, src_dir->i_ino, src_name->name,\n\t\tdst_dir->i_ino, dst_name->name);\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &src_fd);\n\tif (err)\n\t\treturn err;\n\tdst_fd = src_fd;\n\n\t/* find the old dir entry and read the data */\n\thfsplus_cat_build_key(sb, src_fd.search_key, src_dir->i_ino, src_name);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\tif (src_fd.entrylength > sizeof(entry) || src_fd.entrylength < 0) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\thfs_bnode_read(src_fd.bnode, &entry, src_fd.entryoffset,\n\t\t\t\tsrc_fd.entrylength);\n\n\t/* create new dir entry with the data from the old entry */\n\thfsplus_cat_build_key(sb, dst_fd.search_key, dst_dir->i_ino, dst_name);\n\terr = hfs_brec_find(&dst_fd);\n\tif (err != -ENOENT) {\n\t\tif (!err)\n\t\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\n\terr = hfs_brec_insert(&dst_fd, &entry, src_fd.entrylength);\n\tif (err)\n\t\tgoto out;\n\tdst_dir->i_size++;\n\tdst_dir->i_mtime = dst_dir->i_ctime = CURRENT_TIME_SEC;\n\n\t/* finally remove the old entry */\n\thfsplus_cat_build_key(sb, src_fd.search_key, src_dir->i_ino, src_name);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\terr = hfs_brec_remove(&src_fd);\n\tif (err)\n\t\tgoto out;\n\tsrc_dir->i_size--;\n\tsrc_dir->i_mtime = src_dir->i_ctime = CURRENT_TIME_SEC;\n\n\t/* remove old thread entry */\n\thfsplus_cat_build_key(sb, src_fd.search_key, cnid, NULL);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\ttype = hfs_bnode_read_u16(src_fd.bnode, src_fd.entryoffset);\n\terr = hfs_brec_remove(&src_fd);\n\tif (err)\n\t\tgoto out;\n\n\t/* create new thread entry */\n\thfsplus_cat_build_key(sb, dst_fd.search_key, cnid, NULL);\n\tentry_size = hfsplus_fill_cat_thread(sb, &entry, type,\n\t\tdst_dir->i_ino, dst_name);\n\terr = hfs_brec_find(&dst_fd);\n\tif (err != -ENOENT) {\n\t\tif (!err)\n\t\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\terr = hfs_brec_insert(&dst_fd, &entry, entry_size);\n\n\thfsplus_mark_inode_dirty(dst_dir, HFSPLUS_I_CAT_DIRTY);\n\thfsplus_mark_inode_dirty(src_dir, HFSPLUS_I_CAT_DIRTY);\nout:\n\thfs_bnode_put(dst_fd.bnode);\n\thfs_find_exit(&src_fd);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,10 @@\n \terr = hfs_brec_find(&src_fd);\n \tif (err)\n \t\tgoto out;\n+\tif (src_fd.entrylength > sizeof(entry) || src_fd.entrylength < 0) {\n+\t\terr = -EIO;\n+\t\tgoto out;\n+\t}\n \n \thfs_bnode_read(src_fd.bnode, &entry, src_fd.entryoffset,\n \t\t\t\tsrc_fd.entrylength);",
        "function_modified_lines": {
            "added": [
                "\tif (src_fd.entrylength > sizeof(entry) || src_fd.entrylength < 0) {",
                "\t\terr = -EIO;",
                "\t\tgoto out;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "Multiple buffer overflows in the hfsplus filesystem implementation in the Linux kernel before 3.3.5 allow local users to gain privileges via a crafted HFS plus filesystem, a related issue to CVE-2009-4020.",
        "id": 44
    },
    {
        "cve_id": "CVE-2014-9922",
        "code_before_change": "static struct dentry *ecryptfs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *raw_data)\n{\n\tstruct super_block *s;\n\tstruct ecryptfs_sb_info *sbi;\n\tstruct ecryptfs_dentry_info *root_info;\n\tconst char *err = \"Getting sb failed\";\n\tstruct inode *inode;\n\tstruct path path;\n\tuid_t check_ruid;\n\tint rc;\n\n\tsbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);\n\tif (!sbi) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trc = ecryptfs_parse_options(sbi, raw_data, &check_ruid);\n\tif (rc) {\n\t\terr = \"Error parsing options\";\n\t\tgoto out;\n\t}\n\n\ts = sget(fs_type, NULL, set_anon_super, flags, NULL);\n\tif (IS_ERR(s)) {\n\t\trc = PTR_ERR(s);\n\t\tgoto out;\n\t}\n\n\trc = bdi_setup_and_register(&sbi->bdi, \"ecryptfs\", BDI_CAP_MAP_COPY);\n\tif (rc)\n\t\tgoto out1;\n\n\tecryptfs_set_superblock_private(s, sbi);\n\ts->s_bdi = &sbi->bdi;\n\n\t/* ->kill_sb() will take care of sbi after that point */\n\tsbi = NULL;\n\ts->s_op = &ecryptfs_sops;\n\ts->s_d_op = &ecryptfs_dops;\n\n\terr = \"Reading sb failed\";\n\trc = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &path);\n\tif (rc) {\n\t\tecryptfs_printk(KERN_WARNING, \"kern_path() failed\\n\");\n\t\tgoto out1;\n\t}\n\tif (path.dentry->d_sb->s_type == &ecryptfs_fs_type) {\n\t\trc = -EINVAL;\n\t\tprintk(KERN_ERR \"Mount on filesystem of type \"\n\t\t\t\"eCryptfs explicitly disallowed due to \"\n\t\t\t\"known incompatibilities\\n\");\n\t\tgoto out_free;\n\t}\n\n\tif (check_ruid && !uid_eq(path.dentry->d_inode->i_uid, current_uid())) {\n\t\trc = -EPERM;\n\t\tprintk(KERN_ERR \"Mount of device (uid: %d) not owned by \"\n\t\t       \"requested user (uid: %d)\\n\",\n\t\t\ti_uid_read(path.dentry->d_inode),\n\t\t\tfrom_kuid(&init_user_ns, current_uid()));\n\t\tgoto out_free;\n\t}\n\n\tecryptfs_set_superblock_lower(s, path.dentry->d_sb);\n\n\t/**\n\t * Set the POSIX ACL flag based on whether they're enabled in the lower\n\t * mount. Force a read-only eCryptfs mount if the lower mount is ro.\n\t * Allow a ro eCryptfs mount even when the lower mount is rw.\n\t */\n\ts->s_flags = flags & ~MS_POSIXACL;\n\ts->s_flags |= path.dentry->d_sb->s_flags & (MS_RDONLY | MS_POSIXACL);\n\n\ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n\ts->s_blocksize = path.dentry->d_sb->s_blocksize;\n\ts->s_magic = ECRYPTFS_SUPER_MAGIC;\n\n\tinode = ecryptfs_get_inode(path.dentry->d_inode, s);\n\trc = PTR_ERR(inode);\n\tif (IS_ERR(inode))\n\t\tgoto out_free;\n\n\ts->s_root = d_make_root(inode);\n\tif (!s->s_root) {\n\t\trc = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\trc = -ENOMEM;\n\troot_info = kmem_cache_zalloc(ecryptfs_dentry_info_cache, GFP_KERNEL);\n\tif (!root_info)\n\t\tgoto out_free;\n\n\t/* ->kill_sb() will take care of root_info */\n\tecryptfs_set_dentry_private(s->s_root, root_info);\n\troot_info->lower_path = path;\n\n\ts->s_flags |= MS_ACTIVE;\n\treturn dget(s->s_root);\n\nout_free:\n\tpath_put(&path);\nout1:\n\tdeactivate_locked_super(s);\nout:\n\tif (sbi) {\n\t\tecryptfs_destroy_mount_crypt_stat(&sbi->mount_crypt_stat);\n\t\tkmem_cache_free(ecryptfs_sb_info_cache, sbi);\n\t}\n\tprintk(KERN_ERR \"%s; rc = [%d]\\n\", err, rc);\n\treturn ERR_PTR(rc);\n}",
        "code_after_change": "static struct dentry *ecryptfs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *raw_data)\n{\n\tstruct super_block *s;\n\tstruct ecryptfs_sb_info *sbi;\n\tstruct ecryptfs_dentry_info *root_info;\n\tconst char *err = \"Getting sb failed\";\n\tstruct inode *inode;\n\tstruct path path;\n\tuid_t check_ruid;\n\tint rc;\n\n\tsbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);\n\tif (!sbi) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trc = ecryptfs_parse_options(sbi, raw_data, &check_ruid);\n\tif (rc) {\n\t\terr = \"Error parsing options\";\n\t\tgoto out;\n\t}\n\n\ts = sget(fs_type, NULL, set_anon_super, flags, NULL);\n\tif (IS_ERR(s)) {\n\t\trc = PTR_ERR(s);\n\t\tgoto out;\n\t}\n\n\trc = bdi_setup_and_register(&sbi->bdi, \"ecryptfs\", BDI_CAP_MAP_COPY);\n\tif (rc)\n\t\tgoto out1;\n\n\tecryptfs_set_superblock_private(s, sbi);\n\ts->s_bdi = &sbi->bdi;\n\n\t/* ->kill_sb() will take care of sbi after that point */\n\tsbi = NULL;\n\ts->s_op = &ecryptfs_sops;\n\ts->s_d_op = &ecryptfs_dops;\n\n\terr = \"Reading sb failed\";\n\trc = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &path);\n\tif (rc) {\n\t\tecryptfs_printk(KERN_WARNING, \"kern_path() failed\\n\");\n\t\tgoto out1;\n\t}\n\tif (path.dentry->d_sb->s_type == &ecryptfs_fs_type) {\n\t\trc = -EINVAL;\n\t\tprintk(KERN_ERR \"Mount on filesystem of type \"\n\t\t\t\"eCryptfs explicitly disallowed due to \"\n\t\t\t\"known incompatibilities\\n\");\n\t\tgoto out_free;\n\t}\n\n\tif (check_ruid && !uid_eq(path.dentry->d_inode->i_uid, current_uid())) {\n\t\trc = -EPERM;\n\t\tprintk(KERN_ERR \"Mount of device (uid: %d) not owned by \"\n\t\t       \"requested user (uid: %d)\\n\",\n\t\t\ti_uid_read(path.dentry->d_inode),\n\t\t\tfrom_kuid(&init_user_ns, current_uid()));\n\t\tgoto out_free;\n\t}\n\n\tecryptfs_set_superblock_lower(s, path.dentry->d_sb);\n\n\t/**\n\t * Set the POSIX ACL flag based on whether they're enabled in the lower\n\t * mount. Force a read-only eCryptfs mount if the lower mount is ro.\n\t * Allow a ro eCryptfs mount even when the lower mount is rw.\n\t */\n\ts->s_flags = flags & ~MS_POSIXACL;\n\ts->s_flags |= path.dentry->d_sb->s_flags & (MS_RDONLY | MS_POSIXACL);\n\n\ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n\ts->s_blocksize = path.dentry->d_sb->s_blocksize;\n\ts->s_magic = ECRYPTFS_SUPER_MAGIC;\n\ts->s_stack_depth = path.dentry->d_sb->s_stack_depth + 1;\n\n\trc = -EINVAL;\n\tif (s->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n\t\tpr_err(\"eCryptfs: maximum fs stacking depth exceeded\\n\");\n\t\tgoto out_free;\n\t}\n\n\tinode = ecryptfs_get_inode(path.dentry->d_inode, s);\n\trc = PTR_ERR(inode);\n\tif (IS_ERR(inode))\n\t\tgoto out_free;\n\n\ts->s_root = d_make_root(inode);\n\tif (!s->s_root) {\n\t\trc = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\trc = -ENOMEM;\n\troot_info = kmem_cache_zalloc(ecryptfs_dentry_info_cache, GFP_KERNEL);\n\tif (!root_info)\n\t\tgoto out_free;\n\n\t/* ->kill_sb() will take care of root_info */\n\tecryptfs_set_dentry_private(s->s_root, root_info);\n\troot_info->lower_path = path;\n\n\ts->s_flags |= MS_ACTIVE;\n\treturn dget(s->s_root);\n\nout_free:\n\tpath_put(&path);\nout1:\n\tdeactivate_locked_super(s);\nout:\n\tif (sbi) {\n\t\tecryptfs_destroy_mount_crypt_stat(&sbi->mount_crypt_stat);\n\t\tkmem_cache_free(ecryptfs_sb_info_cache, sbi);\n\t}\n\tprintk(KERN_ERR \"%s; rc = [%d]\\n\", err, rc);\n\treturn ERR_PTR(rc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -76,6 +76,13 @@\n \ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n \ts->s_blocksize = path.dentry->d_sb->s_blocksize;\n \ts->s_magic = ECRYPTFS_SUPER_MAGIC;\n+\ts->s_stack_depth = path.dentry->d_sb->s_stack_depth + 1;\n+\n+\trc = -EINVAL;\n+\tif (s->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n+\t\tpr_err(\"eCryptfs: maximum fs stacking depth exceeded\\n\");\n+\t\tgoto out_free;\n+\t}\n \n \tinode = ecryptfs_get_inode(path.dentry->d_inode, s);\n \trc = PTR_ERR(inode);",
        "function_modified_lines": {
            "added": [
                "\ts->s_stack_depth = path.dentry->d_sb->s_stack_depth + 1;",
                "",
                "\trc = -EINVAL;",
                "\tif (s->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {",
                "\t\tpr_err(\"eCryptfs: maximum fs stacking depth exceeded\\n\");",
                "\t\tgoto out_free;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The eCryptfs subsystem in the Linux kernel before 3.18 allows local users to gain privileges via a large filesystem stack that includes an overlayfs layer, related to fs/ecryptfs/main.c and fs/overlayfs/super.c.",
        "id": 712
    }
]