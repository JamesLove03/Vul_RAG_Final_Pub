[
    {
        "cve_id": "CVE-2018-19854",
        "code_before_change": "static int crypto_report_one(struct crypto_alg *alg,\n\t\t\t     struct crypto_user_alg *ualg, struct sk_buff *skb)\n{\n\tstrlcpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));\n\tstrlcpy(ualg->cru_driver_name, alg->cra_driver_name,\n\t\tsizeof(ualg->cru_driver_name));\n\tstrlcpy(ualg->cru_module_name, module_name(alg->cra_module),\n\t\tsizeof(ualg->cru_module_name));\n\n\tualg->cru_type = 0;\n\tualg->cru_mask = 0;\n\tualg->cru_flags = alg->cra_flags;\n\tualg->cru_refcnt = refcount_read(&alg->cra_refcnt);\n\n\tif (nla_put_u32(skb, CRYPTOCFGA_PRIORITY_VAL, alg->cra_priority))\n\t\tgoto nla_put_failure;\n\tif (alg->cra_flags & CRYPTO_ALG_LARVAL) {\n\t\tstruct crypto_report_larval rl;\n\n\t\tstrlcpy(rl.type, \"larval\", sizeof(rl.type));\n\t\tif (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,\n\t\t\t    sizeof(struct crypto_report_larval), &rl))\n\t\t\tgoto nla_put_failure;\n\t\tgoto out;\n\t}\n\n\tif (alg->cra_type && alg->cra_type->report) {\n\t\tif (alg->cra_type->report(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tgoto out;\n\t}\n\n\tswitch (alg->cra_flags & (CRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_LARVAL)) {\n\tcase CRYPTO_ALG_TYPE_CIPHER:\n\t\tif (crypto_report_cipher(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_COMPRESS:\n\t\tif (crypto_report_comp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_ACOMPRESS:\n\t\tif (crypto_report_acomp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_AKCIPHER:\n\t\tif (crypto_report_akcipher(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_KPP:\n\t\tif (crypto_report_kpp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\t}\n\nout:\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "code_after_change": "static int crypto_report_one(struct crypto_alg *alg,\n\t\t\t     struct crypto_user_alg *ualg, struct sk_buff *skb)\n{\n\tstrncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));\n\tstrncpy(ualg->cru_driver_name, alg->cra_driver_name,\n\t\tsizeof(ualg->cru_driver_name));\n\tstrncpy(ualg->cru_module_name, module_name(alg->cra_module),\n\t\tsizeof(ualg->cru_module_name));\n\n\tualg->cru_type = 0;\n\tualg->cru_mask = 0;\n\tualg->cru_flags = alg->cra_flags;\n\tualg->cru_refcnt = refcount_read(&alg->cra_refcnt);\n\n\tif (nla_put_u32(skb, CRYPTOCFGA_PRIORITY_VAL, alg->cra_priority))\n\t\tgoto nla_put_failure;\n\tif (alg->cra_flags & CRYPTO_ALG_LARVAL) {\n\t\tstruct crypto_report_larval rl;\n\n\t\tstrncpy(rl.type, \"larval\", sizeof(rl.type));\n\t\tif (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,\n\t\t\t    sizeof(struct crypto_report_larval), &rl))\n\t\t\tgoto nla_put_failure;\n\t\tgoto out;\n\t}\n\n\tif (alg->cra_type && alg->cra_type->report) {\n\t\tif (alg->cra_type->report(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tgoto out;\n\t}\n\n\tswitch (alg->cra_flags & (CRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_LARVAL)) {\n\tcase CRYPTO_ALG_TYPE_CIPHER:\n\t\tif (crypto_report_cipher(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_COMPRESS:\n\t\tif (crypto_report_comp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_ACOMPRESS:\n\t\tif (crypto_report_acomp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_AKCIPHER:\n\t\tif (crypto_report_akcipher(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_KPP:\n\t\tif (crypto_report_kpp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\t}\n\nout:\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,10 @@\n static int crypto_report_one(struct crypto_alg *alg,\n \t\t\t     struct crypto_user_alg *ualg, struct sk_buff *skb)\n {\n-\tstrlcpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));\n-\tstrlcpy(ualg->cru_driver_name, alg->cra_driver_name,\n+\tstrncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));\n+\tstrncpy(ualg->cru_driver_name, alg->cra_driver_name,\n \t\tsizeof(ualg->cru_driver_name));\n-\tstrlcpy(ualg->cru_module_name, module_name(alg->cra_module),\n+\tstrncpy(ualg->cru_module_name, module_name(alg->cra_module),\n \t\tsizeof(ualg->cru_module_name));\n \n \tualg->cru_type = 0;\n@@ -17,7 +17,7 @@\n \tif (alg->cra_flags & CRYPTO_ALG_LARVAL) {\n \t\tstruct crypto_report_larval rl;\n \n-\t\tstrlcpy(rl.type, \"larval\", sizeof(rl.type));\n+\t\tstrncpy(rl.type, \"larval\", sizeof(rl.type));\n \t\tif (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,\n \t\t\t    sizeof(struct crypto_report_larval), &rl))\n \t\t\tgoto nla_put_failure;",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));",
                "\tstrncpy(ualg->cru_driver_name, alg->cra_driver_name,",
                "\tstrncpy(ualg->cru_module_name, module_name(alg->cra_module),",
                "\t\tstrncpy(rl.type, \"larval\", sizeof(rl.type));"
            ],
            "deleted": [
                "\tstrlcpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));",
                "\tstrlcpy(ualg->cru_driver_name, alg->cra_driver_name,",
                "\tstrlcpy(ualg->cru_module_name, module_name(alg->cra_module),",
                "\t\tstrlcpy(rl.type, \"larval\", sizeof(rl.type));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.19.3. crypto_report_one() and related functions in crypto/crypto_user.c (the crypto user configuration API) do not fully initialize structures that are copied to userspace, potentially leaking sensitive memory to user programs. NOTE: this is a CVE-2013-2547 regression but with easier exploitability because the attacker does not need a capability (however, the system must have the CONFIG_CRYPTO_USER kconfig option).",
        "id": 1748
    },
    {
        "cve_id": "CVE-2018-19854",
        "code_before_change": "static int crypto_report_cipher(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_cipher rcipher;\n\n\tstrlcpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n\n\trcipher.blocksize = alg->cra_blocksize;\n\trcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\trcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_CIPHER,\n\t\t    sizeof(struct crypto_report_cipher), &rcipher))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "code_after_change": "static int crypto_report_cipher(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_cipher rcipher;\n\n\tstrncpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n\n\trcipher.blocksize = alg->cra_blocksize;\n\trcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\trcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_CIPHER,\n\t\t    sizeof(struct crypto_report_cipher), &rcipher))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tstruct crypto_report_cipher rcipher;\n \n-\tstrlcpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n+\tstrncpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n \n \trcipher.blocksize = alg->cra_blocksize;\n \trcipher.min_keysize = alg->cra_cipher.cia_min_keysize;",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(rcipher.type, \"cipher\", sizeof(rcipher.type));"
            ],
            "deleted": [
                "\tstrlcpy(rcipher.type, \"cipher\", sizeof(rcipher.type));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.19.3. crypto_report_one() and related functions in crypto/crypto_user.c (the crypto user configuration API) do not fully initialize structures that are copied to userspace, potentially leaking sensitive memory to user programs. NOTE: this is a CVE-2013-2547 regression but with easier exploitability because the attacker does not need a capability (however, the system must have the CONFIG_CRYPTO_USER kconfig option).",
        "id": 1749
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static inline void save_init_fpu(struct task_struct *tsk)\n{\n\tWARN_ON_ONCE(!__thread_has_fpu(tsk));\n\n\tif (use_xsave()) {\n\t\txsave_state(&tsk->thread.fpu.state->xsave, -1);\n\t\treturn;\n\t}\n\n\tpreempt_disable();\n\t__save_init_fpu(tsk);\n\t__thread_fpu_end(tsk);\n\tpreempt_enable();\n}",
        "code_after_change": "static inline void save_init_fpu(struct task_struct *tsk)\n{\n\tWARN_ON_ONCE(!__thread_has_fpu(tsk));\n\n\tif (use_eager_fpu()) {\n\t\t__save_fpu(tsk);\n\t\treturn;\n\t}\n\n\tpreempt_disable();\n\t__save_init_fpu(tsk);\n\t__thread_fpu_end(tsk);\n\tpreempt_enable();\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,8 +2,8 @@\n {\n \tWARN_ON_ONCE(!__thread_has_fpu(tsk));\n \n-\tif (use_xsave()) {\n-\t\txsave_state(&tsk->thread.fpu.state->xsave, -1);\n+\tif (use_eager_fpu()) {\n+\t\t__save_fpu(tsk);\n \t\treturn;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tif (use_eager_fpu()) {",
                "\t\t__save_fpu(tsk);"
            ],
            "deleted": [
                "\tif (use_xsave()) {",
                "\t\txsave_state(&tsk->thread.fpu.state->xsave, -1);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1802
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static void __cpuinit mxcsr_feature_mask_init(void)\n{\n\tunsigned long mask = 0;\n\n\tclts();\n\tif (cpu_has_fxsr) {\n\t\tmemset(&fx_scratch, 0, sizeof(struct i387_fxsave_struct));\n\t\tasm volatile(\"fxsave %0\" : : \"m\" (fx_scratch));\n\t\tmask = fx_scratch.mxcsr_mask;\n\t\tif (mask == 0)\n\t\t\tmask = 0x0000ffbf;\n\t}\n\tmxcsr_feature_mask &= mask;\n\tstts();\n}",
        "code_after_change": "static void __cpuinit mxcsr_feature_mask_init(void)\n{\n\tunsigned long mask = 0;\n\n\tif (cpu_has_fxsr) {\n\t\tmemset(&fx_scratch, 0, sizeof(struct i387_fxsave_struct));\n\t\tasm volatile(\"fxsave %0\" : : \"m\" (fx_scratch));\n\t\tmask = fx_scratch.mxcsr_mask;\n\t\tif (mask == 0)\n\t\t\tmask = 0x0000ffbf;\n\t}\n\tmxcsr_feature_mask &= mask;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,6 @@\n {\n \tunsigned long mask = 0;\n \n-\tclts();\n \tif (cpu_has_fxsr) {\n \t\tmemset(&fx_scratch, 0, sizeof(struct i387_fxsave_struct));\n \t\tasm volatile(\"fxsave %0\" : : \"m\" (fx_scratch));\n@@ -11,5 +10,4 @@\n \t\t\tmask = 0x0000ffbf;\n \t}\n \tmxcsr_feature_mask &= mask;\n-\tstts();\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tclts();",
                "\tstts();"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1806
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "void __cpuinit xsave_init(void)\n{\n\tstatic __refdata void (*next_func)(void) = xstate_enable_boot_cpu;\n\tvoid (*this_func)(void);\n\n\tif (!cpu_has_xsave)\n\t\treturn;\n\n\tthis_func = next_func;\n\tnext_func = xstate_enable_ap;\n\tthis_func();\n}",
        "code_after_change": "void __cpuinit xsave_init(void)\n{\n\tstatic __refdata void (*next_func)(void) = xstate_enable_boot_cpu;\n\tvoid (*this_func)(void);\n\n\tif (!cpu_has_xsave)\n\t\treturn;\n\n\tthis_func = next_func;\n\tnext_func = xstate_enable;\n\tthis_func();\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,6 @@\n \t\treturn;\n \n \tthis_func = next_func;\n-\tnext_func = xstate_enable_ap;\n+\tnext_func = xstate_enable;\n \tthis_func();\n }",
        "function_modified_lines": {
            "added": [
                "\tnext_func = xstate_enable;"
            ],
            "deleted": [
                "\tnext_func = xstate_enable_ap;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1817
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "void kernel_fpu_begin(void)\n{\n\tstruct task_struct *me = current;\n\n\tWARN_ON_ONCE(!irq_fpu_usable());\n\tpreempt_disable();\n\tif (__thread_has_fpu(me)) {\n\t\t__save_init_fpu(me);\n\t\t__thread_clear_has_fpu(me);\n\t\t/* We do 'stts()' in kernel_fpu_end() */\n\t} else if (!use_xsave()) {\n\t\tthis_cpu_write(fpu_owner_task, NULL);\n\t\tclts();\n\t}\n}",
        "code_after_change": "void kernel_fpu_begin(void)\n{\n\tstruct task_struct *me = current;\n\n\tWARN_ON_ONCE(!irq_fpu_usable());\n\tpreempt_disable();\n\tif (__thread_has_fpu(me)) {\n\t\t__save_init_fpu(me);\n\t\t__thread_clear_has_fpu(me);\n\t\t/* We do 'stts()' in kernel_fpu_end() */\n\t} else if (!use_eager_fpu()) {\n\t\tthis_cpu_write(fpu_owner_task, NULL);\n\t\tclts();\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \t\t__save_init_fpu(me);\n \t\t__thread_clear_has_fpu(me);\n \t\t/* We do 'stts()' in kernel_fpu_end() */\n-\t} else if (!use_xsave()) {\n+\t} else if (!use_eager_fpu()) {\n \t\tthis_cpu_write(fpu_owner_task, NULL);\n \t\tclts();\n \t}",
        "function_modified_lines": {
            "added": [
                "\t} else if (!use_eager_fpu()) {"
            ],
            "deleted": [
                "\t} else if (!use_xsave()) {"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1811
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static int blkif_queue_rw_req(struct request *req, struct blkfront_ring_info *rinfo)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blkif_request *ring_req, *extra_ring_req = NULL;\n\tstruct blkif_request *final_ring_req, *final_extra_ring_req = NULL;\n\tunsigned long id, extra_id = NO_ASSOCIATED_ID;\n\tbool require_extra_req = false;\n\tint i;\n\tstruct setup_rw_req setup = {\n\t\t.grant_idx = 0,\n\t\t.segments = NULL,\n\t\t.rinfo = rinfo,\n\t\t.need_copy = rq_data_dir(req) && info->feature_persistent,\n\t};\n\n\t/*\n\t * Used to store if we are able to queue the request by just using\n\t * existing persistent grants, or if we have to get new grants,\n\t * as there are not sufficiently many free.\n\t */\n\tbool new_persistent_gnts = false;\n\tstruct scatterlist *sg;\n\tint num_sg, max_grefs, num_grant;\n\n\tmax_grefs = req->nr_phys_segments * GRANTS_PER_PSEG;\n\tif (max_grefs > BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\t/*\n\t\t * If we are using indirect segments we need to account\n\t\t * for the indirect grefs used in the request.\n\t\t */\n\t\tmax_grefs += INDIRECT_GREFS(max_grefs);\n\n\t/* Check if we have enough persistent grants to allocate a requests */\n\tif (rinfo->persistent_gnts_c < max_grefs) {\n\t\tnew_persistent_gnts = true;\n\n\t\tif (gnttab_alloc_grant_references(\n\t\t    max_grefs - rinfo->persistent_gnts_c,\n\t\t    &setup.gref_head) < 0) {\n\t\t\tgnttab_request_free_callback(\n\t\t\t\t&rinfo->callback,\n\t\t\t\tblkif_restart_queue_callback,\n\t\t\t\trinfo,\n\t\t\t\tmax_grefs - rinfo->persistent_gnts_c);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* Fill out a communications ring structure. */\n\tid = blkif_ring_get_request(rinfo, req, &final_ring_req);\n\tring_req = &rinfo->shadow[id].req;\n\n\tnum_sg = blk_rq_map_sg(req->q, req, rinfo->shadow[id].sg);\n\tnum_grant = 0;\n\t/* Calculate the number of grant used */\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i)\n\t       num_grant += gnttab_count_grant(sg->offset, sg->length);\n\n\trequire_extra_req = info->max_indirect_segments == 0 &&\n\t\tnum_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\tBUG_ON(!HAS_EXTRA_REQ && require_extra_req);\n\n\trinfo->shadow[id].num_sg = num_sg;\n\tif (num_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST &&\n\t    likely(!require_extra_req)) {\n\t\t/*\n\t\t * The indirect operation can only be a BLKIF_OP_READ or\n\t\t * BLKIF_OP_WRITE\n\t\t */\n\t\tBUG_ON(req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA);\n\t\tring_req->operation = BLKIF_OP_INDIRECT;\n\t\tring_req->u.indirect.indirect_op = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tring_req->u.indirect.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.indirect.handle = info->handle;\n\t\tring_req->u.indirect.nr_segments = num_grant;\n\t} else {\n\t\tring_req->u.rw.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.rw.handle = info->handle;\n\t\tring_req->operation = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tif (req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA) {\n\t\t\t/*\n\t\t\t * Ideally we can do an unordered flush-to-disk.\n\t\t\t * In case the backend onlysupports barriers, use that.\n\t\t\t * A barrier request a superset of FUA, so we can\n\t\t\t * implement it the same way.  (It's also a FLUSH+FUA,\n\t\t\t * since it is guaranteed ordered WRT previous writes.)\n\t\t\t */\n\t\t\tif (info->feature_flush && info->feature_fua)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_WRITE_BARRIER;\n\t\t\telse if (info->feature_flush)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_FLUSH_DISKCACHE;\n\t\t\telse\n\t\t\t\tring_req->operation = 0;\n\t\t}\n\t\tring_req->u.rw.nr_segments = num_grant;\n\t\tif (unlikely(require_extra_req)) {\n\t\t\textra_id = blkif_ring_get_request(rinfo, req,\n\t\t\t\t\t\t\t  &final_extra_ring_req);\n\t\t\textra_ring_req = &rinfo->shadow[extra_id].req;\n\n\t\t\t/*\n\t\t\t * Only the first request contains the scatter-gather\n\t\t\t * list.\n\t\t\t */\n\t\t\trinfo->shadow[extra_id].num_sg = 0;\n\n\t\t\tblkif_setup_extra_req(ring_req, extra_ring_req);\n\n\t\t\t/* Link the 2 requests together */\n\t\t\trinfo->shadow[extra_id].associated_id = id;\n\t\t\trinfo->shadow[id].associated_id = extra_id;\n\t\t}\n\t}\n\n\tsetup.ring_req = ring_req;\n\tsetup.id = id;\n\n\tsetup.require_extra_req = require_extra_req;\n\tif (unlikely(require_extra_req))\n\t\tsetup.extra_ring_req = extra_ring_req;\n\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i) {\n\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\n\t\tif (setup.need_copy) {\n\t\t\tsetup.bvec_off = sg->offset;\n\t\t\tsetup.bvec_data = kmap_atomic(sg_page(sg));\n\t\t}\n\n\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t      sg->offset,\n\t\t\t\t\t      sg->length,\n\t\t\t\t\t      blkif_setup_rw_req_grant,\n\t\t\t\t\t      &setup);\n\n\t\tif (setup.need_copy)\n\t\t\tkunmap_atomic(setup.bvec_data);\n\t}\n\tif (setup.segments)\n\t\tkunmap_atomic(setup.segments);\n\n\t/* Copy request(s) to the ring page. */\n\t*final_ring_req = *ring_req;\n\trinfo->shadow[id].status = REQ_WAITING;\n\tif (unlikely(require_extra_req)) {\n\t\t*final_extra_ring_req = *extra_ring_req;\n\t\trinfo->shadow[extra_id].status = REQ_WAITING;\n\t}\n\n\tif (new_persistent_gnts)\n\t\tgnttab_free_grant_references(setup.gref_head);\n\n\treturn 0;\n}",
        "code_after_change": "static int blkif_queue_rw_req(struct request *req, struct blkfront_ring_info *rinfo)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blkif_request *ring_req, *extra_ring_req = NULL;\n\tstruct blkif_request *final_ring_req, *final_extra_ring_req = NULL;\n\tunsigned long id, extra_id = NO_ASSOCIATED_ID;\n\tbool require_extra_req = false;\n\tint i;\n\tstruct setup_rw_req setup = {\n\t\t.grant_idx = 0,\n\t\t.segments = NULL,\n\t\t.rinfo = rinfo,\n\t\t.need_copy = rq_data_dir(req) && info->bounce,\n\t};\n\n\t/*\n\t * Used to store if we are able to queue the request by just using\n\t * existing persistent grants, or if we have to get new grants,\n\t * as there are not sufficiently many free.\n\t */\n\tbool new_persistent_gnts = false;\n\tstruct scatterlist *sg;\n\tint num_sg, max_grefs, num_grant;\n\n\tmax_grefs = req->nr_phys_segments * GRANTS_PER_PSEG;\n\tif (max_grefs > BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\t/*\n\t\t * If we are using indirect segments we need to account\n\t\t * for the indirect grefs used in the request.\n\t\t */\n\t\tmax_grefs += INDIRECT_GREFS(max_grefs);\n\n\t/* Check if we have enough persistent grants to allocate a requests */\n\tif (rinfo->persistent_gnts_c < max_grefs) {\n\t\tnew_persistent_gnts = true;\n\n\t\tif (gnttab_alloc_grant_references(\n\t\t    max_grefs - rinfo->persistent_gnts_c,\n\t\t    &setup.gref_head) < 0) {\n\t\t\tgnttab_request_free_callback(\n\t\t\t\t&rinfo->callback,\n\t\t\t\tblkif_restart_queue_callback,\n\t\t\t\trinfo,\n\t\t\t\tmax_grefs - rinfo->persistent_gnts_c);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* Fill out a communications ring structure. */\n\tid = blkif_ring_get_request(rinfo, req, &final_ring_req);\n\tring_req = &rinfo->shadow[id].req;\n\n\tnum_sg = blk_rq_map_sg(req->q, req, rinfo->shadow[id].sg);\n\tnum_grant = 0;\n\t/* Calculate the number of grant used */\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i)\n\t       num_grant += gnttab_count_grant(sg->offset, sg->length);\n\n\trequire_extra_req = info->max_indirect_segments == 0 &&\n\t\tnum_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\tBUG_ON(!HAS_EXTRA_REQ && require_extra_req);\n\n\trinfo->shadow[id].num_sg = num_sg;\n\tif (num_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST &&\n\t    likely(!require_extra_req)) {\n\t\t/*\n\t\t * The indirect operation can only be a BLKIF_OP_READ or\n\t\t * BLKIF_OP_WRITE\n\t\t */\n\t\tBUG_ON(req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA);\n\t\tring_req->operation = BLKIF_OP_INDIRECT;\n\t\tring_req->u.indirect.indirect_op = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tring_req->u.indirect.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.indirect.handle = info->handle;\n\t\tring_req->u.indirect.nr_segments = num_grant;\n\t} else {\n\t\tring_req->u.rw.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.rw.handle = info->handle;\n\t\tring_req->operation = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tif (req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA) {\n\t\t\t/*\n\t\t\t * Ideally we can do an unordered flush-to-disk.\n\t\t\t * In case the backend onlysupports barriers, use that.\n\t\t\t * A barrier request a superset of FUA, so we can\n\t\t\t * implement it the same way.  (It's also a FLUSH+FUA,\n\t\t\t * since it is guaranteed ordered WRT previous writes.)\n\t\t\t */\n\t\t\tif (info->feature_flush && info->feature_fua)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_WRITE_BARRIER;\n\t\t\telse if (info->feature_flush)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_FLUSH_DISKCACHE;\n\t\t\telse\n\t\t\t\tring_req->operation = 0;\n\t\t}\n\t\tring_req->u.rw.nr_segments = num_grant;\n\t\tif (unlikely(require_extra_req)) {\n\t\t\textra_id = blkif_ring_get_request(rinfo, req,\n\t\t\t\t\t\t\t  &final_extra_ring_req);\n\t\t\textra_ring_req = &rinfo->shadow[extra_id].req;\n\n\t\t\t/*\n\t\t\t * Only the first request contains the scatter-gather\n\t\t\t * list.\n\t\t\t */\n\t\t\trinfo->shadow[extra_id].num_sg = 0;\n\n\t\t\tblkif_setup_extra_req(ring_req, extra_ring_req);\n\n\t\t\t/* Link the 2 requests together */\n\t\t\trinfo->shadow[extra_id].associated_id = id;\n\t\t\trinfo->shadow[id].associated_id = extra_id;\n\t\t}\n\t}\n\n\tsetup.ring_req = ring_req;\n\tsetup.id = id;\n\n\tsetup.require_extra_req = require_extra_req;\n\tif (unlikely(require_extra_req))\n\t\tsetup.extra_ring_req = extra_ring_req;\n\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i) {\n\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\n\t\tif (setup.need_copy) {\n\t\t\tsetup.bvec_off = sg->offset;\n\t\t\tsetup.bvec_data = kmap_atomic(sg_page(sg));\n\t\t}\n\n\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t      sg->offset,\n\t\t\t\t\t      sg->length,\n\t\t\t\t\t      blkif_setup_rw_req_grant,\n\t\t\t\t\t      &setup);\n\n\t\tif (setup.need_copy)\n\t\t\tkunmap_atomic(setup.bvec_data);\n\t}\n\tif (setup.segments)\n\t\tkunmap_atomic(setup.segments);\n\n\t/* Copy request(s) to the ring page. */\n\t*final_ring_req = *ring_req;\n\trinfo->shadow[id].status = REQ_WAITING;\n\tif (unlikely(require_extra_req)) {\n\t\t*final_extra_ring_req = *extra_ring_req;\n\t\trinfo->shadow[extra_id].status = REQ_WAITING;\n\t}\n\n\tif (new_persistent_gnts)\n\t\tgnttab_free_grant_references(setup.gref_head);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,7 @@\n \t\t.grant_idx = 0,\n \t\t.segments = NULL,\n \t\t.rinfo = rinfo,\n-\t\t.need_copy = rq_data_dir(req) && info->feature_persistent,\n+\t\t.need_copy = rq_data_dir(req) && info->bounce,\n \t};\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\t\t.need_copy = rq_data_dir(req) && info->bounce,"
            ],
            "deleted": [
                "\t\t.need_copy = rq_data_dir(req) && info->feature_persistent,"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3583
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static int blkif_completion(unsigned long *id,\n\t\t\t    struct blkfront_ring_info *rinfo,\n\t\t\t    struct blkif_response *bret)\n{\n\tint i = 0;\n\tstruct scatterlist *sg;\n\tint num_sg, num_grant;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blk_shadow *s = &rinfo->shadow[*id];\n\tstruct copy_from_grant data = {\n\t\t.grant_idx = 0,\n\t};\n\n\tnum_grant = s->req.operation == BLKIF_OP_INDIRECT ?\n\t\ts->req.u.indirect.nr_segments : s->req.u.rw.nr_segments;\n\n\t/* The I/O request may be split in two. */\n\tif (unlikely(s->associated_id != NO_ASSOCIATED_ID)) {\n\t\tstruct blk_shadow *s2 = &rinfo->shadow[s->associated_id];\n\n\t\t/* Keep the status of the current response in shadow. */\n\t\ts->status = blkif_rsp_to_req_status(bret->status);\n\n\t\t/* Wait the second response if not yet here. */\n\t\tif (s2->status < REQ_DONE)\n\t\t\treturn 0;\n\n\t\tbret->status = blkif_get_final_status(s->status,\n\t\t\t\t\t\t      s2->status);\n\n\t\t/*\n\t\t * All the grants is stored in the first shadow in order\n\t\t * to make the completion code simpler.\n\t\t */\n\t\tnum_grant += s2->req.u.rw.nr_segments;\n\n\t\t/*\n\t\t * The two responses may not come in order. Only the\n\t\t * first request will store the scatter-gather list.\n\t\t */\n\t\tif (s2->num_sg != 0) {\n\t\t\t/* Update \"id\" with the ID of the first response. */\n\t\t\t*id = s->associated_id;\n\t\t\ts = s2;\n\t\t}\n\n\t\t/*\n\t\t * We don't need anymore the second request, so recycling\n\t\t * it now.\n\t\t */\n\t\tif (add_id_to_freelist(rinfo, s->associated_id))\n\t\t\tWARN(1, \"%s: can't recycle the second part (id = %ld) of the request\\n\",\n\t\t\t     info->gd->disk_name, s->associated_id);\n\t}\n\n\tdata.s = s;\n\tnum_sg = s->num_sg;\n\n\tif (bret->operation == BLKIF_OP_READ && info->feature_persistent) {\n\t\tfor_each_sg(s->sg, sg, num_sg, i) {\n\t\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\n\t\t\tdata.bvec_offset = sg->offset;\n\t\t\tdata.bvec_data = kmap_atomic(sg_page(sg));\n\n\t\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t\t      sg->offset,\n\t\t\t\t\t\t      sg->length,\n\t\t\t\t\t\t      blkif_copy_from_grant,\n\t\t\t\t\t\t      &data);\n\n\t\t\tkunmap_atomic(data.bvec_data);\n\t\t}\n\t}\n\t/* Add the persistent grant into the list of free grants */\n\tfor (i = 0; i < num_grant; i++) {\n\t\tif (!gnttab_try_end_foreign_access(s->grants_used[i]->gref)) {\n\t\t\t/*\n\t\t\t * If the grant is still mapped by the backend (the\n\t\t\t * backend has chosen to make this grant persistent)\n\t\t\t * we add it at the head of the list, so it will be\n\t\t\t * reused first.\n\t\t\t */\n\t\t\tif (!info->feature_persistent) {\n\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t s->grants_used[i]->gref);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tlist_add(&s->grants_used[i]->node, &rinfo->grants);\n\t\t\trinfo->persistent_gnts_c++;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If the grant is not mapped by the backend we add it\n\t\t\t * to the tail of the list, so it will not be picked\n\t\t\t * again unless we run out of persistent grants.\n\t\t\t */\n\t\t\ts->grants_used[i]->gref = INVALID_GRANT_REF;\n\t\t\tlist_add_tail(&s->grants_used[i]->node, &rinfo->grants);\n\t\t}\n\t}\n\tif (s->req.operation == BLKIF_OP_INDIRECT) {\n\t\tfor (i = 0; i < INDIRECT_GREFS(num_grant); i++) {\n\t\t\tif (!gnttab_try_end_foreign_access(s->indirect_grants[i]->gref)) {\n\t\t\t\tif (!info->feature_persistent) {\n\t\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t\t s->indirect_grants[i]->gref);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tlist_add(&s->indirect_grants[i]->node, &rinfo->grants);\n\t\t\t\trinfo->persistent_gnts_c++;\n\t\t\t} else {\n\t\t\t\tstruct page *indirect_page;\n\n\t\t\t\t/*\n\t\t\t\t * Add the used indirect page back to the list of\n\t\t\t\t * available pages for indirect grefs.\n\t\t\t\t */\n\t\t\t\tif (!info->feature_persistent) {\n\t\t\t\t\tindirect_page = s->indirect_grants[i]->page;\n\t\t\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n\t\t\t\t}\n\t\t\t\ts->indirect_grants[i]->gref = INVALID_GRANT_REF;\n\t\t\t\tlist_add_tail(&s->indirect_grants[i]->node, &rinfo->grants);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 1;\n}",
        "code_after_change": "static int blkif_completion(unsigned long *id,\n\t\t\t    struct blkfront_ring_info *rinfo,\n\t\t\t    struct blkif_response *bret)\n{\n\tint i = 0;\n\tstruct scatterlist *sg;\n\tint num_sg, num_grant;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blk_shadow *s = &rinfo->shadow[*id];\n\tstruct copy_from_grant data = {\n\t\t.grant_idx = 0,\n\t};\n\n\tnum_grant = s->req.operation == BLKIF_OP_INDIRECT ?\n\t\ts->req.u.indirect.nr_segments : s->req.u.rw.nr_segments;\n\n\t/* The I/O request may be split in two. */\n\tif (unlikely(s->associated_id != NO_ASSOCIATED_ID)) {\n\t\tstruct blk_shadow *s2 = &rinfo->shadow[s->associated_id];\n\n\t\t/* Keep the status of the current response in shadow. */\n\t\ts->status = blkif_rsp_to_req_status(bret->status);\n\n\t\t/* Wait the second response if not yet here. */\n\t\tif (s2->status < REQ_DONE)\n\t\t\treturn 0;\n\n\t\tbret->status = blkif_get_final_status(s->status,\n\t\t\t\t\t\t      s2->status);\n\n\t\t/*\n\t\t * All the grants is stored in the first shadow in order\n\t\t * to make the completion code simpler.\n\t\t */\n\t\tnum_grant += s2->req.u.rw.nr_segments;\n\n\t\t/*\n\t\t * The two responses may not come in order. Only the\n\t\t * first request will store the scatter-gather list.\n\t\t */\n\t\tif (s2->num_sg != 0) {\n\t\t\t/* Update \"id\" with the ID of the first response. */\n\t\t\t*id = s->associated_id;\n\t\t\ts = s2;\n\t\t}\n\n\t\t/*\n\t\t * We don't need anymore the second request, so recycling\n\t\t * it now.\n\t\t */\n\t\tif (add_id_to_freelist(rinfo, s->associated_id))\n\t\t\tWARN(1, \"%s: can't recycle the second part (id = %ld) of the request\\n\",\n\t\t\t     info->gd->disk_name, s->associated_id);\n\t}\n\n\tdata.s = s;\n\tnum_sg = s->num_sg;\n\n\tif (bret->operation == BLKIF_OP_READ && info->bounce) {\n\t\tfor_each_sg(s->sg, sg, num_sg, i) {\n\t\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\n\t\t\tdata.bvec_offset = sg->offset;\n\t\t\tdata.bvec_data = kmap_atomic(sg_page(sg));\n\n\t\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t\t      sg->offset,\n\t\t\t\t\t\t      sg->length,\n\t\t\t\t\t\t      blkif_copy_from_grant,\n\t\t\t\t\t\t      &data);\n\n\t\t\tkunmap_atomic(data.bvec_data);\n\t\t}\n\t}\n\t/* Add the persistent grant into the list of free grants */\n\tfor (i = 0; i < num_grant; i++) {\n\t\tif (!gnttab_try_end_foreign_access(s->grants_used[i]->gref)) {\n\t\t\t/*\n\t\t\t * If the grant is still mapped by the backend (the\n\t\t\t * backend has chosen to make this grant persistent)\n\t\t\t * we add it at the head of the list, so it will be\n\t\t\t * reused first.\n\t\t\t */\n\t\t\tif (!info->feature_persistent) {\n\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t s->grants_used[i]->gref);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tlist_add(&s->grants_used[i]->node, &rinfo->grants);\n\t\t\trinfo->persistent_gnts_c++;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If the grant is not mapped by the backend we add it\n\t\t\t * to the tail of the list, so it will not be picked\n\t\t\t * again unless we run out of persistent grants.\n\t\t\t */\n\t\t\ts->grants_used[i]->gref = INVALID_GRANT_REF;\n\t\t\tlist_add_tail(&s->grants_used[i]->node, &rinfo->grants);\n\t\t}\n\t}\n\tif (s->req.operation == BLKIF_OP_INDIRECT) {\n\t\tfor (i = 0; i < INDIRECT_GREFS(num_grant); i++) {\n\t\t\tif (!gnttab_try_end_foreign_access(s->indirect_grants[i]->gref)) {\n\t\t\t\tif (!info->feature_persistent) {\n\t\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t\t s->indirect_grants[i]->gref);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tlist_add(&s->indirect_grants[i]->node, &rinfo->grants);\n\t\t\t\trinfo->persistent_gnts_c++;\n\t\t\t} else {\n\t\t\t\tstruct page *indirect_page;\n\n\t\t\t\t/*\n\t\t\t\t * Add the used indirect page back to the list of\n\t\t\t\t * available pages for indirect grefs.\n\t\t\t\t */\n\t\t\t\tif (!info->bounce) {\n\t\t\t\t\tindirect_page = s->indirect_grants[i]->page;\n\t\t\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n\t\t\t\t}\n\t\t\t\ts->indirect_grants[i]->gref = INVALID_GRANT_REF;\n\t\t\t\tlist_add_tail(&s->indirect_grants[i]->node, &rinfo->grants);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -56,7 +56,7 @@\n \tdata.s = s;\n \tnum_sg = s->num_sg;\n \n-\tif (bret->operation == BLKIF_OP_READ && info->feature_persistent) {\n+\tif (bret->operation == BLKIF_OP_READ && info->bounce) {\n \t\tfor_each_sg(s->sg, sg, num_sg, i) {\n \t\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n \n@@ -115,7 +115,7 @@\n \t\t\t\t * Add the used indirect page back to the list of\n \t\t\t\t * available pages for indirect grefs.\n \t\t\t\t */\n-\t\t\t\tif (!info->feature_persistent) {\n+\t\t\t\tif (!info->bounce) {\n \t\t\t\t\tindirect_page = s->indirect_grants[i]->page;\n \t\t\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n \t\t\t\t}",
        "function_modified_lines": {
            "added": [
                "\tif (bret->operation == BLKIF_OP_READ && info->bounce) {",
                "\t\t\t\tif (!info->bounce) {"
            ],
            "deleted": [
                "\tif (bret->operation == BLKIF_OP_READ && info->feature_persistent) {",
                "\t\t\t\tif (!info->feature_persistent) {"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3587
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static int blkfront_setup_indirect(struct blkfront_ring_info *rinfo)\n{\n\tunsigned int psegs, grants, memflags;\n\tint err, i;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tmemflags = memalloc_noio_save();\n\n\tif (info->max_indirect_segments == 0) {\n\t\tif (!HAS_EXTRA_REQ)\n\t\t\tgrants = BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\t\telse {\n\t\t\t/*\n\t\t\t * When an extra req is required, the maximum\n\t\t\t * grants supported is related to the size of the\n\t\t\t * Linux block segment.\n\t\t\t */\n\t\t\tgrants = GRANTS_PER_PSEG;\n\t\t}\n\t}\n\telse\n\t\tgrants = info->max_indirect_segments;\n\tpsegs = DIV_ROUND_UP(grants, GRANTS_PER_PSEG);\n\n\terr = fill_grant_buffer(rinfo,\n\t\t\t\t(grants + INDIRECT_GREFS(grants)) * BLK_RING_SIZE(info));\n\tif (err)\n\t\tgoto out_of_memory;\n\n\tif (!info->feature_persistent && info->max_indirect_segments) {\n\t\t/*\n\t\t * We are using indirect descriptors but not persistent\n\t\t * grants, we need to allocate a set of pages that can be\n\t\t * used for mapping indirect grefs\n\t\t */\n\t\tint num = INDIRECT_GREFS(grants) * BLK_RING_SIZE(info);\n\n\t\tBUG_ON(!list_empty(&rinfo->indirect_pages));\n\t\tfor (i = 0; i < num; i++) {\n\t\t\tstruct page *indirect_page = alloc_page(GFP_KERNEL |\n\t\t\t\t\t\t\t\t__GFP_ZERO);\n\t\t\tif (!indirect_page)\n\t\t\t\tgoto out_of_memory;\n\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n\t\t}\n\t}\n\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\trinfo->shadow[i].grants_used =\n\t\t\tkvcalloc(grants,\n\t\t\t\t sizeof(rinfo->shadow[i].grants_used[0]),\n\t\t\t\t GFP_KERNEL);\n\t\trinfo->shadow[i].sg = kvcalloc(psegs,\n\t\t\t\t\t       sizeof(rinfo->shadow[i].sg[0]),\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (info->max_indirect_segments)\n\t\t\trinfo->shadow[i].indirect_grants =\n\t\t\t\tkvcalloc(INDIRECT_GREFS(grants),\n\t\t\t\t\t sizeof(rinfo->shadow[i].indirect_grants[0]),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif ((rinfo->shadow[i].grants_used == NULL) ||\n\t\t\t(rinfo->shadow[i].sg == NULL) ||\n\t\t     (info->max_indirect_segments &&\n\t\t     (rinfo->shadow[i].indirect_grants == NULL)))\n\t\t\tgoto out_of_memory;\n\t\tsg_init_table(rinfo->shadow[i].sg, psegs);\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn 0;\n\nout_of_memory:\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\tkvfree(rinfo->shadow[i].grants_used);\n\t\trinfo->shadow[i].grants_used = NULL;\n\t\tkvfree(rinfo->shadow[i].sg);\n\t\trinfo->shadow[i].sg = NULL;\n\t\tkvfree(rinfo->shadow[i].indirect_grants);\n\t\trinfo->shadow[i].indirect_grants = NULL;\n\t}\n\tif (!list_empty(&rinfo->indirect_pages)) {\n\t\tstruct page *indirect_page, *n;\n\t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n\t\t\tlist_del(&indirect_page->lru);\n\t\t\t__free_page(indirect_page);\n\t\t}\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn -ENOMEM;\n}",
        "code_after_change": "static int blkfront_setup_indirect(struct blkfront_ring_info *rinfo)\n{\n\tunsigned int psegs, grants, memflags;\n\tint err, i;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tmemflags = memalloc_noio_save();\n\n\tif (info->max_indirect_segments == 0) {\n\t\tif (!HAS_EXTRA_REQ)\n\t\t\tgrants = BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\t\telse {\n\t\t\t/*\n\t\t\t * When an extra req is required, the maximum\n\t\t\t * grants supported is related to the size of the\n\t\t\t * Linux block segment.\n\t\t\t */\n\t\t\tgrants = GRANTS_PER_PSEG;\n\t\t}\n\t}\n\telse\n\t\tgrants = info->max_indirect_segments;\n\tpsegs = DIV_ROUND_UP(grants, GRANTS_PER_PSEG);\n\n\terr = fill_grant_buffer(rinfo,\n\t\t\t\t(grants + INDIRECT_GREFS(grants)) * BLK_RING_SIZE(info));\n\tif (err)\n\t\tgoto out_of_memory;\n\n\tif (!info->bounce && info->max_indirect_segments) {\n\t\t/*\n\t\t * We are using indirect descriptors but don't have a bounce\n\t\t * buffer, we need to allocate a set of pages that can be\n\t\t * used for mapping indirect grefs\n\t\t */\n\t\tint num = INDIRECT_GREFS(grants) * BLK_RING_SIZE(info);\n\n\t\tBUG_ON(!list_empty(&rinfo->indirect_pages));\n\t\tfor (i = 0; i < num; i++) {\n\t\t\tstruct page *indirect_page = alloc_page(GFP_KERNEL |\n\t\t\t\t\t\t\t\t__GFP_ZERO);\n\t\t\tif (!indirect_page)\n\t\t\t\tgoto out_of_memory;\n\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n\t\t}\n\t}\n\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\trinfo->shadow[i].grants_used =\n\t\t\tkvcalloc(grants,\n\t\t\t\t sizeof(rinfo->shadow[i].grants_used[0]),\n\t\t\t\t GFP_KERNEL);\n\t\trinfo->shadow[i].sg = kvcalloc(psegs,\n\t\t\t\t\t       sizeof(rinfo->shadow[i].sg[0]),\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (info->max_indirect_segments)\n\t\t\trinfo->shadow[i].indirect_grants =\n\t\t\t\tkvcalloc(INDIRECT_GREFS(grants),\n\t\t\t\t\t sizeof(rinfo->shadow[i].indirect_grants[0]),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif ((rinfo->shadow[i].grants_used == NULL) ||\n\t\t\t(rinfo->shadow[i].sg == NULL) ||\n\t\t     (info->max_indirect_segments &&\n\t\t     (rinfo->shadow[i].indirect_grants == NULL)))\n\t\t\tgoto out_of_memory;\n\t\tsg_init_table(rinfo->shadow[i].sg, psegs);\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn 0;\n\nout_of_memory:\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\tkvfree(rinfo->shadow[i].grants_used);\n\t\trinfo->shadow[i].grants_used = NULL;\n\t\tkvfree(rinfo->shadow[i].sg);\n\t\trinfo->shadow[i].sg = NULL;\n\t\tkvfree(rinfo->shadow[i].indirect_grants);\n\t\trinfo->shadow[i].indirect_grants = NULL;\n\t}\n\tif (!list_empty(&rinfo->indirect_pages)) {\n\t\tstruct page *indirect_page, *n;\n\t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n\t\t\tlist_del(&indirect_page->lru);\n\t\t\t__free_page(indirect_page);\n\t\t}\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,10 +27,10 @@\n \tif (err)\n \t\tgoto out_of_memory;\n \n-\tif (!info->feature_persistent && info->max_indirect_segments) {\n+\tif (!info->bounce && info->max_indirect_segments) {\n \t\t/*\n-\t\t * We are using indirect descriptors but not persistent\n-\t\t * grants, we need to allocate a set of pages that can be\n+\t\t * We are using indirect descriptors but don't have a bounce\n+\t\t * buffer, we need to allocate a set of pages that can be\n \t\t * used for mapping indirect grefs\n \t\t */\n \t\tint num = INDIRECT_GREFS(grants) * BLK_RING_SIZE(info);",
        "function_modified_lines": {
            "added": [
                "\tif (!info->bounce && info->max_indirect_segments) {",
                "\t\t * We are using indirect descriptors but don't have a bounce",
                "\t\t * buffer, we need to allocate a set of pages that can be"
            ],
            "deleted": [
                "\tif (!info->feature_persistent && info->max_indirect_segments) {",
                "\t\t * We are using indirect descriptors but not persistent",
                "\t\t * grants, we need to allocate a set of pages that can be"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3582
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static void xlvbd_flush(struct blkfront_info *info)\n{\n\tblk_queue_write_cache(info->rq, info->feature_flush ? true : false,\n\t\t\t      info->feature_fua ? true : false);\n\tpr_info(\"blkfront: %s: %s %s %s %s %s\\n\",\n\t\tinfo->gd->disk_name, flush_info(info),\n\t\t\"persistent grants:\", info->feature_persistent ?\n\t\t\"enabled;\" : \"disabled;\", \"indirect descriptors:\",\n\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\");\n}",
        "code_after_change": "static void xlvbd_flush(struct blkfront_info *info)\n{\n\tblk_queue_write_cache(info->rq, info->feature_flush ? true : false,\n\t\t\t      info->feature_fua ? true : false);\n\tpr_info(\"blkfront: %s: %s %s %s %s %s %s %s\\n\",\n\t\tinfo->gd->disk_name, flush_info(info),\n\t\t\"persistent grants:\", info->feature_persistent ?\n\t\t\"enabled;\" : \"disabled;\", \"indirect descriptors:\",\n\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\",\n\t\t\"bounce buffer:\", info->bounce ? \"enabled\" : \"disabled;\");\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,9 +2,10 @@\n {\n \tblk_queue_write_cache(info->rq, info->feature_flush ? true : false,\n \t\t\t      info->feature_fua ? true : false);\n-\tpr_info(\"blkfront: %s: %s %s %s %s %s\\n\",\n+\tpr_info(\"blkfront: %s: %s %s %s %s %s %s %s\\n\",\n \t\tinfo->gd->disk_name, flush_info(info),\n \t\t\"persistent grants:\", info->feature_persistent ?\n \t\t\"enabled;\" : \"disabled;\", \"indirect descriptors:\",\n-\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\");\n+\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\",\n+\t\t\"bounce buffer:\", info->bounce ? \"enabled\" : \"disabled;\");\n }",
        "function_modified_lines": {
            "added": [
                "\tpr_info(\"blkfront: %s: %s %s %s %s %s %s %s\\n\",",
                "\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\",",
                "\t\t\"bounce buffer:\", info->bounce ? \"enabled\" : \"disabled;\");"
            ],
            "deleted": [
                "\tpr_info(\"blkfront: %s: %s %s %s %s %s\\n\",",
                "\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\");"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3580
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static int binder_inc_ref(struct binder_ref *ref, int strong,\n\t\t\t  struct list_head *target_list)\n{\n\tint ret;\n\n\tif (strong) {\n\t\tif (ref->strong == 0) {\n\t\t\tret = binder_inc_node(ref->node, 1, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->strong++;\n\t} else {\n\t\tif (ref->weak == 0) {\n\t\t\tret = binder_inc_node(ref->node, 0, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->weak++;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int binder_inc_ref(struct binder_ref *ref, int strong,\n\t\t\t  struct list_head *target_list)\n{\n\tint ret;\n\n\tif (strong) {\n\t\tif (ref->data.strong == 0) {\n\t\t\tret = binder_inc_node(ref->node, 1, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->data.strong++;\n\t} else {\n\t\tif (ref->data.weak == 0) {\n\t\t\tret = binder_inc_node(ref->node, 0, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->data.weak++;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,19 +4,19 @@\n \tint ret;\n \n \tif (strong) {\n-\t\tif (ref->strong == 0) {\n+\t\tif (ref->data.strong == 0) {\n \t\t\tret = binder_inc_node(ref->node, 1, 1, target_list);\n \t\t\tif (ret)\n \t\t\t\treturn ret;\n \t\t}\n-\t\tref->strong++;\n+\t\tref->data.strong++;\n \t} else {\n-\t\tif (ref->weak == 0) {\n+\t\tif (ref->data.weak == 0) {\n \t\t\tret = binder_inc_node(ref->node, 0, 1, target_list);\n \t\t\tif (ret)\n \t\t\t\treturn ret;\n \t\t}\n-\t\tref->weak++;\n+\t\tref->data.weak++;\n \t}\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (ref->data.strong == 0) {",
                "\t\tref->data.strong++;",
                "\t\tif (ref->data.weak == 0) {",
                "\t\tref->data.weak++;"
            ],
            "deleted": [
                "\t\tif (ref->strong == 0) {",
                "\t\tref->strong++;",
                "\t\tif (ref->weak == 0) {",
                "\t\tref->weak++;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1762
    },
    {
        "cve_id": "CVE-2013-7281",
        "code_before_change": "static int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "code_after_change": "static int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,6 @@\n \n \tif (flags & MSG_OOB)\n \t\tgoto out;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(*sin);\n \n \tskb = skb_recv_datagram(sk, flags, noblock, &err);\n \tif (!skb)\n@@ -35,6 +32,7 @@\n \t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n \t\tsin->sin_port = 0;\n \t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n+\t\t*addr_len = sizeof(*sin);\n \t}\n \tif (inet->cmsg_flags)\n \t\tip_cmsg_recv(msg, skb);",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(*sin);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The dgram_recvmsg function in net/ieee802154/dgram.c in the Linux kernel before 3.12.4 updates a certain length value without ensuring that an associated data structure has been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 412
    },
    {
        "cve_id": "CVE-2017-7495",
        "code_before_change": "int ext4_map_blocks(handle_t *handle, struct inode *inode,\n\t\t    struct ext4_map_blocks *map, int flags)\n{\n\tstruct extent_status es;\n\tint retval;\n\tint ret = 0;\n#ifdef ES_AGGRESSIVE_TEST\n\tstruct ext4_map_blocks orig_map;\n\n\tmemcpy(&orig_map, map, sizeof(*map));\n#endif\n\n\tmap->m_flags = 0;\n\text_debug(\"ext4_map_blocks(): inode %lu, flag %d, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, flags, map->m_len,\n\t\t  (unsigned long) map->m_lblk);\n\n\t/*\n\t * ext4_map_blocks returns an int, and m_len is an unsigned int\n\t */\n\tif (unlikely(map->m_len > INT_MAX))\n\t\tmap->m_len = INT_MAX;\n\n\t/* We can handle the block number less than EXT_MAX_BLOCKS */\n\tif (unlikely(map->m_lblk >= EXT_MAX_BLOCKS))\n\t\treturn -EFSCORRUPTED;\n\n\t/* Lookup extent status tree firstly */\n\tif (ext4_es_lookup_extent(inode, map->m_lblk, &es)) {\n\t\tif (ext4_es_is_written(&es) || ext4_es_is_unwritten(&es)) {\n\t\t\tmap->m_pblk = ext4_es_pblock(&es) +\n\t\t\t\t\tmap->m_lblk - es.es_lblk;\n\t\t\tmap->m_flags |= ext4_es_is_written(&es) ?\n\t\t\t\t\tEXT4_MAP_MAPPED : EXT4_MAP_UNWRITTEN;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t} else if (ext4_es_is_delayed(&es) || ext4_es_is_hole(&es)) {\n\t\t\tmap->m_pblk = 0;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t\tretval = 0;\n\t\t} else {\n\t\t\tBUG_ON(1);\n\t\t}\n#ifdef ES_AGGRESSIVE_TEST\n\t\text4_map_blocks_es_recheck(handle, inode, map,\n\t\t\t\t\t   &orig_map, flags);\n#endif\n\t\tgoto found;\n\t}\n\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t}\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_find_delalloc_range(inode, map->m_lblk,\n\t\t\t\t\t     map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk,\n\t\t\t\t\t    map->m_len, map->m_pblk, status);\n\t\tif (ret < 0)\n\t\t\tretval = ret;\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\nfound:\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t/* If it is only a block(s) look up */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0)\n\t\treturn retval;\n\n\t/*\n\t * Returns if the blocks have already allocated\n\t *\n\t * Note that if blocks have been preallocated\n\t * ext4_ext_get_block() returns the create = 0\n\t * with buffer head unmapped.\n\t */\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED)\n\t\t/*\n\t\t * If we need to convert extent to unwritten\n\t\t * we continue and do the actual work in\n\t\t * ext4_ext_map_blocks()\n\t\t */\n\t\tif (!(flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN))\n\t\t\treturn retval;\n\n\t/*\n\t * Here we clear m_flags because after allocating an new extent,\n\t * it will be set again.\n\t */\n\tmap->m_flags &= ~EXT4_MAP_FLAGS;\n\n\t/*\n\t * New blocks allocate and/or writing to unwritten extent\n\t * will possibly result in updating i_data, so we take\n\t * the write lock of i_data_sem, and call get_block()\n\t * with create == 1 flag.\n\t */\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\n\t/*\n\t * We need to check for EXT4 here because migrate\n\t * could have changed the inode type in between\n\t */\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags);\n\n\t\tif (retval > 0 && map->m_flags & EXT4_MAP_NEW) {\n\t\t\t/*\n\t\t\t * We allocated new blocks which will result in\n\t\t\t * i_data's format changing.  Force the migrate\n\t\t\t * to fail by clearing migrate flags\n\t\t\t */\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);\n\t\t}\n\n\t\t/*\n\t\t * Update reserved blocks/metadata blocks after successful\n\t\t * block allocation which had been deferred till now. We don't\n\t\t * support fallocate for non extent files. So we can update\n\t\t * reserve space here.\n\t\t */\n\t\tif ((retval > 0) &&\n\t\t\t(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))\n\t\t\text4_da_update_reserve_space(inode, retval, 1);\n\t}\n\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\t/*\n\t\t * We have to zeroout blocks before inserting them into extent\n\t\t * status tree. Otherwise someone could look them up there and\n\t\t * use them before they are really zeroed.\n\t\t */\n\t\tif (flags & EXT4_GET_BLOCKS_ZERO &&\n\t\t    map->m_flags & EXT4_MAP_MAPPED &&\n\t\t    map->m_flags & EXT4_MAP_NEW) {\n\t\t\tret = ext4_issue_zeroout(inode, map->m_lblk,\n\t\t\t\t\t\t map->m_pblk, map->m_len);\n\t\t\tif (ret) {\n\t\t\t\tretval = ret;\n\t\t\t\tgoto out_sem;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * If the extent has been zeroed out, we don't need to update\n\t\t * extent status tree.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO) &&\n\t\t    ext4_es_lookup_extent(inode, map->m_lblk, &es)) {\n\t\t\tif (ext4_es_is_written(&es))\n\t\t\t\tgoto out_sem;\n\t\t}\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_find_delalloc_range(inode, map->m_lblk,\n\t\t\t\t\t     map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk, map->m_len,\n\t\t\t\t\t    map->m_pblk, status);\n\t\tif (ret < 0) {\n\t\t\tretval = ret;\n\t\t\tgoto out_sem;\n\t\t}\n\t}\n\nout_sem:\n\tup_write((&EXT4_I(inode)->i_data_sem));\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\treturn retval;\n}",
        "code_after_change": "int ext4_map_blocks(handle_t *handle, struct inode *inode,\n\t\t    struct ext4_map_blocks *map, int flags)\n{\n\tstruct extent_status es;\n\tint retval;\n\tint ret = 0;\n#ifdef ES_AGGRESSIVE_TEST\n\tstruct ext4_map_blocks orig_map;\n\n\tmemcpy(&orig_map, map, sizeof(*map));\n#endif\n\n\tmap->m_flags = 0;\n\text_debug(\"ext4_map_blocks(): inode %lu, flag %d, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, flags, map->m_len,\n\t\t  (unsigned long) map->m_lblk);\n\n\t/*\n\t * ext4_map_blocks returns an int, and m_len is an unsigned int\n\t */\n\tif (unlikely(map->m_len > INT_MAX))\n\t\tmap->m_len = INT_MAX;\n\n\t/* We can handle the block number less than EXT_MAX_BLOCKS */\n\tif (unlikely(map->m_lblk >= EXT_MAX_BLOCKS))\n\t\treturn -EFSCORRUPTED;\n\n\t/* Lookup extent status tree firstly */\n\tif (ext4_es_lookup_extent(inode, map->m_lblk, &es)) {\n\t\tif (ext4_es_is_written(&es) || ext4_es_is_unwritten(&es)) {\n\t\t\tmap->m_pblk = ext4_es_pblock(&es) +\n\t\t\t\t\tmap->m_lblk - es.es_lblk;\n\t\t\tmap->m_flags |= ext4_es_is_written(&es) ?\n\t\t\t\t\tEXT4_MAP_MAPPED : EXT4_MAP_UNWRITTEN;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t} else if (ext4_es_is_delayed(&es) || ext4_es_is_hole(&es)) {\n\t\t\tmap->m_pblk = 0;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t\tretval = 0;\n\t\t} else {\n\t\t\tBUG_ON(1);\n\t\t}\n#ifdef ES_AGGRESSIVE_TEST\n\t\text4_map_blocks_es_recheck(handle, inode, map,\n\t\t\t\t\t   &orig_map, flags);\n#endif\n\t\tgoto found;\n\t}\n\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t}\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_find_delalloc_range(inode, map->m_lblk,\n\t\t\t\t\t     map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk,\n\t\t\t\t\t    map->m_len, map->m_pblk, status);\n\t\tif (ret < 0)\n\t\t\tretval = ret;\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\nfound:\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t/* If it is only a block(s) look up */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0)\n\t\treturn retval;\n\n\t/*\n\t * Returns if the blocks have already allocated\n\t *\n\t * Note that if blocks have been preallocated\n\t * ext4_ext_get_block() returns the create = 0\n\t * with buffer head unmapped.\n\t */\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED)\n\t\t/*\n\t\t * If we need to convert extent to unwritten\n\t\t * we continue and do the actual work in\n\t\t * ext4_ext_map_blocks()\n\t\t */\n\t\tif (!(flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN))\n\t\t\treturn retval;\n\n\t/*\n\t * Here we clear m_flags because after allocating an new extent,\n\t * it will be set again.\n\t */\n\tmap->m_flags &= ~EXT4_MAP_FLAGS;\n\n\t/*\n\t * New blocks allocate and/or writing to unwritten extent\n\t * will possibly result in updating i_data, so we take\n\t * the write lock of i_data_sem, and call get_block()\n\t * with create == 1 flag.\n\t */\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\n\t/*\n\t * We need to check for EXT4 here because migrate\n\t * could have changed the inode type in between\n\t */\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags);\n\n\t\tif (retval > 0 && map->m_flags & EXT4_MAP_NEW) {\n\t\t\t/*\n\t\t\t * We allocated new blocks which will result in\n\t\t\t * i_data's format changing.  Force the migrate\n\t\t\t * to fail by clearing migrate flags\n\t\t\t */\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);\n\t\t}\n\n\t\t/*\n\t\t * Update reserved blocks/metadata blocks after successful\n\t\t * block allocation which had been deferred till now. We don't\n\t\t * support fallocate for non extent files. So we can update\n\t\t * reserve space here.\n\t\t */\n\t\tif ((retval > 0) &&\n\t\t\t(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))\n\t\t\text4_da_update_reserve_space(inode, retval, 1);\n\t}\n\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\t/*\n\t\t * We have to zeroout blocks before inserting them into extent\n\t\t * status tree. Otherwise someone could look them up there and\n\t\t * use them before they are really zeroed.\n\t\t */\n\t\tif (flags & EXT4_GET_BLOCKS_ZERO &&\n\t\t    map->m_flags & EXT4_MAP_MAPPED &&\n\t\t    map->m_flags & EXT4_MAP_NEW) {\n\t\t\tret = ext4_issue_zeroout(inode, map->m_lblk,\n\t\t\t\t\t\t map->m_pblk, map->m_len);\n\t\t\tif (ret) {\n\t\t\t\tretval = ret;\n\t\t\t\tgoto out_sem;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * If the extent has been zeroed out, we don't need to update\n\t\t * extent status tree.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO) &&\n\t\t    ext4_es_lookup_extent(inode, map->m_lblk, &es)) {\n\t\t\tif (ext4_es_is_written(&es))\n\t\t\t\tgoto out_sem;\n\t\t}\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_find_delalloc_range(inode, map->m_lblk,\n\t\t\t\t\t     map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk, map->m_len,\n\t\t\t\t\t    map->m_pblk, status);\n\t\tif (ret < 0) {\n\t\t\tretval = ret;\n\t\t\tgoto out_sem;\n\t\t}\n\t}\n\nout_sem:\n\tup_write((&EXT4_I(inode)->i_data_sem));\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * Inodes with freshly allocated blocks where contents will be\n\t\t * visible after transaction commit must be on transaction's\n\t\t * ordered data list.\n\t\t */\n\t\tif (map->m_flags & EXT4_MAP_NEW &&\n\t\t    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&\n\t\t    !(flags & EXT4_GET_BLOCKS_ZERO) &&\n\t\t    !IS_NOQUOTA(inode) &&\n\t\t    ext4_should_order_data(inode)) {\n\t\t\tret = ext4_jbd2_file_inode(handle, inode);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -217,6 +217,21 @@\n \t\tret = check_block_validity(inode, map);\n \t\tif (ret != 0)\n \t\t\treturn ret;\n+\n+\t\t/*\n+\t\t * Inodes with freshly allocated blocks where contents will be\n+\t\t * visible after transaction commit must be on transaction's\n+\t\t * ordered data list.\n+\t\t */\n+\t\tif (map->m_flags & EXT4_MAP_NEW &&\n+\t\t    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&\n+\t\t    !(flags & EXT4_GET_BLOCKS_ZERO) &&\n+\t\t    !IS_NOQUOTA(inode) &&\n+\t\t    ext4_should_order_data(inode)) {\n+\t\t\tret = ext4_jbd2_file_inode(handle, inode);\n+\t\t\tif (ret)\n+\t\t\t\treturn ret;\n+\t\t}\n \t}\n \treturn retval;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t/*",
                "\t\t * Inodes with freshly allocated blocks where contents will be",
                "\t\t * visible after transaction commit must be on transaction's",
                "\t\t * ordered data list.",
                "\t\t */",
                "\t\tif (map->m_flags & EXT4_MAP_NEW &&",
                "\t\t    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&",
                "\t\t    !(flags & EXT4_GET_BLOCKS_ZERO) &&",
                "\t\t    !IS_NOQUOTA(inode) &&",
                "\t\t    ext4_should_order_data(inode)) {",
                "\t\t\tret = ext4_jbd2_file_inode(handle, inode);",
                "\t\t\tif (ret)",
                "\t\t\t\treturn ret;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "fs/ext4/inode.c in the Linux kernel before 4.6.2, when ext4 data=ordered mode is used, mishandles a needs-flushing-before-commit list, which allows local users to obtain sensitive information from other users' files in opportunistic circumstances by waiting for a hardware reset, creating a new file, making write system calls, and reading this file.",
        "id": 1508
    },
    {
        "cve_id": "CVE-2017-14991",
        "code_before_change": "static void\nsg_fill_request_table(Sg_fd *sfp, sg_req_info_t *rinfo)\n{\n\tSg_request *srp;\n\tint val;\n\tunsigned int ms;\n\n\tval = 0;\n\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\tif (val > SG_MAX_QUEUE)\n\t\t\tbreak;\n\t\tmemset(&rinfo[val], 0, SZ_SG_REQ_INFO);\n\t\trinfo[val].req_state = srp->done + 1;\n\t\trinfo[val].problem =\n\t\t\tsrp->header.masked_status &\n\t\t\tsrp->header.host_status &\n\t\t\tsrp->header.driver_status;\n\t\tif (srp->done)\n\t\t\trinfo[val].duration =\n\t\t\t\tsrp->header.duration;\n\t\telse {\n\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\trinfo[val].duration =\n\t\t\t\t(ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\t\t}\n\t\trinfo[val].orphan = srp->orphan;\n\t\trinfo[val].sg_io_owned = srp->sg_io_owned;\n\t\trinfo[val].pack_id = srp->header.pack_id;\n\t\trinfo[val].usr_ptr = srp->header.usr_ptr;\n\t\tval++;\n\t}\n}",
        "code_after_change": "static void\nsg_fill_request_table(Sg_fd *sfp, sg_req_info_t *rinfo)\n{\n\tSg_request *srp;\n\tint val;\n\tunsigned int ms;\n\n\tval = 0;\n\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\tif (val > SG_MAX_QUEUE)\n\t\t\tbreak;\n\t\trinfo[val].req_state = srp->done + 1;\n\t\trinfo[val].problem =\n\t\t\tsrp->header.masked_status &\n\t\t\tsrp->header.host_status &\n\t\t\tsrp->header.driver_status;\n\t\tif (srp->done)\n\t\t\trinfo[val].duration =\n\t\t\t\tsrp->header.duration;\n\t\telse {\n\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\trinfo[val].duration =\n\t\t\t\t(ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\t\t}\n\t\trinfo[val].orphan = srp->orphan;\n\t\trinfo[val].sg_io_owned = srp->sg_io_owned;\n\t\trinfo[val].pack_id = srp->header.pack_id;\n\t\trinfo[val].usr_ptr = srp->header.usr_ptr;\n\t\tval++;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,6 @@\n \tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n \t\tif (val > SG_MAX_QUEUE)\n \t\t\tbreak;\n-\t\tmemset(&rinfo[val], 0, SZ_SG_REQ_INFO);\n \t\trinfo[val].req_state = srp->done + 1;\n \t\trinfo[val].problem =\n \t\t\tsrp->header.masked_status &",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tmemset(&rinfo[val], 0, SZ_SG_REQ_INFO);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The sg_ioctl function in drivers/scsi/sg.c in the Linux kernel before 4.13.4 allows local users to obtain sensitive information from uninitialized kernel heap-memory locations via an SG_GET_REQUEST_TABLE ioctl call for /dev/sg0.",
        "id": 1286
    },
    {
        "cve_id": "CVE-2013-3235",
        "code_before_change": "static int recv_msg(struct kiocb *iocb, struct socket *sock,\n\t\t    struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tu32 err;\n\tint res;\n\n\t/* Catch invalid receive requests */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (unlikely(sock->state == SS_UNCONNECTED)) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\nrestart:\n\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\n\t/* Capture sender's address (optional) */\n\tset_orig_addr(m, msg);\n\n\t/* Capture ancillary data (optional) */\n\tres = anc_data_recv(m, msg, tport);\n\tif (res)\n\t\tgoto exit;\n\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tif (unlikely(buf_len < sz)) {\n\t\t\tsz = buf_len;\n\t\t\tm->msg_flags |= MSG_TRUNC;\n\t\t}\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg),\n\t\t\t\t\t      m->msg_iov, sz);\n\t\tif (res)\n\t\t\tgoto exit;\n\t\tres = sz;\n\t} else {\n\t\tif ((sock->state == SS_READY) ||\n\t\t    ((err == TIPC_CONN_SHUTDOWN) || m->msg_control))\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif ((sock->state != SS_READY) &&\n\t\t    (++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\nexit:\n\trelease_sock(sk);\n\treturn res;\n}",
        "code_after_change": "static int recv_msg(struct kiocb *iocb, struct socket *sock,\n\t\t    struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tu32 err;\n\tint res;\n\n\t/* Catch invalid receive requests */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (unlikely(sock->state == SS_UNCONNECTED)) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\n\t/* will be updated in set_orig_addr() if needed */\n\tm->msg_namelen = 0;\n\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\nrestart:\n\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\n\t/* Capture sender's address (optional) */\n\tset_orig_addr(m, msg);\n\n\t/* Capture ancillary data (optional) */\n\tres = anc_data_recv(m, msg, tport);\n\tif (res)\n\t\tgoto exit;\n\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tif (unlikely(buf_len < sz)) {\n\t\t\tsz = buf_len;\n\t\t\tm->msg_flags |= MSG_TRUNC;\n\t\t}\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg),\n\t\t\t\t\t      m->msg_iov, sz);\n\t\tif (res)\n\t\t\tgoto exit;\n\t\tres = sz;\n\t} else {\n\t\tif ((sock->state == SS_READY) ||\n\t\t    ((err == TIPC_CONN_SHUTDOWN) || m->msg_control))\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif ((sock->state != SS_READY) &&\n\t\t    (++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\nexit:\n\trelease_sock(sk);\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,6 +20,9 @@\n \t\tres = -ENOTCONN;\n \t\tgoto exit;\n \t}\n+\n+\t/* will be updated in set_orig_addr() if needed */\n+\tm->msg_namelen = 0;\n \n \ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n restart:",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* will be updated in set_orig_addr() if needed */",
                "\tm->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "net/tipc/socket.c in the Linux kernel before 3.9-rc7 does not initialize a certain data structure and a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 275
    },
    {
        "cve_id": "CVE-2015-4176",
        "code_before_change": "void __detach_mounts(struct dentry *dentry)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\n\tnamespace_lock();\n\tmp = lookup_mountpoint(dentry);\n\tif (IS_ERR_OR_NULL(mp))\n\t\tgoto out_unlock;\n\n\tlock_mount_hash();\n\twhile (!hlist_empty(&mp->m_list)) {\n\t\tmnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);\n\t\tif (mnt->mnt.mnt_flags & MNT_UMOUNT) {\n\t\t\tstruct mount *p, *tmp;\n\t\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t\thlist_add_head(&p->mnt_umount.s_list, &unmounted);\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\telse umount_tree(mnt, 0);\n\t}\n\tunlock_mount_hash();\n\tput_mountpoint(mp);\nout_unlock:\n\tnamespace_unlock();\n}",
        "code_after_change": "void __detach_mounts(struct dentry *dentry)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\n\tnamespace_lock();\n\tmp = lookup_mountpoint(dentry);\n\tif (IS_ERR_OR_NULL(mp))\n\t\tgoto out_unlock;\n\n\tlock_mount_hash();\n\twhile (!hlist_empty(&mp->m_list)) {\n\t\tmnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);\n\t\tif (mnt->mnt.mnt_flags & MNT_UMOUNT) {\n\t\t\tstruct mount *p, *tmp;\n\t\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t\thlist_add_head(&p->mnt_umount.s_list, &unmounted);\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\telse umount_tree(mnt, UMOUNT_CONNECTED);\n\t}\n\tunlock_mount_hash();\n\tput_mountpoint(mp);\nout_unlock:\n\tnamespace_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \t\t\t\tumount_mnt(p);\n \t\t\t}\n \t\t}\n-\t\telse umount_tree(mnt, 0);\n+\t\telse umount_tree(mnt, UMOUNT_CONNECTED);\n \t}\n \tunlock_mount_hash();\n \tput_mountpoint(mp);",
        "function_modified_lines": {
            "added": [
                "\t\telse umount_tree(mnt, UMOUNT_CONNECTED);"
            ],
            "deleted": [
                "\t\telse umount_tree(mnt, 0);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "fs/namespace.c in the Linux kernel before 4.0.2 does not properly support mount connectivity, which allows local users to read arbitrary files by leveraging user-namespace root access for deletion of a file or directory.",
        "id": 765
    },
    {
        "cve_id": "CVE-2012-6536",
        "code_before_change": "static int xfrm_alloc_replay_state_esn(struct xfrm_replay_state_esn **replay_esn,\n\t\t\t\t       struct xfrm_replay_state_esn **preplay_esn,\n\t\t\t\t       struct nlattr *rta)\n{\n\tstruct xfrm_replay_state_esn *p, *pp, *up;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tup = nla_data(rta);\n\n\tp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tpp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);\n\tif (!pp) {\n\t\tkfree(p);\n\t\treturn -ENOMEM;\n\t}\n\n\t*replay_esn = p;\n\t*preplay_esn = pp;\n\n\treturn 0;\n}",
        "code_after_change": "static int xfrm_alloc_replay_state_esn(struct xfrm_replay_state_esn **replay_esn,\n\t\t\t\t       struct xfrm_replay_state_esn **preplay_esn,\n\t\t\t\t       struct nlattr *rta)\n{\n\tstruct xfrm_replay_state_esn *p, *pp, *up;\n\tint klen, ulen;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tup = nla_data(rta);\n\tklen = xfrm_replay_state_esn_len(up);\n\tulen = nla_len(rta) >= klen ? klen : sizeof(*up);\n\n\tp = kzalloc(klen, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tpp = kzalloc(klen, GFP_KERNEL);\n\tif (!pp) {\n\t\tkfree(p);\n\t\treturn -ENOMEM;\n\t}\n\n\tmemcpy(p, up, ulen);\n\tmemcpy(pp, up, ulen);\n\n\t*replay_esn = p;\n\t*preplay_esn = pp;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,21 +3,27 @@\n \t\t\t\t       struct nlattr *rta)\n {\n \tstruct xfrm_replay_state_esn *p, *pp, *up;\n+\tint klen, ulen;\n \n \tif (!rta)\n \t\treturn 0;\n \n \tup = nla_data(rta);\n+\tklen = xfrm_replay_state_esn_len(up);\n+\tulen = nla_len(rta) >= klen ? klen : sizeof(*up);\n \n-\tp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);\n+\tp = kzalloc(klen, GFP_KERNEL);\n \tif (!p)\n \t\treturn -ENOMEM;\n \n-\tpp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);\n+\tpp = kzalloc(klen, GFP_KERNEL);\n \tif (!pp) {\n \t\tkfree(p);\n \t\treturn -ENOMEM;\n \t}\n+\n+\tmemcpy(p, up, ulen);\n+\tmemcpy(pp, up, ulen);\n \n \t*replay_esn = p;\n \t*preplay_esn = pp;",
        "function_modified_lines": {
            "added": [
                "\tint klen, ulen;",
                "\tklen = xfrm_replay_state_esn_len(up);",
                "\tulen = nla_len(rta) >= klen ? klen : sizeof(*up);",
                "\tp = kzalloc(klen, GFP_KERNEL);",
                "\tpp = kzalloc(klen, GFP_KERNEL);",
                "",
                "\tmemcpy(p, up, ulen);",
                "\tmemcpy(pp, up, ulen);"
            ],
            "deleted": [
                "\tp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);",
                "\tpp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "net/xfrm/xfrm_user.c in the Linux kernel before 3.6 does not verify that the actual Netlink message length is consistent with a certain header field, which allows local users to obtain sensitive information from kernel heap memory by leveraging the CAP_NET_ADMIN capability and providing a (1) new or (2) updated state.",
        "id": 120
    },
    {
        "cve_id": "CVE-2022-33741",
        "code_before_change": "static int xennet_connect(struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tunsigned int num_queues = 0;\n\tint err;\n\tunsigned int j = 0;\n\tstruct netfront_queue *queue = NULL;\n\n\tif (!xenbus_read_unsigned(np->xbdev->otherend, \"feature-rx-copy\", 0)) {\n\t\tdev_info(&dev->dev,\n\t\t\t \"backend does not support copying receive path\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\terr = talk_to_netback(np->xbdev, np);\n\tif (err)\n\t\treturn err;\n\tif (np->netback_has_xdp_headroom)\n\t\tpr_info(\"backend supports XDP headroom\\n\");\n\n\t/* talk_to_netback() sets the correct number of queues */\n\tnum_queues = dev->real_num_tx_queues;\n\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\terr = register_netdev(dev);\n\t\tif (err) {\n\t\t\tpr_warn(\"%s: register_netdev err=%d\\n\", __func__, err);\n\t\t\tdevice_unregister(&np->xbdev->dev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\trtnl_lock();\n\tnetdev_update_features(dev);\n\trtnl_unlock();\n\n\t/*\n\t * All public and private state should now be sane.  Get\n\t * ready to start sending and receiving packets and give the driver\n\t * domain a kick because we've probably just requeued some\n\t * packets.\n\t */\n\tnetif_tx_lock_bh(np->netdev);\n\tnetif_device_attach(np->netdev);\n\tnetif_tx_unlock_bh(np->netdev);\n\n\tnetif_carrier_on(np->netdev);\n\tfor (j = 0; j < num_queues; ++j) {\n\t\tqueue = &np->queues[j];\n\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\t\tif (queue->tx_irq != queue->rx_irq)\n\t\t\tnotify_remote_via_irq(queue->rx_irq);\n\n\t\tspin_lock_irq(&queue->tx_lock);\n\t\txennet_tx_buf_gc(queue);\n\t\tspin_unlock_irq(&queue->tx_lock);\n\n\t\tspin_lock_bh(&queue->rx_lock);\n\t\txennet_alloc_rx_buffers(queue);\n\t\tspin_unlock_bh(&queue->rx_lock);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int xennet_connect(struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tunsigned int num_queues = 0;\n\tint err;\n\tunsigned int j = 0;\n\tstruct netfront_queue *queue = NULL;\n\n\tif (!xenbus_read_unsigned(np->xbdev->otherend, \"feature-rx-copy\", 0)) {\n\t\tdev_info(&dev->dev,\n\t\t\t \"backend does not support copying receive path\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\terr = talk_to_netback(np->xbdev, np);\n\tif (err)\n\t\treturn err;\n\tif (np->netback_has_xdp_headroom)\n\t\tpr_info(\"backend supports XDP headroom\\n\");\n\tif (np->bounce)\n\t\tdev_info(&np->xbdev->dev,\n\t\t\t \"bouncing transmitted data to zeroed pages\\n\");\n\n\t/* talk_to_netback() sets the correct number of queues */\n\tnum_queues = dev->real_num_tx_queues;\n\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\terr = register_netdev(dev);\n\t\tif (err) {\n\t\t\tpr_warn(\"%s: register_netdev err=%d\\n\", __func__, err);\n\t\t\tdevice_unregister(&np->xbdev->dev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\trtnl_lock();\n\tnetdev_update_features(dev);\n\trtnl_unlock();\n\n\t/*\n\t * All public and private state should now be sane.  Get\n\t * ready to start sending and receiving packets and give the driver\n\t * domain a kick because we've probably just requeued some\n\t * packets.\n\t */\n\tnetif_tx_lock_bh(np->netdev);\n\tnetif_device_attach(np->netdev);\n\tnetif_tx_unlock_bh(np->netdev);\n\n\tnetif_carrier_on(np->netdev);\n\tfor (j = 0; j < num_queues; ++j) {\n\t\tqueue = &np->queues[j];\n\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\t\tif (queue->tx_irq != queue->rx_irq)\n\t\t\tnotify_remote_via_irq(queue->rx_irq);\n\n\t\tspin_lock_irq(&queue->tx_lock);\n\t\txennet_tx_buf_gc(queue);\n\t\tspin_unlock_irq(&queue->tx_lock);\n\n\t\tspin_lock_bh(&queue->rx_lock);\n\t\txennet_alloc_rx_buffers(queue);\n\t\tspin_unlock_bh(&queue->rx_lock);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,9 @@\n \t\treturn err;\n \tif (np->netback_has_xdp_headroom)\n \t\tpr_info(\"backend supports XDP headroom\\n\");\n+\tif (np->bounce)\n+\t\tdev_info(&np->xbdev->dev,\n+\t\t\t \"bouncing transmitted data to zeroed pages\\n\");\n \n \t/* talk_to_netback() sets the correct number of queues */\n \tnum_queues = dev->real_num_tx_queues;",
        "function_modified_lines": {
            "added": [
                "\tif (np->bounce)",
                "\t\tdev_info(&np->xbdev->dev,",
                "\t\t\t \"bouncing transmitted data to zeroed pages\\n\");"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3579
    },
    {
        "cve_id": "CVE-2022-33741",
        "code_before_change": "static int talk_to_netback(struct xenbus_device *dev,\n\t\t\t   struct netfront_info *info)\n{\n\tconst char *message;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tunsigned int feature_split_evtchn;\n\tunsigned int i = 0;\n\tunsigned int max_queues = 0;\n\tstruct netfront_queue *queue = NULL;\n\tunsigned int num_queues = 1;\n\tu8 addr[ETH_ALEN];\n\n\tinfo->netdev->irq = 0;\n\n\t/* Check if backend supports multiple queues */\n\tmax_queues = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t  \"multi-queue-max-queues\", 1);\n\tnum_queues = min(max_queues, xennet_max_queues);\n\n\t/* Check feature-split-event-channels */\n\tfeature_split_evtchn = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\"feature-split-event-channels\", 0);\n\n\t/* Read mac addr. */\n\terr = xen_net_read_mac(dev, addr);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"parsing %s/mac\", dev->nodename);\n\t\tgoto out_unlocked;\n\t}\n\teth_hw_addr_set(info->netdev, addr);\n\n\tinfo->netback_has_xdp_headroom = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t\t      \"feature-xdp-headroom\", 0);\n\tif (info->netback_has_xdp_headroom) {\n\t\t/* set the current xen-netfront xdp state */\n\t\terr = talk_to_netback_xdp(info, info->netfront_xdp_enabled ?\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_ENABLE :\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_DISABLE);\n\t\tif (err)\n\t\t\tgoto out_unlocked;\n\t}\n\n\trtnl_lock();\n\tif (info->queues)\n\t\txennet_destroy_queues(info);\n\n\t/* For the case of a reconnect reset the \"broken\" indicator. */\n\tinfo->broken = false;\n\n\terr = xennet_create_queues(info, &num_queues);\n\tif (err < 0) {\n\t\txenbus_dev_fatal(dev, err, \"creating queues\");\n\t\tkfree(info->queues);\n\t\tinfo->queues = NULL;\n\t\tgoto out;\n\t}\n\trtnl_unlock();\n\n\t/* Create shared ring, alloc event channel -- for each queue */\n\tfor (i = 0; i < num_queues; ++i) {\n\t\tqueue = &info->queues[i];\n\t\terr = setup_netfront(dev, queue, feature_split_evtchn);\n\t\tif (err)\n\t\t\tgoto destroy_ring;\n\t}\n\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"starting transaction\");\n\t\tgoto destroy_ring;\n\t}\n\n\tif (xenbus_exists(XBT_NIL,\n\t\t\t  info->xbdev->otherend, \"multi-queue-max-queues\")) {\n\t\t/* Write the number of queues */\n\t\terr = xenbus_printf(xbt, dev->nodename,\n\t\t\t\t    \"multi-queue-num-queues\", \"%u\", num_queues);\n\t\tif (err) {\n\t\t\tmessage = \"writing multi-queue-num-queues\";\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\n\tif (num_queues == 1) {\n\t\terr = write_queue_xenstore_keys(&info->queues[0], &xbt, 0); /* flat */\n\t\tif (err)\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t} else {\n\t\t/* Write the keys for each queue */\n\t\tfor (i = 0; i < num_queues; ++i) {\n\t\t\tqueue = &info->queues[i];\n\t\t\terr = write_queue_xenstore_keys(queue, &xbt, 1); /* hierarchical */\n\t\t\tif (err)\n\t\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\n\t/* The remaining keys are not queue-specific */\n\terr = xenbus_printf(xbt, dev->nodename, \"request-rx-copy\", \"%u\",\n\t\t\t    1);\n\tif (err) {\n\t\tmessage = \"writing request-rx-copy\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-rx-notify\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-rx-notify\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-sg\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-sg\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-gso-tcpv4\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-gso-tcpv4\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_write(xbt, dev->nodename, \"feature-gso-tcpv6\", \"1\");\n\tif (err) {\n\t\tmessage = \"writing feature-gso-tcpv6\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_write(xbt, dev->nodename, \"feature-ipv6-csum-offload\",\n\t\t\t   \"1\");\n\tif (err) {\n\t\tmessage = \"writing feature-ipv6-csum-offload\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_transaction_end(xbt, 0);\n\tif (err) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto again;\n\t\txenbus_dev_fatal(dev, err, \"completing transaction\");\n\t\tgoto destroy_ring;\n\t}\n\n\treturn 0;\n\n abort_transaction:\n\txenbus_dev_fatal(dev, err, \"%s\", message);\nabort_transaction_no_dev_fatal:\n\txenbus_transaction_end(xbt, 1);\n destroy_ring:\n\txennet_disconnect_backend(info);\n\trtnl_lock();\n\txennet_destroy_queues(info);\n out:\n\trtnl_unlock();\nout_unlocked:\n\tdevice_unregister(&dev->dev);\n\treturn err;\n}",
        "code_after_change": "static int talk_to_netback(struct xenbus_device *dev,\n\t\t\t   struct netfront_info *info)\n{\n\tconst char *message;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tunsigned int feature_split_evtchn;\n\tunsigned int i = 0;\n\tunsigned int max_queues = 0;\n\tstruct netfront_queue *queue = NULL;\n\tunsigned int num_queues = 1;\n\tu8 addr[ETH_ALEN];\n\n\tinfo->netdev->irq = 0;\n\n\t/* Check if backend is trusted. */\n\tinfo->bounce = !xennet_trusted ||\n\t\t       !xenbus_read_unsigned(dev->nodename, \"trusted\", 1);\n\n\t/* Check if backend supports multiple queues */\n\tmax_queues = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t  \"multi-queue-max-queues\", 1);\n\tnum_queues = min(max_queues, xennet_max_queues);\n\n\t/* Check feature-split-event-channels */\n\tfeature_split_evtchn = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\"feature-split-event-channels\", 0);\n\n\t/* Read mac addr. */\n\terr = xen_net_read_mac(dev, addr);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"parsing %s/mac\", dev->nodename);\n\t\tgoto out_unlocked;\n\t}\n\teth_hw_addr_set(info->netdev, addr);\n\n\tinfo->netback_has_xdp_headroom = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t\t      \"feature-xdp-headroom\", 0);\n\tif (info->netback_has_xdp_headroom) {\n\t\t/* set the current xen-netfront xdp state */\n\t\terr = talk_to_netback_xdp(info, info->netfront_xdp_enabled ?\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_ENABLE :\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_DISABLE);\n\t\tif (err)\n\t\t\tgoto out_unlocked;\n\t}\n\n\trtnl_lock();\n\tif (info->queues)\n\t\txennet_destroy_queues(info);\n\n\t/* For the case of a reconnect reset the \"broken\" indicator. */\n\tinfo->broken = false;\n\n\terr = xennet_create_queues(info, &num_queues);\n\tif (err < 0) {\n\t\txenbus_dev_fatal(dev, err, \"creating queues\");\n\t\tkfree(info->queues);\n\t\tinfo->queues = NULL;\n\t\tgoto out;\n\t}\n\trtnl_unlock();\n\n\t/* Create shared ring, alloc event channel -- for each queue */\n\tfor (i = 0; i < num_queues; ++i) {\n\t\tqueue = &info->queues[i];\n\t\terr = setup_netfront(dev, queue, feature_split_evtchn);\n\t\tif (err)\n\t\t\tgoto destroy_ring;\n\t}\n\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"starting transaction\");\n\t\tgoto destroy_ring;\n\t}\n\n\tif (xenbus_exists(XBT_NIL,\n\t\t\t  info->xbdev->otherend, \"multi-queue-max-queues\")) {\n\t\t/* Write the number of queues */\n\t\terr = xenbus_printf(xbt, dev->nodename,\n\t\t\t\t    \"multi-queue-num-queues\", \"%u\", num_queues);\n\t\tif (err) {\n\t\t\tmessage = \"writing multi-queue-num-queues\";\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\n\tif (num_queues == 1) {\n\t\terr = write_queue_xenstore_keys(&info->queues[0], &xbt, 0); /* flat */\n\t\tif (err)\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t} else {\n\t\t/* Write the keys for each queue */\n\t\tfor (i = 0; i < num_queues; ++i) {\n\t\t\tqueue = &info->queues[i];\n\t\t\terr = write_queue_xenstore_keys(queue, &xbt, 1); /* hierarchical */\n\t\t\tif (err)\n\t\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\n\t/* The remaining keys are not queue-specific */\n\terr = xenbus_printf(xbt, dev->nodename, \"request-rx-copy\", \"%u\",\n\t\t\t    1);\n\tif (err) {\n\t\tmessage = \"writing request-rx-copy\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-rx-notify\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-rx-notify\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-sg\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-sg\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-gso-tcpv4\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-gso-tcpv4\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_write(xbt, dev->nodename, \"feature-gso-tcpv6\", \"1\");\n\tif (err) {\n\t\tmessage = \"writing feature-gso-tcpv6\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_write(xbt, dev->nodename, \"feature-ipv6-csum-offload\",\n\t\t\t   \"1\");\n\tif (err) {\n\t\tmessage = \"writing feature-ipv6-csum-offload\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_transaction_end(xbt, 0);\n\tif (err) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto again;\n\t\txenbus_dev_fatal(dev, err, \"completing transaction\");\n\t\tgoto destroy_ring;\n\t}\n\n\treturn 0;\n\n abort_transaction:\n\txenbus_dev_fatal(dev, err, \"%s\", message);\nabort_transaction_no_dev_fatal:\n\txenbus_transaction_end(xbt, 1);\n destroy_ring:\n\txennet_disconnect_backend(info);\n\trtnl_lock();\n\txennet_destroy_queues(info);\n out:\n\trtnl_unlock();\nout_unlocked:\n\tdevice_unregister(&dev->dev);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,10 @@\n \tu8 addr[ETH_ALEN];\n \n \tinfo->netdev->irq = 0;\n+\n+\t/* Check if backend is trusted. */\n+\tinfo->bounce = !xennet_trusted ||\n+\t\t       !xenbus_read_unsigned(dev->nodename, \"trusted\", 1);\n \n \t/* Check if backend supports multiple queues */\n \tmax_queues = xenbus_read_unsigned(info->xbdev->otherend,",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* Check if backend is trusted. */",
                "\tinfo->bounce = !xennet_trusted ||",
                "\t\t       !xenbus_read_unsigned(dev->nodename, \"trusted\", 1);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3578
    },
    {
        "cve_id": "CVE-2016-9756",
        "code_before_change": "static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel, old_sel;\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\t/* Assignment of RIP may only fail in 64-bit mode */\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_CALL_JMP,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\n\t\t/* assigning eip failed; restore the old cs */\n\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\n\t\treturn rc;\n\t}\n\treturn rc;\n}",
        "code_after_change": "static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel;\n\tstruct desc_struct new_desc;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_CALL_JMP,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n\t/* Error handling is not implemented. */\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,15 +1,9 @@\n static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n {\n \tint rc;\n-\tunsigned short sel, old_sel;\n-\tstruct desc_struct old_desc, new_desc;\n-\tconst struct x86_emulate_ops *ops = ctxt->ops;\n+\tunsigned short sel;\n+\tstruct desc_struct new_desc;\n \tu8 cpl = ctxt->ops->cpl(ctxt);\n-\n-\t/* Assignment of RIP may only fail in 64-bit mode */\n-\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n-\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,\n-\t\t\t\t VCPU_SREG_CS);\n \n \tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n \n@@ -20,11 +14,9 @@\n \t\treturn rc;\n \n \trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n-\tif (rc != X86EMUL_CONTINUE) {\n-\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\n-\t\t/* assigning eip failed; restore the old cs */\n-\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\n-\t\treturn rc;\n-\t}\n+\t/* Error handling is not implemented. */\n+\tif (rc != X86EMUL_CONTINUE)\n+\t\treturn X86EMUL_UNHANDLEABLE;\n+\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned short sel;",
                "\tstruct desc_struct new_desc;",
                "\t/* Error handling is not implemented. */",
                "\tif (rc != X86EMUL_CONTINUE)",
                "\t\treturn X86EMUL_UNHANDLEABLE;",
                ""
            ],
            "deleted": [
                "\tunsigned short sel, old_sel;",
                "\tstruct desc_struct old_desc, new_desc;",
                "\tconst struct x86_emulate_ops *ops = ctxt->ops;",
                "",
                "\t/* Assignment of RIP may only fail in 64-bit mode */",
                "\tif (ctxt->mode == X86EMUL_MODE_PROT64)",
                "\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,",
                "\t\t\t\t VCPU_SREG_CS);",
                "\tif (rc != X86EMUL_CONTINUE) {",
                "\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);",
                "\t\t/* assigning eip failed; restore the old cs */",
                "\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);",
                "\t\treturn rc;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel before 4.8.12 does not properly initialize Code Segment (CS) in certain error cases, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 1165
    },
    {
        "cve_id": "CVE-2017-2584",
        "code_before_change": "static int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  void (*get)(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\t      struct desc_ptr *ptr))\n{\n\tstruct desc_ptr desc_ptr;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tctxt->op_bytes = 8;\n\tget(ctxt, &desc_ptr);\n\tif (ctxt->op_bytes == 2) {\n\t\tctxt->op_bytes = 4;\n\t\tdesc_ptr.address &= 0x00ffffff;\n\t}\n\t/* Disable writeback. */\n\tctxt->dst.type = OP_NONE;\n\treturn segmented_write(ctxt, ctxt->dst.addr.mem,\n\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);\n}",
        "code_after_change": "static int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  void (*get)(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\t      struct desc_ptr *ptr))\n{\n\tstruct desc_ptr desc_ptr;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tctxt->op_bytes = 8;\n\tget(ctxt, &desc_ptr);\n\tif (ctxt->op_bytes == 2) {\n\t\tctxt->op_bytes = 4;\n\t\tdesc_ptr.address &= 0x00ffffff;\n\t}\n\t/* Disable writeback. */\n\tctxt->dst.type = OP_NONE;\n\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,\n\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,6 @@\n \t}\n \t/* Disable writeback. */\n \tctxt->dst.type = OP_NONE;\n-\treturn segmented_write(ctxt, ctxt->dst.addr.mem,\n-\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);\n+\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,\n+\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,",
                "\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);"
            ],
            "deleted": [
                "\treturn segmented_write(ctxt, ctxt->dst.addr.mem,",
                "\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-416"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel through 4.9.3 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free) via a crafted application that leverages instruction emulation for fxrstor, fxsave, sgdt, and sidt.",
        "id": 1445
    },
    {
        "cve_id": "CVE-2017-15537",
        "code_before_change": "int xstateregs_set(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct xregs_state *xsave;\n\tint ret;\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn -ENODEV;\n\n\t/*\n\t * A whole standard-format XSAVE buffer is needed:\n\t */\n\tif ((pos != 0) || (count < fpu_user_xstate_size))\n\t\treturn -EFAULT;\n\n\txsave = &fpu->state.xsave;\n\n\tfpu__activate_fpstate_write(fpu);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES)) {\n\t\tif (kbuf)\n\t\t\tret = copy_kernel_to_xstate(xsave, kbuf);\n\t\telse\n\t\t\tret = copy_user_to_xstate(xsave, ubuf);\n\t} else {\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);\n\t}\n\n\t/*\n\t * In case of failure, mark all states as init:\n\t */\n\tif (ret)\n\t\tfpstate_init(&fpu->state);\n\n\t/*\n\t * mxcsr reserved bits must be masked to zero for security reasons.\n\t */\n\txsave->i387.mxcsr &= mxcsr_feature_mask;\n\txsave->header.xfeatures &= xfeatures_mask;\n\t/*\n\t * These bits must be zero.\n\t */\n\tmemset(&xsave->header.reserved, 0, 48);\n\n\treturn ret;\n}",
        "code_after_change": "int xstateregs_set(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct xregs_state *xsave;\n\tint ret;\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn -ENODEV;\n\n\t/*\n\t * A whole standard-format XSAVE buffer is needed:\n\t */\n\tif ((pos != 0) || (count < fpu_user_xstate_size))\n\t\treturn -EFAULT;\n\n\txsave = &fpu->state.xsave;\n\n\tfpu__activate_fpstate_write(fpu);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES)) {\n\t\tif (kbuf)\n\t\t\tret = copy_kernel_to_xstate(xsave, kbuf);\n\t\telse\n\t\t\tret = copy_user_to_xstate(xsave, ubuf);\n\t} else {\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);\n\n\t\t/* xcomp_bv must be 0 when using uncompacted format */\n\t\tif (!ret && xsave->header.xcomp_bv)\n\t\t\tret = -EINVAL;\n\t}\n\n\t/*\n\t * In case of failure, mark all states as init:\n\t */\n\tif (ret)\n\t\tfpstate_init(&fpu->state);\n\n\t/*\n\t * mxcsr reserved bits must be masked to zero for security reasons.\n\t */\n\txsave->i387.mxcsr &= mxcsr_feature_mask;\n\txsave->header.xfeatures &= xfeatures_mask;\n\t/*\n\t * These bits must be zero.\n\t */\n\tmemset(&xsave->header.reserved, 0, 48);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,10 @@\n \t\t\tret = copy_user_to_xstate(xsave, ubuf);\n \t} else {\n \t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);\n+\n+\t\t/* xcomp_bv must be 0 when using uncompacted format */\n+\t\tif (!ret && xsave->header.xcomp_bv)\n+\t\t\tret = -EINVAL;\n \t}\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t/* xcomp_bv must be 0 when using uncompacted format */",
                "\t\tif (!ret && xsave->header.xcomp_bv)",
                "\t\t\tret = -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The x86/fpu (Floating Point Unit) subsystem in the Linux kernel before 4.13.5, when a processor supports the xsave feature but not the xsaves feature, does not correctly handle attempts to set reserved bits in the xstate header via the ptrace() or rt_sigreturn() system call, allowing local users to read the FPU registers of other processes on the system, related to arch/x86/kernel/fpu/regset.c and arch/x86/kernel/fpu/signal.c.",
        "id": 1306
    },
    {
        "cve_id": "CVE-2018-20449",
        "code_before_change": "char *pointer(const char *fmt, char *buf, char *end, void *ptr,\n\t      struct printf_spec spec)\n{\n\tconst int default_width = 2 * sizeof(void *);\n\n\tif (!ptr && *fmt != 'K') {\n\t\t/*\n\t\t * Print (null) with the same width as a pointer so it makes\n\t\t * tabular output look nice.\n\t\t */\n\t\tif (spec.field_width == -1)\n\t\t\tspec.field_width = default_width;\n\t\treturn string(buf, end, \"(null)\", spec);\n\t}\n\n\tswitch (*fmt) {\n\tcase 'F':\n\tcase 'f':\n\t\tptr = dereference_function_descriptor(ptr);\n\t\t/* Fallthrough */\n\tcase 'S':\n\tcase 's':\n\tcase 'B':\n\t\treturn symbol_string(buf, end, ptr, spec, fmt);\n\tcase 'R':\n\tcase 'r':\n\t\treturn resource_string(buf, end, ptr, spec, fmt);\n\tcase 'h':\n\t\treturn hex_string(buf, end, ptr, spec, fmt);\n\tcase 'b':\n\t\tswitch (fmt[1]) {\n\t\tcase 'l':\n\t\t\treturn bitmap_list_string(buf, end, ptr, spec, fmt);\n\t\tdefault:\n\t\t\treturn bitmap_string(buf, end, ptr, spec, fmt);\n\t\t}\n\tcase 'M':\t\t\t/* Colon separated: 00:01:02:03:04:05 */\n\tcase 'm':\t\t\t/* Contiguous: 000102030405 */\n\t\t\t\t\t/* [mM]F (FDDI) */\n\t\t\t\t\t/* [mM]R (Reverse order; Bluetooth) */\n\t\treturn mac_address_string(buf, end, ptr, spec, fmt);\n\tcase 'I':\t\t\t/* Formatted IP supported\n\t\t\t\t\t * 4:\t1.2.3.4\n\t\t\t\t\t * 6:\t0001:0203:...:0708\n\t\t\t\t\t * 6c:\t1::708 or 1::1.2.3.4\n\t\t\t\t\t */\n\tcase 'i':\t\t\t/* Contiguous:\n\t\t\t\t\t * 4:\t001.002.003.004\n\t\t\t\t\t * 6:   000102...0f\n\t\t\t\t\t */\n\t\tswitch (fmt[1]) {\n\t\tcase '6':\n\t\t\treturn ip6_addr_string(buf, end, ptr, spec, fmt);\n\t\tcase '4':\n\t\t\treturn ip4_addr_string(buf, end, ptr, spec, fmt);\n\t\tcase 'S': {\n\t\t\tconst union {\n\t\t\t\tstruct sockaddr\t\traw;\n\t\t\t\tstruct sockaddr_in\tv4;\n\t\t\t\tstruct sockaddr_in6\tv6;\n\t\t\t} *sa = ptr;\n\n\t\t\tswitch (sa->raw.sa_family) {\n\t\t\tcase AF_INET:\n\t\t\t\treturn ip4_addr_string_sa(buf, end, &sa->v4, spec, fmt);\n\t\t\tcase AF_INET6:\n\t\t\t\treturn ip6_addr_string_sa(buf, end, &sa->v6, spec, fmt);\n\t\t\tdefault:\n\t\t\t\treturn string(buf, end, \"(invalid address)\", spec);\n\t\t\t}}\n\t\t}\n\t\tbreak;\n\tcase 'E':\n\t\treturn escaped_string(buf, end, ptr, spec, fmt);\n\tcase 'U':\n\t\treturn uuid_string(buf, end, ptr, spec, fmt);\n\tcase 'V':\n\t\t{\n\t\t\tva_list va;\n\n\t\t\tva_copy(va, *((struct va_format *)ptr)->va);\n\t\t\tbuf += vsnprintf(buf, end > buf ? end - buf : 0,\n\t\t\t\t\t ((struct va_format *)ptr)->fmt, va);\n\t\t\tva_end(va);\n\t\t\treturn buf;\n\t\t}\n\tcase 'K':\n\t\treturn restricted_pointer(buf, end, ptr, spec);\n\tcase 'N':\n\t\treturn netdev_bits(buf, end, ptr, fmt);\n\tcase 'a':\n\t\treturn address_val(buf, end, ptr, fmt);\n\tcase 'd':\n\t\treturn dentry_name(buf, end, ptr, spec, fmt);\n\tcase 'C':\n\t\treturn clock(buf, end, ptr, spec, fmt);\n\tcase 'D':\n\t\treturn dentry_name(buf, end,\n\t\t\t\t   ((const struct file *)ptr)->f_path.dentry,\n\t\t\t\t   spec, fmt);\n#ifdef CONFIG_BLOCK\n\tcase 'g':\n\t\treturn bdev_name(buf, end, ptr, spec, fmt);\n#endif\n\n\tcase 'G':\n\t\treturn flags_string(buf, end, ptr, fmt);\n\tcase 'O':\n\t\tswitch (fmt[1]) {\n\t\tcase 'F':\n\t\t\treturn device_node_string(buf, end, ptr, spec, fmt + 1);\n\t\t}\n\t}\n\tspec.flags |= SMALL;\n\tif (spec.field_width == -1) {\n\t\tspec.field_width = default_width;\n\t\tspec.flags |= ZEROPAD;\n\t}\n\tspec.base = 16;\n\n\treturn number(buf, end, (unsigned long) ptr, spec);\n}",
        "code_after_change": "char *pointer(const char *fmt, char *buf, char *end, void *ptr,\n\t      struct printf_spec spec)\n{\n\tconst int default_width = 2 * sizeof(void *);\n\n\tif (!ptr && *fmt != 'K') {\n\t\t/*\n\t\t * Print (null) with the same width as a pointer so it makes\n\t\t * tabular output look nice.\n\t\t */\n\t\tif (spec.field_width == -1)\n\t\t\tspec.field_width = default_width;\n\t\treturn string(buf, end, \"(null)\", spec);\n\t}\n\n\tswitch (*fmt) {\n\tcase 'F':\n\tcase 'f':\n\t\tptr = dereference_function_descriptor(ptr);\n\t\t/* Fallthrough */\n\tcase 'S':\n\tcase 's':\n\tcase 'B':\n\t\treturn symbol_string(buf, end, ptr, spec, fmt);\n\tcase 'R':\n\tcase 'r':\n\t\treturn resource_string(buf, end, ptr, spec, fmt);\n\tcase 'h':\n\t\treturn hex_string(buf, end, ptr, spec, fmt);\n\tcase 'b':\n\t\tswitch (fmt[1]) {\n\t\tcase 'l':\n\t\t\treturn bitmap_list_string(buf, end, ptr, spec, fmt);\n\t\tdefault:\n\t\t\treturn bitmap_string(buf, end, ptr, spec, fmt);\n\t\t}\n\tcase 'M':\t\t\t/* Colon separated: 00:01:02:03:04:05 */\n\tcase 'm':\t\t\t/* Contiguous: 000102030405 */\n\t\t\t\t\t/* [mM]F (FDDI) */\n\t\t\t\t\t/* [mM]R (Reverse order; Bluetooth) */\n\t\treturn mac_address_string(buf, end, ptr, spec, fmt);\n\tcase 'I':\t\t\t/* Formatted IP supported\n\t\t\t\t\t * 4:\t1.2.3.4\n\t\t\t\t\t * 6:\t0001:0203:...:0708\n\t\t\t\t\t * 6c:\t1::708 or 1::1.2.3.4\n\t\t\t\t\t */\n\tcase 'i':\t\t\t/* Contiguous:\n\t\t\t\t\t * 4:\t001.002.003.004\n\t\t\t\t\t * 6:   000102...0f\n\t\t\t\t\t */\n\t\tswitch (fmt[1]) {\n\t\tcase '6':\n\t\t\treturn ip6_addr_string(buf, end, ptr, spec, fmt);\n\t\tcase '4':\n\t\t\treturn ip4_addr_string(buf, end, ptr, spec, fmt);\n\t\tcase 'S': {\n\t\t\tconst union {\n\t\t\t\tstruct sockaddr\t\traw;\n\t\t\t\tstruct sockaddr_in\tv4;\n\t\t\t\tstruct sockaddr_in6\tv6;\n\t\t\t} *sa = ptr;\n\n\t\t\tswitch (sa->raw.sa_family) {\n\t\t\tcase AF_INET:\n\t\t\t\treturn ip4_addr_string_sa(buf, end, &sa->v4, spec, fmt);\n\t\t\tcase AF_INET6:\n\t\t\t\treturn ip6_addr_string_sa(buf, end, &sa->v6, spec, fmt);\n\t\t\tdefault:\n\t\t\t\treturn string(buf, end, \"(invalid address)\", spec);\n\t\t\t}}\n\t\t}\n\t\tbreak;\n\tcase 'E':\n\t\treturn escaped_string(buf, end, ptr, spec, fmt);\n\tcase 'U':\n\t\treturn uuid_string(buf, end, ptr, spec, fmt);\n\tcase 'V':\n\t\t{\n\t\t\tva_list va;\n\n\t\t\tva_copy(va, *((struct va_format *)ptr)->va);\n\t\t\tbuf += vsnprintf(buf, end > buf ? end - buf : 0,\n\t\t\t\t\t ((struct va_format *)ptr)->fmt, va);\n\t\t\tva_end(va);\n\t\t\treturn buf;\n\t\t}\n\tcase 'K':\n\t\treturn restricted_pointer(buf, end, ptr, spec);\n\tcase 'N':\n\t\treturn netdev_bits(buf, end, ptr, fmt);\n\tcase 'a':\n\t\treturn address_val(buf, end, ptr, fmt);\n\tcase 'd':\n\t\treturn dentry_name(buf, end, ptr, spec, fmt);\n\tcase 'C':\n\t\treturn clock(buf, end, ptr, spec, fmt);\n\tcase 'D':\n\t\treturn dentry_name(buf, end,\n\t\t\t\t   ((const struct file *)ptr)->f_path.dentry,\n\t\t\t\t   spec, fmt);\n#ifdef CONFIG_BLOCK\n\tcase 'g':\n\t\treturn bdev_name(buf, end, ptr, spec, fmt);\n#endif\n\n\tcase 'G':\n\t\treturn flags_string(buf, end, ptr, fmt);\n\tcase 'O':\n\t\tswitch (fmt[1]) {\n\t\tcase 'F':\n\t\t\treturn device_node_string(buf, end, ptr, spec, fmt + 1);\n\t\t}\n\t}\n\n\t/* default is to _not_ leak addresses, hash before printing */\n\treturn ptr_to_id(buf, end, ptr, spec);\n}",
        "patch": "--- code before\n+++ code after\n@@ -111,12 +111,7 @@\n \t\t\treturn device_node_string(buf, end, ptr, spec, fmt + 1);\n \t\t}\n \t}\n-\tspec.flags |= SMALL;\n-\tif (spec.field_width == -1) {\n-\t\tspec.field_width = default_width;\n-\t\tspec.flags |= ZEROPAD;\n-\t}\n-\tspec.base = 16;\n \n-\treturn number(buf, end, (unsigned long) ptr, spec);\n+\t/* default is to _not_ leak addresses, hash before printing */\n+\treturn ptr_to_id(buf, end, ptr, spec);\n }",
        "function_modified_lines": {
            "added": [
                "\t/* default is to _not_ leak addresses, hash before printing */",
                "\treturn ptr_to_id(buf, end, ptr, spec);"
            ],
            "deleted": [
                "\tspec.flags |= SMALL;",
                "\tif (spec.field_width == -1) {",
                "\t\tspec.field_width = default_width;",
                "\t\tspec.flags |= ZEROPAD;",
                "\t}",
                "\tspec.base = 16;",
                "\treturn number(buf, end, (unsigned long) ptr, spec);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The hidma_chan_stats function in drivers/dma/qcom/hidma_dbg.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \"callback=\" lines in a debugfs file.",
        "id": 1758
    },
    {
        "cve_id": "CVE-2015-8569",
        "code_before_change": "static int pptp_bind(struct socket *sock, struct sockaddr *uservaddr,\n\tint sockaddr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppox *sp = (struct sockaddr_pppox *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct pptp_opt *opt = &po->proto.pptp;\n\tint error = 0;\n\n\tlock_sock(sk);\n\n\topt->src_addr = sp->sa_addr.pptp;\n\tif (add_chan(po))\n\t\terror = -EBUSY;\n\n\trelease_sock(sk);\n\treturn error;\n}",
        "code_after_change": "static int pptp_bind(struct socket *sock, struct sockaddr *uservaddr,\n\tint sockaddr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppox *sp = (struct sockaddr_pppox *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct pptp_opt *opt = &po->proto.pptp;\n\tint error = 0;\n\n\tif (sockaddr_len < sizeof(struct sockaddr_pppox))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\topt->src_addr = sp->sa_addr.pptp;\n\tif (add_chan(po))\n\t\terror = -EBUSY;\n\n\trelease_sock(sk);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,9 @@\n \tstruct pppox_sock *po = pppox_sk(sk);\n \tstruct pptp_opt *opt = &po->proto.pptp;\n \tint error = 0;\n+\n+\tif (sockaddr_len < sizeof(struct sockaddr_pppox))\n+\t\treturn -EINVAL;\n \n \tlock_sock(sk);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (sockaddr_len < sizeof(struct sockaddr_pppox))",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The (1) pptp_bind and (2) pptp_connect functions in drivers/net/ppp/pptp.c in the Linux kernel through 4.3.3 do not verify an address length, which allows local users to obtain sensitive information from kernel memory and bypass the KASLR protection mechanism via a crafted application.",
        "id": 832
    },
    {
        "cve_id": "CVE-2019-18660",
        "code_before_change": "void setup_count_cache_flush(void)\n{\n\tbool enable = true;\n\n\tif (no_spectrev2 || cpu_mitigations_off()) {\n\t\tif (security_ftr_enabled(SEC_FTR_BCCTRL_SERIALISED) ||\n\t\t    security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED))\n\t\t\tpr_warn(\"Spectre v2 mitigations not under software control, can't disable\\n\");\n\n\t\tenable = false;\n\t}\n\n\ttoggle_count_cache_flush(enable);\n}",
        "code_after_change": "void setup_count_cache_flush(void)\n{\n\tbool enable = true;\n\n\tif (no_spectrev2 || cpu_mitigations_off()) {\n\t\tif (security_ftr_enabled(SEC_FTR_BCCTRL_SERIALISED) ||\n\t\t    security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED))\n\t\t\tpr_warn(\"Spectre v2 mitigations not fully under software control, can't disable\\n\");\n\n\t\tenable = false;\n\t}\n\n\t/*\n\t * There's no firmware feature flag/hypervisor bit to tell us we need to\n\t * flush the link stack on context switch. So we set it here if we see\n\t * either of the Spectre v2 mitigations that aim to protect userspace.\n\t */\n\tif (security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED) ||\n\t    security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE))\n\t\tsecurity_ftr_set(SEC_FTR_FLUSH_LINK_STACK);\n\n\ttoggle_count_cache_flush(enable);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,10 +5,19 @@\n \tif (no_spectrev2 || cpu_mitigations_off()) {\n \t\tif (security_ftr_enabled(SEC_FTR_BCCTRL_SERIALISED) ||\n \t\t    security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED))\n-\t\t\tpr_warn(\"Spectre v2 mitigations not under software control, can't disable\\n\");\n+\t\t\tpr_warn(\"Spectre v2 mitigations not fully under software control, can't disable\\n\");\n \n \t\tenable = false;\n \t}\n \n+\t/*\n+\t * There's no firmware feature flag/hypervisor bit to tell us we need to\n+\t * flush the link stack on context switch. So we set it here if we see\n+\t * either of the Spectre v2 mitigations that aim to protect userspace.\n+\t */\n+\tif (security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED) ||\n+\t    security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE))\n+\t\tsecurity_ftr_set(SEC_FTR_FLUSH_LINK_STACK);\n+\n \ttoggle_count_cache_flush(enable);\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\tpr_warn(\"Spectre v2 mitigations not fully under software control, can't disable\\n\");",
                "\t/*",
                "\t * There's no firmware feature flag/hypervisor bit to tell us we need to",
                "\t * flush the link stack on context switch. So we set it here if we see",
                "\t * either of the Spectre v2 mitigations that aim to protect userspace.",
                "\t */",
                "\tif (security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED) ||",
                "\t    security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE))",
                "\t\tsecurity_ftr_set(SEC_FTR_FLUSH_LINK_STACK);",
                ""
            ],
            "deleted": [
                "\t\t\tpr_warn(\"Spectre v2 mitigations not under software control, can't disable\\n\");"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The Linux kernel before 5.4.1 on powerpc allows Information Exposure because the Spectre-RSB mitigation is not in place for all applicable CPUs, aka CID-39e72bf96f58. This is related to arch/powerpc/kernel/entry_64.S and arch/powerpc/kernel/security.c.",
        "id": 2088
    },
    {
        "cve_id": "CVE-2017-5550",
        "code_before_change": "void iov_iter_pipe(struct iov_iter *i, int direction,\n\t\t\tstruct pipe_inode_info *pipe,\n\t\t\tsize_t count)\n{\n\tBUG_ON(direction != ITER_PIPE);\n\ti->type = direction;\n\ti->pipe = pipe;\n\ti->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\ti->iov_offset = 0;\n\ti->count = count;\n}",
        "code_after_change": "void iov_iter_pipe(struct iov_iter *i, int direction,\n\t\t\tstruct pipe_inode_info *pipe,\n\t\t\tsize_t count)\n{\n\tBUG_ON(direction != ITER_PIPE);\n\tWARN_ON(pipe->nrbufs == pipe->buffers);\n\ti->type = direction;\n\ti->pipe = pipe;\n\ti->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\ti->iov_offset = 0;\n\ti->count = count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \t\t\tsize_t count)\n {\n \tBUG_ON(direction != ITER_PIPE);\n+\tWARN_ON(pipe->nrbufs == pipe->buffers);\n \ti->type = direction;\n \ti->pipe = pipe;\n \ti->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);",
        "function_modified_lines": {
            "added": [
                "\tWARN_ON(pipe->nrbufs == pipe->buffers);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Off-by-one error in the pipe_advance function in lib/iov_iter.c in the Linux kernel before 4.9.5 allows local users to obtain sensitive information from uninitialized heap-memory locations in opportunistic circumstances by reading from a pipe after an incorrect buffer-release decision.",
        "id": 1468
    },
    {
        "cve_id": "CVE-2013-3076",
        "code_before_change": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tunsigned long iovlen;\n\tstruct iovec *iov;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n\t     iovlen--, iov++) {\n\t\tunsigned long seglen = iov->iov_len;\n\t\tchar __user *from = iov->iov_base;\n\n\t\twhile (seglen) {\n\t\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t\t       struct skcipher_sg_list, list);\n\t\t\tsg = sgl->sg;\n\n\t\t\twhile (!sg->length)\n\t\t\t\tsg++;\n\n\t\t\tused = ctx->used;\n\t\t\tif (!used) {\n\t\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto unlock;\n\t\t\t}\n\n\t\t\tused = min_t(unsigned long, used, seglen);\n\n\t\t\tused = af_alg_make_sg(&ctx->rsgl, from, used, 1);\n\t\t\terr = used;\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\n\t\t\tif (ctx->more || used < ctx->used)\n\t\t\t\tused -= used % bs;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (!used)\n\t\t\t\tgoto free;\n\n\t\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t\t     ctx->iv);\n\n\t\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\n\t\t\tcopied += used;\n\t\t\tfrom += used;\n\t\t\tseglen -= used;\n\t\t\tskcipher_pull_sgl(sk, used);\n\t\t}\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "code_after_change": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tunsigned long iovlen;\n\tstruct iovec *iov;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\tmsg->msg_namelen = 0;\n\tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n\t     iovlen--, iov++) {\n\t\tunsigned long seglen = iov->iov_len;\n\t\tchar __user *from = iov->iov_base;\n\n\t\twhile (seglen) {\n\t\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t\t       struct skcipher_sg_list, list);\n\t\t\tsg = sgl->sg;\n\n\t\t\twhile (!sg->length)\n\t\t\t\tsg++;\n\n\t\t\tused = ctx->used;\n\t\t\tif (!used) {\n\t\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto unlock;\n\t\t\t}\n\n\t\t\tused = min_t(unsigned long, used, seglen);\n\n\t\t\tused = af_alg_make_sg(&ctx->rsgl, from, used, 1);\n\t\t\terr = used;\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\n\t\t\tif (ctx->more || used < ctx->used)\n\t\t\t\tused -= used % bs;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (!used)\n\t\t\t\tgoto free;\n\n\t\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t\t     ctx->iv);\n\n\t\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\n\t\t\tcopied += used;\n\t\t\tfrom += used;\n\t\t\tseglen -= used;\n\t\t\tskcipher_pull_sgl(sk, used);\n\t\t}\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,7 @@\n \tlong copied = 0;\n \n \tlock_sock(sk);\n+\tmsg->msg_namelen = 0;\n \tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n \t     iovlen--, iov++) {\n \t\tunsigned long seglen = iov->iov_len;",
        "function_modified_lines": {
            "added": [
                "\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The crypto API in the Linux kernel through 3.9-rc8 does not initialize certain length variables, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call, related to the hash_recvmsg function in crypto/algif_hash.c and the skcipher_recvmsg function in crypto/algif_skcipher.c.",
        "id": 262
    },
    {
        "cve_id": "CVE-2015-8950",
        "code_before_change": "static void *__dma_alloc_coherent(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *dma_handle, gfp_t flags,\n\t\t\t\t  struct dma_attrs *attrs)\n{\n\tif (dev == NULL) {\n\t\tWARN_ONCE(1, \"Use an actual device structure for DMA allocation\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (IS_ENABLED(CONFIG_ZONE_DMA) &&\n\t    dev->coherent_dma_mask <= DMA_BIT_MASK(32))\n\t\tflags |= GFP_DMA;\n\tif (IS_ENABLED(CONFIG_DMA_CMA) && (flags & __GFP_WAIT)) {\n\t\tstruct page *page;\n\t\tvoid *addr;\n\n\t\tsize = PAGE_ALIGN(size);\n\t\tpage = dma_alloc_from_contiguous(dev, size >> PAGE_SHIFT,\n\t\t\t\t\t\t\tget_order(size));\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n\t\taddr = page_address(page);\n\t\tif (flags & __GFP_ZERO)\n\t\t\tmemset(addr, 0, size);\n\t\treturn addr;\n\t} else {\n\t\treturn swiotlb_alloc_coherent(dev, size, dma_handle, flags);\n\t}\n}",
        "code_after_change": "static void *__dma_alloc_coherent(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *dma_handle, gfp_t flags,\n\t\t\t\t  struct dma_attrs *attrs)\n{\n\tif (dev == NULL) {\n\t\tWARN_ONCE(1, \"Use an actual device structure for DMA allocation\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (IS_ENABLED(CONFIG_ZONE_DMA) &&\n\t    dev->coherent_dma_mask <= DMA_BIT_MASK(32))\n\t\tflags |= GFP_DMA;\n\tif (IS_ENABLED(CONFIG_DMA_CMA) && (flags & __GFP_WAIT)) {\n\t\tstruct page *page;\n\t\tvoid *addr;\n\n\t\tsize = PAGE_ALIGN(size);\n\t\tpage = dma_alloc_from_contiguous(dev, size >> PAGE_SHIFT,\n\t\t\t\t\t\t\tget_order(size));\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n\t\taddr = page_address(page);\n\t\tmemset(addr, 0, size);\n\t\treturn addr;\n\t} else {\n\t\treturn swiotlb_alloc_coherent(dev, size, dma_handle, flags);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,8 +22,7 @@\n \n \t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n \t\taddr = page_address(page);\n-\t\tif (flags & __GFP_ZERO)\n-\t\t\tmemset(addr, 0, size);\n+\t\tmemset(addr, 0, size);\n \t\treturn addr;\n \t} else {\n \t\treturn swiotlb_alloc_coherent(dev, size, dma_handle, flags);",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(addr, 0, size);"
            ],
            "deleted": [
                "\t\tif (flags & __GFP_ZERO)",
                "\t\t\tmemset(addr, 0, size);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "arch/arm64/mm/dma-mapping.c in the Linux kernel before 4.0.3, as used in the ION subsystem in Android and other products, does not initialize certain data structures, which allows local users to obtain sensitive information from kernel memory by triggering a dma_mmap call.",
        "id": 864
    },
    {
        "cve_id": "CVE-2014-3917",
        "code_before_change": "static enum audit_state audit_filter_syscall(struct task_struct *tsk,\n\t\t\t\t\t     struct audit_context *ctx,\n\t\t\t\t\t     struct list_head *list)\n{\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tif (audit_pid && tsk->tgid == audit_pid)\n\t\treturn AUDIT_DISABLED;\n\n\trcu_read_lock();\n\tif (!list_empty(list)) {\n\t\tint word = AUDIT_WORD(ctx->major);\n\t\tint bit  = AUDIT_BIT(ctx->major);\n\n\t\tlist_for_each_entry_rcu(e, list, list) {\n\t\t\tif ((e->rule.mask[word] & bit) == bit &&\n\t\t\t    audit_filter_rules(tsk, &e->rule, ctx, NULL,\n\t\t\t\t\t       &state, false)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tctx->current_state = state;\n\t\t\t\treturn state;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn AUDIT_BUILD_CONTEXT;\n}",
        "code_after_change": "static enum audit_state audit_filter_syscall(struct task_struct *tsk,\n\t\t\t\t\t     struct audit_context *ctx,\n\t\t\t\t\t     struct list_head *list)\n{\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tif (audit_pid && tsk->tgid == audit_pid)\n\t\treturn AUDIT_DISABLED;\n\n\trcu_read_lock();\n\tif (!list_empty(list)) {\n\t\tlist_for_each_entry_rcu(e, list, list) {\n\t\t\tif (audit_in_mask(&e->rule, ctx->major) &&\n\t\t\t    audit_filter_rules(tsk, &e->rule, ctx, NULL,\n\t\t\t\t\t       &state, false)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tctx->current_state = state;\n\t\t\t\treturn state;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn AUDIT_BUILD_CONTEXT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,11 +10,8 @@\n \n \trcu_read_lock();\n \tif (!list_empty(list)) {\n-\t\tint word = AUDIT_WORD(ctx->major);\n-\t\tint bit  = AUDIT_BIT(ctx->major);\n-\n \t\tlist_for_each_entry_rcu(e, list, list) {\n-\t\t\tif ((e->rule.mask[word] & bit) == bit &&\n+\t\t\tif (audit_in_mask(&e->rule, ctx->major) &&\n \t\t\t    audit_filter_rules(tsk, &e->rule, ctx, NULL,\n \t\t\t\t\t       &state, false)) {\n \t\t\t\trcu_read_unlock();",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (audit_in_mask(&e->rule, ctx->major) &&"
            ],
            "deleted": [
                "\t\tint word = AUDIT_WORD(ctx->major);",
                "\t\tint bit  = AUDIT_BIT(ctx->major);",
                "",
                "\t\t\tif ((e->rule.mask[word] & bit) == bit &&"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "kernel/auditsc.c in the Linux kernel through 3.14.5, when CONFIG_AUDITSYSCALL is enabled with certain syscall rules, allows local users to obtain potentially sensitive single-bit values from kernel memory or cause a denial of service (OOPS) via a large value of a syscall number.",
        "id": 548
    },
    {
        "cve_id": "CVE-2013-4299",
        "code_before_change": "static int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tuint32_t stride;\n\tchunk_t next_free;\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tstride = (ps->exceptions_per_area + 1);\n\tnext_free = ++ps->next_free;\n\tif (sector_div(next_free, stride) == 1)\n\t\tps->next_free++;\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}",
        "code_after_change": "static int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tps->next_free++;\n\tskip_metadata(ps);\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,8 +2,6 @@\n \t\t\t\t\tstruct dm_exception *e)\n {\n \tstruct pstore *ps = get_info(store);\n-\tuint32_t stride;\n-\tchunk_t next_free;\n \tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n \n \t/* Is there enough room ? */\n@@ -16,10 +14,8 @@\n \t * Move onto the next free pending, making sure to take\n \t * into account the location of the metadata chunks.\n \t */\n-\tstride = (ps->exceptions_per_area + 1);\n-\tnext_free = ++ps->next_free;\n-\tif (sector_div(next_free, stride) == 1)\n-\t\tps->next_free++;\n+\tps->next_free++;\n+\tskip_metadata(ps);\n \n \tatomic_inc(&ps->pending_count);\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tps->next_free++;",
                "\tskip_metadata(ps);"
            ],
            "deleted": [
                "\tuint32_t stride;",
                "\tchunk_t next_free;",
                "\tstride = (ps->exceptions_per_area + 1);",
                "\tnext_free = ++ps->next_free;",
                "\tif (sector_div(next_free, stride) == 1)",
                "\t\tps->next_free++;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-200"
        ],
        "cve_description": "Interpretation conflict in drivers/md/dm-snap-persistent.c in the Linux kernel through 3.11.6 allows remote authenticated users to obtain sensitive information or modify data via a crafted mapping to a snapshot block device.",
        "id": 293
    },
    {
        "cve_id": "CVE-2012-4530",
        "code_before_change": "int search_binary_handler(struct linux_binprm *bprm)\n{\n\tunsigned int depth = bprm->recursion_depth;\n\tint try,retval;\n\tstruct linux_binfmt *fmt;\n\tpid_t old_pid, old_vpid;\n\n\tretval = security_bprm_check(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = audit_bprm(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\t/* Need to fetch pid before load_binary changes it */\n\told_pid = current->pid;\n\trcu_read_lock();\n\told_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));\n\trcu_read_unlock();\n\n\tretval = -ENOENT;\n\tfor (try=0; try<2; try++) {\n\t\tread_lock(&binfmt_lock);\n\t\tlist_for_each_entry(fmt, &formats, lh) {\n\t\t\tint (*fn)(struct linux_binprm *) = fmt->load_binary;\n\t\t\tif (!fn)\n\t\t\t\tcontinue;\n\t\t\tif (!try_module_get(fmt->module))\n\t\t\t\tcontinue;\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\tretval = fn(bprm);\n\t\t\t/*\n\t\t\t * Restore the depth counter to its starting value\n\t\t\t * in this call, so we don't have to rely on every\n\t\t\t * load_binary function to restore it on return.\n\t\t\t */\n\t\t\tbprm->recursion_depth = depth;\n\t\t\tif (retval >= 0) {\n\t\t\t\tif (depth == 0) {\n\t\t\t\t\ttrace_sched_process_exec(current, old_pid, bprm);\n\t\t\t\t\tptrace_event(PTRACE_EVENT_EXEC, old_vpid);\n\t\t\t\t}\n\t\t\t\tput_binfmt(fmt);\n\t\t\t\tallow_write_access(bprm->file);\n\t\t\t\tif (bprm->file)\n\t\t\t\t\tfput(bprm->file);\n\t\t\t\tbprm->file = NULL;\n\t\t\t\tcurrent->did_exec = 1;\n\t\t\t\tproc_exec_connector(current);\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t\tread_lock(&binfmt_lock);\n\t\t\tput_binfmt(fmt);\n\t\t\tif (retval != -ENOEXEC || bprm->mm == NULL)\n\t\t\t\tbreak;\n\t\t\tif (!bprm->file) {\n\t\t\t\tread_unlock(&binfmt_lock);\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t}\n\t\tread_unlock(&binfmt_lock);\n#ifdef CONFIG_MODULES\n\t\tif (retval != -ENOEXEC || bprm->mm == NULL) {\n\t\t\tbreak;\n\t\t} else {\n#define printable(c) (((c)=='\\t') || ((c)=='\\n') || (0x20<=(c) && (c)<=0x7e))\n\t\t\tif (printable(bprm->buf[0]) &&\n\t\t\t    printable(bprm->buf[1]) &&\n\t\t\t    printable(bprm->buf[2]) &&\n\t\t\t    printable(bprm->buf[3]))\n\t\t\t\tbreak; /* -ENOEXEC */\n\t\t\tif (try)\n\t\t\t\tbreak; /* -ENOEXEC */\n\t\t\trequest_module(\"binfmt-%04x\", *(unsigned short *)(&bprm->buf[2]));\n\t\t}\n#else\n\t\tbreak;\n#endif\n\t}\n\treturn retval;\n}",
        "code_after_change": "int search_binary_handler(struct linux_binprm *bprm)\n{\n\tunsigned int depth = bprm->recursion_depth;\n\tint try,retval;\n\tstruct linux_binfmt *fmt;\n\tpid_t old_pid, old_vpid;\n\n\t/* This allows 4 levels of binfmt rewrites before failing hard. */\n\tif (depth > 5)\n\t\treturn -ELOOP;\n\n\tretval = security_bprm_check(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = audit_bprm(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\t/* Need to fetch pid before load_binary changes it */\n\told_pid = current->pid;\n\trcu_read_lock();\n\told_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));\n\trcu_read_unlock();\n\n\tretval = -ENOENT;\n\tfor (try=0; try<2; try++) {\n\t\tread_lock(&binfmt_lock);\n\t\tlist_for_each_entry(fmt, &formats, lh) {\n\t\t\tint (*fn)(struct linux_binprm *) = fmt->load_binary;\n\t\t\tif (!fn)\n\t\t\t\tcontinue;\n\t\t\tif (!try_module_get(fmt->module))\n\t\t\t\tcontinue;\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\tbprm->recursion_depth = depth + 1;\n\t\t\tretval = fn(bprm);\n\t\t\tbprm->recursion_depth = depth;\n\t\t\tif (retval >= 0) {\n\t\t\t\tif (depth == 0) {\n\t\t\t\t\ttrace_sched_process_exec(current, old_pid, bprm);\n\t\t\t\t\tptrace_event(PTRACE_EVENT_EXEC, old_vpid);\n\t\t\t\t}\n\t\t\t\tput_binfmt(fmt);\n\t\t\t\tallow_write_access(bprm->file);\n\t\t\t\tif (bprm->file)\n\t\t\t\t\tfput(bprm->file);\n\t\t\t\tbprm->file = NULL;\n\t\t\t\tcurrent->did_exec = 1;\n\t\t\t\tproc_exec_connector(current);\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t\tread_lock(&binfmt_lock);\n\t\t\tput_binfmt(fmt);\n\t\t\tif (retval != -ENOEXEC || bprm->mm == NULL)\n\t\t\t\tbreak;\n\t\t\tif (!bprm->file) {\n\t\t\t\tread_unlock(&binfmt_lock);\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t}\n\t\tread_unlock(&binfmt_lock);\n#ifdef CONFIG_MODULES\n\t\tif (retval != -ENOEXEC || bprm->mm == NULL) {\n\t\t\tbreak;\n\t\t} else {\n#define printable(c) (((c)=='\\t') || ((c)=='\\n') || (0x20<=(c) && (c)<=0x7e))\n\t\t\tif (printable(bprm->buf[0]) &&\n\t\t\t    printable(bprm->buf[1]) &&\n\t\t\t    printable(bprm->buf[2]) &&\n\t\t\t    printable(bprm->buf[3]))\n\t\t\t\tbreak; /* -ENOEXEC */\n\t\t\tif (try)\n\t\t\t\tbreak; /* -ENOEXEC */\n\t\t\trequest_module(\"binfmt-%04x\", *(unsigned short *)(&bprm->buf[2]));\n\t\t}\n#else\n\t\tbreak;\n#endif\n\t}\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,10 @@\n \tint try,retval;\n \tstruct linux_binfmt *fmt;\n \tpid_t old_pid, old_vpid;\n+\n+\t/* This allows 4 levels of binfmt rewrites before failing hard. */\n+\tif (depth > 5)\n+\t\treturn -ELOOP;\n \n \tretval = security_bprm_check(bprm);\n \tif (retval)\n@@ -29,12 +33,8 @@\n \t\t\tif (!try_module_get(fmt->module))\n \t\t\t\tcontinue;\n \t\t\tread_unlock(&binfmt_lock);\n+\t\t\tbprm->recursion_depth = depth + 1;\n \t\t\tretval = fn(bprm);\n-\t\t\t/*\n-\t\t\t * Restore the depth counter to its starting value\n-\t\t\t * in this call, so we don't have to rely on every\n-\t\t\t * load_binary function to restore it on return.\n-\t\t\t */\n \t\t\tbprm->recursion_depth = depth;\n \t\t\tif (retval >= 0) {\n \t\t\t\tif (depth == 0) {",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* This allows 4 levels of binfmt rewrites before failing hard. */",
                "\tif (depth > 5)",
                "\t\treturn -ELOOP;",
                "\t\t\tbprm->recursion_depth = depth + 1;"
            ],
            "deleted": [
                "\t\t\t/*",
                "\t\t\t * Restore the depth counter to its starting value",
                "\t\t\t * in this call, so we don't have to rely on every",
                "\t\t\t * load_binary function to restore it on return.",
                "\t\t\t */"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The load_script function in fs/binfmt_script.c in the Linux kernel before 3.7.2 does not properly handle recursion, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 113
    },
    {
        "cve_id": "CVE-2018-20510",
        "code_before_change": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!buffer->allow_user_free) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\n\t\t\tif (buffer->transaction) {\n\t\t\t\tbuffer->transaction->buffer = NULL;\n\t\t\t\tbuffer->transaction = NULL;\n\t\t\t}\n\t\t\tif (buffer->async_transaction && buffer->target_node) {\n\t\t\t\tstruct binder_node *buf_node;\n\t\t\t\tstruct binder_work *w;\n\n\t\t\t\tbuf_node = buffer->target_node;\n\t\t\t\tbinder_node_inner_lock(buf_node);\n\t\t\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\t\t\tBUG_ON(buf_node->proc != proc);\n\t\t\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t\t\t&buf_node->async_todo);\n\t\t\t\tif (!w) {\n\t\t\t\t\tbuf_node->has_async_transaction = false;\n\t\t\t\t} else {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\tw, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t\tbinder_node_inner_unlock(buf_node);\n\t\t\t}\n\t\t\ttrace_binder_transaction_buffer_release(buffer);\n\t\t\tbinder_transaction_buffer_release(proc, buffer, NULL);\n\t\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %p\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!buffer->allow_user_free) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\n\t\t\tif (buffer->transaction) {\n\t\t\t\tbuffer->transaction->buffer = NULL;\n\t\t\t\tbuffer->transaction = NULL;\n\t\t\t}\n\t\t\tif (buffer->async_transaction && buffer->target_node) {\n\t\t\t\tstruct binder_node *buf_node;\n\t\t\t\tstruct binder_work *w;\n\n\t\t\t\tbuf_node = buffer->target_node;\n\t\t\t\tbinder_node_inner_lock(buf_node);\n\t\t\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\t\t\tBUG_ON(buf_node->proc != proc);\n\t\t\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t\t\t&buf_node->async_todo);\n\t\t\t\tif (!w) {\n\t\t\t\t\tbuf_node->has_async_transaction = false;\n\t\t\t\t} else {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\tw, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t\tbinder_node_inner_unlock(buf_node);\n\t\t\t}\n\t\t\ttrace_binder_transaction_buffer_release(buffer);\n\t\t\tbinder_transaction_buffer_release(proc, buffer, NULL);\n\t\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -422,7 +422,7 @@\n \t\t\t\t}\n \t\t\t}\n \t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n-\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %p\\n\",\n+\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n \t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n \t\t\t\t     death);\n \t\t\tif (death == NULL) {",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\","
            ],
            "deleted": [
                "\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %p\\n\","
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_transaction_ilocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \"*from *code *flags\" lines in a debugfs file.",
        "id": 1771
    },
    {
        "cve_id": "CVE-2018-15594",
        "code_before_change": "unsigned paravirt_patch_call(void *insnbuf,\n\t\t\t     const void *target, u16 tgt_clobbers,\n\t\t\t     unsigned long addr, u16 site_clobbers,\n\t\t\t     unsigned len)\n{\n\tstruct branch *b = insnbuf;\n\tunsigned long delta = (unsigned long)target - (addr+5);\n\n\tif (tgt_clobbers & ~site_clobbers)\n\t\treturn len;\t/* target would clobber too much for this site */\n\tif (len < 5)\n\t\treturn len;\t/* call too long for patch site */\n\n\tb->opcode = 0xe8; /* call */\n\tb->delta = delta;\n\tBUILD_BUG_ON(sizeof(*b) != 5);\n\n\treturn 5;\n}",
        "code_after_change": "unsigned paravirt_patch_call(void *insnbuf,\n\t\t\t     const void *target, u16 tgt_clobbers,\n\t\t\t     unsigned long addr, u16 site_clobbers,\n\t\t\t     unsigned len)\n{\n\tstruct branch *b = insnbuf;\n\tunsigned long delta = (unsigned long)target - (addr+5);\n\n\tif (len < 5) {\n#ifdef CONFIG_RETPOLINE\n\t\tWARN_ONCE(\"Failing to patch indirect CALL in %ps\\n\", (void *)addr);\n#endif\n\t\treturn len;\t/* call too long for patch site */\n\t}\n\n\tb->opcode = 0xe8; /* call */\n\tb->delta = delta;\n\tBUILD_BUG_ON(sizeof(*b) != 5);\n\n\treturn 5;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,10 +6,12 @@\n \tstruct branch *b = insnbuf;\n \tunsigned long delta = (unsigned long)target - (addr+5);\n \n-\tif (tgt_clobbers & ~site_clobbers)\n-\t\treturn len;\t/* target would clobber too much for this site */\n-\tif (len < 5)\n+\tif (len < 5) {\n+#ifdef CONFIG_RETPOLINE\n+\t\tWARN_ONCE(\"Failing to patch indirect CALL in %ps\\n\", (void *)addr);\n+#endif\n \t\treturn len;\t/* call too long for patch site */\n+\t}\n \n \tb->opcode = 0xe8; /* call */\n \tb->delta = delta;",
        "function_modified_lines": {
            "added": [
                "\tif (len < 5) {",
                "#ifdef CONFIG_RETPOLINE",
                "\t\tWARN_ONCE(\"Failing to patch indirect CALL in %ps\\n\", (void *)addr);",
                "#endif",
                "\t}"
            ],
            "deleted": [
                "\tif (tgt_clobbers & ~site_clobbers)",
                "\t\treturn len;\t/* target would clobber too much for this site */",
                "\tif (len < 5)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "arch/x86/kernel/paravirt.c in the Linux kernel before 4.18.1 mishandles certain indirect calls, which makes it easier for attackers to conduct Spectre-v2 attacks against paravirtual guests.",
        "id": 1709
    },
    {
        "cve_id": "CVE-2014-4027",
        "code_before_change": "static void rd_release_device_space(struct rd_dev *rd_dev)\n{\n\tu32 i, j, page_count = 0, sg_per_table;\n\tstruct rd_dev_sg_table *sg_table;\n\tstruct page *pg;\n\tstruct scatterlist *sg;\n\n\tif (!rd_dev->sg_table_array || !rd_dev->sg_table_count)\n\t\treturn;\n\n\tsg_table = rd_dev->sg_table_array;\n\n\tfor (i = 0; i < rd_dev->sg_table_count; i++) {\n\t\tsg = sg_table[i].sg_table;\n\t\tsg_per_table = sg_table[i].rd_sg_count;\n\n\t\tfor (j = 0; j < sg_per_table; j++) {\n\t\t\tpg = sg_page(&sg[j]);\n\t\t\tif (pg) {\n\t\t\t\t__free_page(pg);\n\t\t\t\tpage_count++;\n\t\t\t}\n\t\t}\n\n\t\tkfree(sg);\n\t}\n\n\tpr_debug(\"CORE_RD[%u] - Released device space for Ramdisk\"\n\t\t\" Device ID: %u, pages %u in %u tables total bytes %lu\\n\",\n\t\trd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,\n\t\trd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);\n\n\tkfree(sg_table);\n\trd_dev->sg_table_array = NULL;\n\trd_dev->sg_table_count = 0;\n}",
        "code_after_change": "static void rd_release_device_space(struct rd_dev *rd_dev)\n{\n\tu32 page_count;\n\n\tif (!rd_dev->sg_table_array || !rd_dev->sg_table_count)\n\t\treturn;\n\n\tpage_count = rd_release_sgl_table(rd_dev, rd_dev->sg_table_array,\n\t\t\t\t\t  rd_dev->sg_table_count);\n\n\tpr_debug(\"CORE_RD[%u] - Released device space for Ramdisk\"\n\t\t\" Device ID: %u, pages %u in %u tables total bytes %lu\\n\",\n\t\trd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,\n\t\trd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);\n\n\trd_dev->sg_table_array = NULL;\n\trd_dev->sg_table_count = 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,36 +1,18 @@\n static void rd_release_device_space(struct rd_dev *rd_dev)\n {\n-\tu32 i, j, page_count = 0, sg_per_table;\n-\tstruct rd_dev_sg_table *sg_table;\n-\tstruct page *pg;\n-\tstruct scatterlist *sg;\n+\tu32 page_count;\n \n \tif (!rd_dev->sg_table_array || !rd_dev->sg_table_count)\n \t\treturn;\n \n-\tsg_table = rd_dev->sg_table_array;\n-\n-\tfor (i = 0; i < rd_dev->sg_table_count; i++) {\n-\t\tsg = sg_table[i].sg_table;\n-\t\tsg_per_table = sg_table[i].rd_sg_count;\n-\n-\t\tfor (j = 0; j < sg_per_table; j++) {\n-\t\t\tpg = sg_page(&sg[j]);\n-\t\t\tif (pg) {\n-\t\t\t\t__free_page(pg);\n-\t\t\t\tpage_count++;\n-\t\t\t}\n-\t\t}\n-\n-\t\tkfree(sg);\n-\t}\n+\tpage_count = rd_release_sgl_table(rd_dev, rd_dev->sg_table_array,\n+\t\t\t\t\t  rd_dev->sg_table_count);\n \n \tpr_debug(\"CORE_RD[%u] - Released device space for Ramdisk\"\n \t\t\" Device ID: %u, pages %u in %u tables total bytes %lu\\n\",\n \t\trd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,\n \t\trd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);\n \n-\tkfree(sg_table);\n \trd_dev->sg_table_array = NULL;\n \trd_dev->sg_table_count = 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tu32 page_count;",
                "\tpage_count = rd_release_sgl_table(rd_dev, rd_dev->sg_table_array,",
                "\t\t\t\t\t  rd_dev->sg_table_count);"
            ],
            "deleted": [
                "\tu32 i, j, page_count = 0, sg_per_table;",
                "\tstruct rd_dev_sg_table *sg_table;",
                "\tstruct page *pg;",
                "\tstruct scatterlist *sg;",
                "\tsg_table = rd_dev->sg_table_array;",
                "",
                "\tfor (i = 0; i < rd_dev->sg_table_count; i++) {",
                "\t\tsg = sg_table[i].sg_table;",
                "\t\tsg_per_table = sg_table[i].rd_sg_count;",
                "",
                "\t\tfor (j = 0; j < sg_per_table; j++) {",
                "\t\t\tpg = sg_page(&sg[j]);",
                "\t\t\tif (pg) {",
                "\t\t\t\t__free_page(pg);",
                "\t\t\t\tpage_count++;",
                "\t\t\t}",
                "\t\t}",
                "",
                "\t\tkfree(sg);",
                "\t}",
                "\tkfree(sg_table);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The rd_build_device_space function in drivers/target/target_core_rd.c in the Linux kernel before 3.14 does not properly initialize a certain data structure, which allows local users to obtain sensitive information from ramdisk_mcp memory by leveraging access to a SCSI initiator.",
        "id": 556
    },
    {
        "cve_id": "CVE-2013-0160",
        "code_before_change": "static inline ssize_t do_tty_write(\n\tssize_t (*write)(struct tty_struct *, struct file *, const unsigned char *, size_t),\n\tstruct tty_struct *tty,\n\tstruct file *file,\n\tconst char __user *buf,\n\tsize_t count)\n{\n\tssize_t ret, written = 0;\n\tunsigned int chunk;\n\n\tret = tty_write_lock(tty, file->f_flags & O_NDELAY);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/*\n\t * We chunk up writes into a temporary buffer. This\n\t * simplifies low-level drivers immensely, since they\n\t * don't have locking issues and user mode accesses.\n\t *\n\t * But if TTY_NO_WRITE_SPLIT is set, we should use a\n\t * big chunk-size..\n\t *\n\t * The default chunk-size is 2kB, because the NTTY\n\t * layer has problems with bigger chunks. It will\n\t * claim to be able to handle more characters than\n\t * it actually does.\n\t *\n\t * FIXME: This can probably go away now except that 64K chunks\n\t * are too likely to fail unless switched to vmalloc...\n\t */\n\tchunk = 2048;\n\tif (test_bit(TTY_NO_WRITE_SPLIT, &tty->flags))\n\t\tchunk = 65536;\n\tif (count < chunk)\n\t\tchunk = count;\n\n\t/* write_buf/write_cnt is protected by the atomic_write_lock mutex */\n\tif (tty->write_cnt < chunk) {\n\t\tunsigned char *buf_chunk;\n\n\t\tif (chunk < 1024)\n\t\t\tchunk = 1024;\n\n\t\tbuf_chunk = kmalloc(chunk, GFP_KERNEL);\n\t\tif (!buf_chunk) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tkfree(tty->write_buf);\n\t\ttty->write_cnt = chunk;\n\t\ttty->write_buf = buf_chunk;\n\t}\n\n\t/* Do the write .. */\n\tfor (;;) {\n\t\tsize_t size = count;\n\t\tif (size > chunk)\n\t\t\tsize = chunk;\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(tty->write_buf, buf, size))\n\t\t\tbreak;\n\t\tret = write(tty, file, tty->write_buf, size);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\twritten += ret;\n\t\tbuf += ret;\n\t\tcount -= ret;\n\t\tif (!count)\n\t\t\tbreak;\n\t\tret = -ERESTARTSYS;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (written) {\n\t\tstruct inode *inode = file->f_path.dentry->d_inode;\n\t\tinode->i_mtime = current_fs_time(inode->i_sb);\n\t\tret = written;\n\t}\nout:\n\ttty_write_unlock(tty);\n\treturn ret;\n}",
        "code_after_change": "static inline ssize_t do_tty_write(\n\tssize_t (*write)(struct tty_struct *, struct file *, const unsigned char *, size_t),\n\tstruct tty_struct *tty,\n\tstruct file *file,\n\tconst char __user *buf,\n\tsize_t count)\n{\n\tssize_t ret, written = 0;\n\tunsigned int chunk;\n\n\tret = tty_write_lock(tty, file->f_flags & O_NDELAY);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/*\n\t * We chunk up writes into a temporary buffer. This\n\t * simplifies low-level drivers immensely, since they\n\t * don't have locking issues and user mode accesses.\n\t *\n\t * But if TTY_NO_WRITE_SPLIT is set, we should use a\n\t * big chunk-size..\n\t *\n\t * The default chunk-size is 2kB, because the NTTY\n\t * layer has problems with bigger chunks. It will\n\t * claim to be able to handle more characters than\n\t * it actually does.\n\t *\n\t * FIXME: This can probably go away now except that 64K chunks\n\t * are too likely to fail unless switched to vmalloc...\n\t */\n\tchunk = 2048;\n\tif (test_bit(TTY_NO_WRITE_SPLIT, &tty->flags))\n\t\tchunk = 65536;\n\tif (count < chunk)\n\t\tchunk = count;\n\n\t/* write_buf/write_cnt is protected by the atomic_write_lock mutex */\n\tif (tty->write_cnt < chunk) {\n\t\tunsigned char *buf_chunk;\n\n\t\tif (chunk < 1024)\n\t\t\tchunk = 1024;\n\n\t\tbuf_chunk = kmalloc(chunk, GFP_KERNEL);\n\t\tif (!buf_chunk) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tkfree(tty->write_buf);\n\t\ttty->write_cnt = chunk;\n\t\ttty->write_buf = buf_chunk;\n\t}\n\n\t/* Do the write .. */\n\tfor (;;) {\n\t\tsize_t size = count;\n\t\tif (size > chunk)\n\t\t\tsize = chunk;\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(tty->write_buf, buf, size))\n\t\t\tbreak;\n\t\tret = write(tty, file, tty->write_buf, size);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\twritten += ret;\n\t\tbuf += ret;\n\t\tcount -= ret;\n\t\tif (!count)\n\t\t\tbreak;\n\t\tret = -ERESTARTSYS;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (written)\n\t\tret = written;\nout:\n\ttty_write_unlock(tty);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -72,11 +72,8 @@\n \t\t\tbreak;\n \t\tcond_resched();\n \t}\n-\tif (written) {\n-\t\tstruct inode *inode = file->f_path.dentry->d_inode;\n-\t\tinode->i_mtime = current_fs_time(inode->i_sb);\n+\tif (written)\n \t\tret = written;\n-\t}\n out:\n \ttty_write_unlock(tty);\n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "\tif (written)"
            ],
            "deleted": [
                "\tif (written) {",
                "\t\tstruct inode *inode = file->f_path.dentry->d_inode;",
                "\t\tinode->i_mtime = current_fs_time(inode->i_sb);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The Linux kernel through 3.7.9 allows local users to obtain sensitive information about keystroke timing by using the inotify API on the /dev/ptmx device.",
        "id": 146
    },
    {
        "cve_id": "CVE-2017-18549",
        "code_before_change": "static int aac_get_hba_info(struct aac_dev *dev, void __user *arg)\n{\n\tstruct aac_hba_info hbainfo;\n\n\thbainfo.adapter_number\t\t= (u8) dev->id;\n\thbainfo.system_io_bus_number\t= dev->pdev->bus->number;\n\thbainfo.device_number\t\t= (dev->pdev->devfn >> 3);\n\thbainfo.function_number\t\t= (dev->pdev->devfn & 0x0007);\n\n\thbainfo.vendor_id\t\t= dev->pdev->vendor;\n\thbainfo.device_id\t\t= dev->pdev->device;\n\thbainfo.sub_vendor_id\t\t= dev->pdev->subsystem_vendor;\n\thbainfo.sub_system_id\t\t= dev->pdev->subsystem_device;\n\n\tif (copy_to_user(arg, &hbainfo, sizeof(struct aac_hba_info))) {\n\t\tdprintk((KERN_DEBUG \"aacraid: Could not copy hba info\\n\"));\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int aac_get_hba_info(struct aac_dev *dev, void __user *arg)\n{\n\tstruct aac_hba_info hbainfo;\n\n\tmemset(&hbainfo, 0, sizeof(hbainfo));\n\thbainfo.adapter_number\t\t= (u8) dev->id;\n\thbainfo.system_io_bus_number\t= dev->pdev->bus->number;\n\thbainfo.device_number\t\t= (dev->pdev->devfn >> 3);\n\thbainfo.function_number\t\t= (dev->pdev->devfn & 0x0007);\n\n\thbainfo.vendor_id\t\t= dev->pdev->vendor;\n\thbainfo.device_id\t\t= dev->pdev->device;\n\thbainfo.sub_vendor_id\t\t= dev->pdev->subsystem_vendor;\n\thbainfo.sub_system_id\t\t= dev->pdev->subsystem_device;\n\n\tif (copy_to_user(arg, &hbainfo, sizeof(struct aac_hba_info))) {\n\t\tdprintk((KERN_DEBUG \"aacraid: Could not copy hba info\\n\"));\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct aac_hba_info hbainfo;\n \n+\tmemset(&hbainfo, 0, sizeof(hbainfo));\n \thbainfo.adapter_number\t\t= (u8) dev->id;\n \thbainfo.system_io_bus_number\t= dev->pdev->bus->number;\n \thbainfo.device_number\t\t= (dev->pdev->devfn >> 3);",
        "function_modified_lines": {
            "added": [
                "\tmemset(&hbainfo, 0, sizeof(hbainfo));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in drivers/scsi/aacraid/commctrl.c in the Linux kernel before 4.13. There is potential exposure of kernel stack memory because aac_send_raw_srb does not initialize the reply structure.",
        "id": 1438
    },
    {
        "cve_id": "CVE-2017-16911",
        "code_before_change": "static ssize_t store_attach(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct socket *socket;\n\tint sockfd = 0;\n\t__u32 port = 0, pdev_nr = 0, rhport = 0, devid = 0, speed = 0;\n\tstruct usb_hcd *hcd;\n\tstruct vhci_hcd *vhci_hcd;\n\tstruct vhci_device *vdev;\n\tstruct vhci *vhci;\n\tint err;\n\tunsigned long flags;\n\n\t/*\n\t * @rhport: port number of vhci_hcd\n\t * @sockfd: socket descriptor of an established TCP connection\n\t * @devid: unique device identifier in a remote host\n\t * @speed: usb device speed in a remote host\n\t */\n\tif (sscanf(buf, \"%u %u %u %u\", &port, &sockfd, &devid, &speed) != 4)\n\t\treturn -EINVAL;\n\tpdev_nr = port_to_pdev_nr(port);\n\trhport = port_to_rhport(port);\n\n\tusbip_dbg_vhci_sysfs(\"port(%u) pdev(%d) rhport(%u)\\n\",\n\t\t\t     port, pdev_nr, rhport);\n\tusbip_dbg_vhci_sysfs(\"sockfd(%u) devid(%u) speed(%u)\\n\",\n\t\t\t     sockfd, devid, speed);\n\n\t/* check received parameters */\n\tif (!valid_args(pdev_nr, rhport, speed))\n\t\treturn -EINVAL;\n\n\thcd = platform_get_drvdata(vhcis[pdev_nr].pdev);\n\tif (hcd == NULL) {\n\t\tdev_err(dev, \"port %d is not ready\\n\", port);\n\t\treturn -EAGAIN;\n\t}\n\n\tvhci_hcd = hcd_to_vhci_hcd(hcd);\n\tvhci = vhci_hcd->vhci;\n\n\tif (speed == USB_SPEED_SUPER)\n\t\tvdev = &vhci->vhci_hcd_ss->vdev[rhport];\n\telse\n\t\tvdev = &vhci->vhci_hcd_hs->vdev[rhport];\n\n\t/* Extract socket from fd. */\n\tsocket = sockfd_lookup(sockfd, &err);\n\tif (!socket)\n\t\treturn -EINVAL;\n\n\t/* now need lock until setting vdev status as used */\n\n\t/* begin a lock */\n\tspin_lock_irqsave(&vhci->lock, flags);\n\tspin_lock(&vdev->ud.lock);\n\n\tif (vdev->ud.status != VDEV_ST_NULL) {\n\t\t/* end of the lock */\n\t\tspin_unlock(&vdev->ud.lock);\n\t\tspin_unlock_irqrestore(&vhci->lock, flags);\n\n\t\tsockfd_put(socket);\n\n\t\tdev_err(dev, \"port %d already used\\n\", rhport);\n\t\t/*\n\t\t * Will be retried from userspace\n\t\t * if there's another free port.\n\t\t */\n\t\treturn -EBUSY;\n\t}\n\n\tdev_info(dev, \"pdev(%u) rhport(%u) sockfd(%d)\\n\",\n\t\t pdev_nr, rhport, sockfd);\n\tdev_info(dev, \"devid(%u) speed(%u) speed_str(%s)\\n\",\n\t\t devid, speed, usb_speed_string(speed));\n\n\tvdev->devid         = devid;\n\tvdev->speed         = speed;\n\tvdev->ud.tcp_socket = socket;\n\tvdev->ud.status     = VDEV_ST_NOTASSIGNED;\n\n\tspin_unlock(&vdev->ud.lock);\n\tspin_unlock_irqrestore(&vhci->lock, flags);\n\t/* end the lock */\n\n\tvdev->ud.tcp_rx = kthread_get_run(vhci_rx_loop, &vdev->ud, \"vhci_rx\");\n\tvdev->ud.tcp_tx = kthread_get_run(vhci_tx_loop, &vdev->ud, \"vhci_tx\");\n\n\trh_port_connect(vdev, speed);\n\n\treturn count;\n}",
        "code_after_change": "static ssize_t store_attach(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct socket *socket;\n\tint sockfd = 0;\n\t__u32 port = 0, pdev_nr = 0, rhport = 0, devid = 0, speed = 0;\n\tstruct usb_hcd *hcd;\n\tstruct vhci_hcd *vhci_hcd;\n\tstruct vhci_device *vdev;\n\tstruct vhci *vhci;\n\tint err;\n\tunsigned long flags;\n\n\t/*\n\t * @rhport: port number of vhci_hcd\n\t * @sockfd: socket descriptor of an established TCP connection\n\t * @devid: unique device identifier in a remote host\n\t * @speed: usb device speed in a remote host\n\t */\n\tif (sscanf(buf, \"%u %u %u %u\", &port, &sockfd, &devid, &speed) != 4)\n\t\treturn -EINVAL;\n\tpdev_nr = port_to_pdev_nr(port);\n\trhport = port_to_rhport(port);\n\n\tusbip_dbg_vhci_sysfs(\"port(%u) pdev(%d) rhport(%u)\\n\",\n\t\t\t     port, pdev_nr, rhport);\n\tusbip_dbg_vhci_sysfs(\"sockfd(%u) devid(%u) speed(%u)\\n\",\n\t\t\t     sockfd, devid, speed);\n\n\t/* check received parameters */\n\tif (!valid_args(pdev_nr, rhport, speed))\n\t\treturn -EINVAL;\n\n\thcd = platform_get_drvdata(vhcis[pdev_nr].pdev);\n\tif (hcd == NULL) {\n\t\tdev_err(dev, \"port %d is not ready\\n\", port);\n\t\treturn -EAGAIN;\n\t}\n\n\tvhci_hcd = hcd_to_vhci_hcd(hcd);\n\tvhci = vhci_hcd->vhci;\n\n\tif (speed == USB_SPEED_SUPER)\n\t\tvdev = &vhci->vhci_hcd_ss->vdev[rhport];\n\telse\n\t\tvdev = &vhci->vhci_hcd_hs->vdev[rhport];\n\n\t/* Extract socket from fd. */\n\tsocket = sockfd_lookup(sockfd, &err);\n\tif (!socket)\n\t\treturn -EINVAL;\n\n\t/* now need lock until setting vdev status as used */\n\n\t/* begin a lock */\n\tspin_lock_irqsave(&vhci->lock, flags);\n\tspin_lock(&vdev->ud.lock);\n\n\tif (vdev->ud.status != VDEV_ST_NULL) {\n\t\t/* end of the lock */\n\t\tspin_unlock(&vdev->ud.lock);\n\t\tspin_unlock_irqrestore(&vhci->lock, flags);\n\n\t\tsockfd_put(socket);\n\n\t\tdev_err(dev, \"port %d already used\\n\", rhport);\n\t\t/*\n\t\t * Will be retried from userspace\n\t\t * if there's another free port.\n\t\t */\n\t\treturn -EBUSY;\n\t}\n\n\tdev_info(dev, \"pdev(%u) rhport(%u) sockfd(%d)\\n\",\n\t\t pdev_nr, rhport, sockfd);\n\tdev_info(dev, \"devid(%u) speed(%u) speed_str(%s)\\n\",\n\t\t devid, speed, usb_speed_string(speed));\n\n\tvdev->devid         = devid;\n\tvdev->speed         = speed;\n\tvdev->ud.sockfd     = sockfd;\n\tvdev->ud.tcp_socket = socket;\n\tvdev->ud.status     = VDEV_ST_NOTASSIGNED;\n\n\tspin_unlock(&vdev->ud.lock);\n\tspin_unlock_irqrestore(&vhci->lock, flags);\n\t/* end the lock */\n\n\tvdev->ud.tcp_rx = kthread_get_run(vhci_rx_loop, &vdev->ud, \"vhci_rx\");\n\tvdev->ud.tcp_tx = kthread_get_run(vhci_tx_loop, &vdev->ud, \"vhci_tx\");\n\n\trh_port_connect(vdev, speed);\n\n\treturn count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -78,6 +78,7 @@\n \n \tvdev->devid         = devid;\n \tvdev->speed         = speed;\n+\tvdev->ud.sockfd     = sockfd;\n \tvdev->ud.tcp_socket = socket;\n \tvdev->ud.status     = VDEV_ST_NOTASSIGNED;\n ",
        "function_modified_lines": {
            "added": [
                "\tvdev->ud.sockfd     = sockfd;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vhci_hcd driver in the Linux Kernel before version 4.14.8 and 4.4.114 allows allows local attackers to disclose kernel memory addresses. Successful exploitation requires that a USB device is attached over IP.",
        "id": 1347
    },
    {
        "cve_id": "CVE-2017-16911",
        "code_before_change": "static void port_show_vhci(char **out, int hub, int port, struct vhci_device *vdev)\n{\n\tif (hub == HUB_SPEED_HIGH)\n\t\t*out += sprintf(*out, \"hs  %04u %03u \",\n\t\t\t\t      port, vdev->ud.status);\n\telse /* hub == HUB_SPEED_SUPER */\n\t\t*out += sprintf(*out, \"ss  %04u %03u \",\n\t\t\t\t      port, vdev->ud.status);\n\n\tif (vdev->ud.status == VDEV_ST_USED) {\n\t\t*out += sprintf(*out, \"%03u %08x \",\n\t\t\t\t      vdev->speed, vdev->devid);\n\t\t*out += sprintf(*out, \"%16p %s\",\n\t\t\t\t      vdev->ud.tcp_socket,\n\t\t\t\t      dev_name(&vdev->udev->dev));\n\n\t} else {\n\t\t*out += sprintf(*out, \"000 00000000 \");\n\t\t*out += sprintf(*out, \"0000000000000000 0-0\");\n\t}\n\n\t*out += sprintf(*out, \"\\n\");\n}",
        "code_after_change": "static void port_show_vhci(char **out, int hub, int port, struct vhci_device *vdev)\n{\n\tif (hub == HUB_SPEED_HIGH)\n\t\t*out += sprintf(*out, \"hs  %04u %03u \",\n\t\t\t\t      port, vdev->ud.status);\n\telse /* hub == HUB_SPEED_SUPER */\n\t\t*out += sprintf(*out, \"ss  %04u %03u \",\n\t\t\t\t      port, vdev->ud.status);\n\n\tif (vdev->ud.status == VDEV_ST_USED) {\n\t\t*out += sprintf(*out, \"%03u %08x \",\n\t\t\t\t      vdev->speed, vdev->devid);\n\t\t*out += sprintf(*out, \"%u %s\",\n\t\t\t\t      vdev->ud.sockfd,\n\t\t\t\t      dev_name(&vdev->udev->dev));\n\n\t} else {\n\t\t*out += sprintf(*out, \"000 00000000 \");\n\t\t*out += sprintf(*out, \"0000000000000000 0-0\");\n\t}\n\n\t*out += sprintf(*out, \"\\n\");\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,8 @@\n \tif (vdev->ud.status == VDEV_ST_USED) {\n \t\t*out += sprintf(*out, \"%03u %08x \",\n \t\t\t\t      vdev->speed, vdev->devid);\n-\t\t*out += sprintf(*out, \"%16p %s\",\n-\t\t\t\t      vdev->ud.tcp_socket,\n+\t\t*out += sprintf(*out, \"%u %s\",\n+\t\t\t\t      vdev->ud.sockfd,\n \t\t\t\t      dev_name(&vdev->udev->dev));\n \n \t} else {",
        "function_modified_lines": {
            "added": [
                "\t\t*out += sprintf(*out, \"%u %s\",",
                "\t\t\t\t      vdev->ud.sockfd,"
            ],
            "deleted": [
                "\t\t*out += sprintf(*out, \"%16p %s\",",
                "\t\t\t\t      vdev->ud.tcp_socket,"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vhci_hcd driver in the Linux Kernel before version 4.14.8 and 4.4.114 allows allows local attackers to disclose kernel memory addresses. Successful exploitation requires that a USB device is attached over IP.",
        "id": 1346
    },
    {
        "cve_id": "CVE-2017-16911",
        "code_before_change": "static ssize_t nports_show(struct device *dev, struct device_attribute *attr,\n\t\t\t   char *out)\n{\n\tchar *s = out;\n\n\t/*\n\t * Half the ports are for SPEED_HIGH and half for SPEED_SUPER, thus the * 2.\n\t */\n\tout += sprintf(out, \"%d\\n\", VHCI_PORTS * vhci_num_controllers);\n\treturn out - s;\n}",
        "code_after_change": "static ssize_t nports_show(struct device *dev, struct device_attribute *attr,\n\t\t\t   char *out)\n{\n\tchar *s = out;\n\n\t/*\n\t * Half the ports are for SPEED_HIGH and half for SPEED_SUPER,\n\t * thus the * 2.\n\t */\n\tout += sprintf(out, \"%d\\n\", VHCI_PORTS * vhci_num_controllers);\n\treturn out - s;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,8 @@\n \tchar *s = out;\n \n \t/*\n-\t * Half the ports are for SPEED_HIGH and half for SPEED_SUPER, thus the * 2.\n+\t * Half the ports are for SPEED_HIGH and half for SPEED_SUPER,\n+\t * thus the * 2.\n \t */\n \tout += sprintf(out, \"%d\\n\", VHCI_PORTS * vhci_num_controllers);\n \treturn out - s;",
        "function_modified_lines": {
            "added": [
                "\t * Half the ports are for SPEED_HIGH and half for SPEED_SUPER,",
                "\t * thus the * 2."
            ],
            "deleted": [
                "\t * Half the ports are for SPEED_HIGH and half for SPEED_SUPER, thus the * 2."
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vhci_hcd driver in the Linux Kernel before version 4.14.8 and 4.4.114 allows allows local attackers to disclose kernel memory addresses. Successful exploitation requires that a USB device is attached over IP.",
        "id": 1345
    },
    {
        "cve_id": "CVE-2016-8405",
        "code_before_change": "int fb_copy_cmap(const struct fb_cmap *from, struct fb_cmap *to)\n{\n\tint tooff = 0, fromoff = 0;\n\tint size;\n\n\tif (to->start > from->start)\n\t\tfromoff = to->start - from->start;\n\telse\n\t\ttooff = from->start - to->start;\n\tsize = to->len - tooff;\n\tif (size > (int) (from->len - fromoff))\n\t\tsize = from->len - fromoff;\n\tif (size <= 0)\n\t\treturn -EINVAL;\n\tsize *= sizeof(u16);\n\n\tmemcpy(to->red+tooff, from->red+fromoff, size);\n\tmemcpy(to->green+tooff, from->green+fromoff, size);\n\tmemcpy(to->blue+tooff, from->blue+fromoff, size);\n\tif (from->transp && to->transp)\n\t\tmemcpy(to->transp+tooff, from->transp+fromoff, size);\n\treturn 0;\n}",
        "code_after_change": "int fb_copy_cmap(const struct fb_cmap *from, struct fb_cmap *to)\n{\n\tunsigned int tooff = 0, fromoff = 0;\n\tsize_t size;\n\n\tif (to->start > from->start)\n\t\tfromoff = to->start - from->start;\n\telse\n\t\ttooff = from->start - to->start;\n\tif (fromoff >= from->len || tooff >= to->len)\n\t\treturn -EINVAL;\n\n\tsize = min_t(size_t, to->len - tooff, from->len - fromoff);\n\tif (size == 0)\n\t\treturn -EINVAL;\n\tsize *= sizeof(u16);\n\n\tmemcpy(to->red+tooff, from->red+fromoff, size);\n\tmemcpy(to->green+tooff, from->green+fromoff, size);\n\tmemcpy(to->blue+tooff, from->blue+fromoff, size);\n\tif (from->transp && to->transp)\n\t\tmemcpy(to->transp+tooff, from->transp+fromoff, size);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,16 +1,17 @@\n int fb_copy_cmap(const struct fb_cmap *from, struct fb_cmap *to)\n {\n-\tint tooff = 0, fromoff = 0;\n-\tint size;\n+\tunsigned int tooff = 0, fromoff = 0;\n+\tsize_t size;\n \n \tif (to->start > from->start)\n \t\tfromoff = to->start - from->start;\n \telse\n \t\ttooff = from->start - to->start;\n-\tsize = to->len - tooff;\n-\tif (size > (int) (from->len - fromoff))\n-\t\tsize = from->len - fromoff;\n-\tif (size <= 0)\n+\tif (fromoff >= from->len || tooff >= to->len)\n+\t\treturn -EINVAL;\n+\n+\tsize = min_t(size_t, to->len - tooff, from->len - fromoff);\n+\tif (size == 0)\n \t\treturn -EINVAL;\n \tsize *= sizeof(u16);\n ",
        "function_modified_lines": {
            "added": [
                "\tunsigned int tooff = 0, fromoff = 0;",
                "\tsize_t size;",
                "\tif (fromoff >= from->len || tooff >= to->len)",
                "\t\treturn -EINVAL;",
                "",
                "\tsize = min_t(size_t, to->len - tooff, from->len - fromoff);",
                "\tif (size == 0)"
            ],
            "deleted": [
                "\tint tooff = 0, fromoff = 0;",
                "\tint size;",
                "\tsize = to->len - tooff;",
                "\tif (size > (int) (from->len - fromoff))",
                "\t\tsize = from->len - fromoff;",
                "\tif (size <= 0)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An information disclosure vulnerability in kernel components including the ION subsystem, Binder, USB driver and networking subsystem could enable a local malicious application to access data outside of its permission levels. This issue is rated as Moderate because it first requires compromising a privileged process. Product: Android. Versions: Kernel-3.10, Kernel-3.18. Android ID: A-31651010.",
        "id": 1117
    }
]