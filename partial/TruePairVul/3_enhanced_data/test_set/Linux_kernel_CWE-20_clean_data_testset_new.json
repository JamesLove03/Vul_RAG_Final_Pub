[
    {
        "cve_id": "CVE-2018-20669",
        "code_before_change": "long strnlen_user(const char __user *str, long count)\n{\n\tunsigned long max_addr, src_addr;\n\n\tif (unlikely(count <= 0))\n\t\treturn 0;\n\n\tmax_addr = user_addr_max();\n\tsrc_addr = (unsigned long)str;\n\tif (likely(src_addr < max_addr)) {\n\t\tunsigned long max = max_addr - src_addr;\n\t\tlong retval;\n\n\t\tuser_access_begin();\n\t\tretval = do_strnlen_user(str, count, max);\n\t\tuser_access_end();\n\t\treturn retval;\n\t}\n\treturn 0;\n}",
        "code_after_change": "long strnlen_user(const char __user *str, long count)\n{\n\tunsigned long max_addr, src_addr;\n\n\tif (unlikely(count <= 0))\n\t\treturn 0;\n\n\tmax_addr = user_addr_max();\n\tsrc_addr = (unsigned long)str;\n\tif (likely(src_addr < max_addr)) {\n\t\tunsigned long max = max_addr - src_addr;\n\t\tlong retval;\n\n\t\tif (user_access_begin(str, max)) {\n\t\t\tretval = do_strnlen_user(str, count, max);\n\t\t\tuser_access_end();\n\t\t\treturn retval;\n\t\t}\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,10 +11,11 @@\n \t\tunsigned long max = max_addr - src_addr;\n \t\tlong retval;\n \n-\t\tuser_access_begin();\n-\t\tretval = do_strnlen_user(str, count, max);\n-\t\tuser_access_end();\n-\t\treturn retval;\n+\t\tif (user_access_begin(str, max)) {\n+\t\t\tretval = do_strnlen_user(str, count, max);\n+\t\t\tuser_access_end();\n+\t\t\treturn retval;\n+\t\t}\n \t}\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (user_access_begin(str, max)) {",
                "\t\t\tretval = do_strnlen_user(str, count, max);",
                "\t\t\tuser_access_end();",
                "\t\t\treturn retval;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tuser_access_begin();",
                "\t\tretval = do_strnlen_user(str, count, max);",
                "\t\tuser_access_end();",
                "\t\treturn retval;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue where a provided address with access_ok() is not checked was discovered in i915_gem_execbuffer2_ioctl in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Linux kernel through 4.19.13. A local attacker can craft a malicious IOCTL function call to overwrite arbitrary kernel memory, resulting in a Denial of Service or privilege escalation.",
        "id": 1780
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int unix_stream_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size,\n\t\t\t       int flags)\n{\n\tstruct sock_iocb *siocb = kiocb_to_siocb(iocb);\n\tstruct scm_cookie tmp_scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sockaddr_un *sunaddr = msg->msg_name;\n\tint copied = 0;\n\tint check_creds = 0;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttarget = sock_rcvlowat(sk, flags&MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, flags&MSG_DONTWAIT);\n\n\tmsg->msg_namelen = 0;\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\n\tif (!siocb->scm) {\n\t\tsiocb->scm = &tmp_scm;\n\t\tmemset(&tmp_scm, 0, sizeof(tmp_scm));\n\t}\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err) {\n\t\terr = sock_intr_errno(timeo);\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tint chunk;\n\t\tstruct sk_buff *skb, *last;\n\n\t\tunix_state_lock(sk);\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\nagain:\n\t\tif (skb == NULL) {\n\t\t\tunix_sk(sk)->recursion_level = 0;\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t\tmutex_unlock(&u->readlock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last);\n\n\t\t\tif (signal_pending(current)\n\t\t\t    ||  mutex_lock_interruptible(&u->readlock)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcontinue;\n unlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif ((UNIXCB(skb).pid  != siocb->scm->pid) ||\n\t\t\t    !uid_eq(UNIXCB(skb).uid, siocb->scm->creds.uid) ||\n\t\t\t    !gid_eq(UNIXCB(skb).gid, siocb->scm->creds.gid))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(siocb->scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tcheck_creds = 1;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (sunaddr) {\n\t\t\tunix_copy_addr(msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tif (skb_copy_datagram_iovec(skb, UNIXCB(skb).consumed + skip,\n\t\t\t\t\t    msg->msg_iov, chunk)) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_detach_fds(siocb->scm, skb);\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (siocb->scm->fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tsiocb->scm->fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->readlock);\n\tscm_recv(sock, msg, siocb->scm, flags);\nout:\n\treturn copied ? : err;\n}",
        "code_after_change": "static int unix_stream_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size,\n\t\t\t       int flags)\n{\n\tstruct sock_iocb *siocb = kiocb_to_siocb(iocb);\n\tstruct scm_cookie tmp_scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sockaddr_un *sunaddr = msg->msg_name;\n\tint copied = 0;\n\tint check_creds = 0;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttarget = sock_rcvlowat(sk, flags&MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, flags&MSG_DONTWAIT);\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\n\tif (!siocb->scm) {\n\t\tsiocb->scm = &tmp_scm;\n\t\tmemset(&tmp_scm, 0, sizeof(tmp_scm));\n\t}\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err) {\n\t\terr = sock_intr_errno(timeo);\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tint chunk;\n\t\tstruct sk_buff *skb, *last;\n\n\t\tunix_state_lock(sk);\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\nagain:\n\t\tif (skb == NULL) {\n\t\t\tunix_sk(sk)->recursion_level = 0;\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t\tmutex_unlock(&u->readlock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last);\n\n\t\t\tif (signal_pending(current)\n\t\t\t    ||  mutex_lock_interruptible(&u->readlock)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcontinue;\n unlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif ((UNIXCB(skb).pid  != siocb->scm->pid) ||\n\t\t\t    !uid_eq(UNIXCB(skb).uid, siocb->scm->creds.uid) ||\n\t\t\t    !gid_eq(UNIXCB(skb).gid, siocb->scm->creds.gid))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(siocb->scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tcheck_creds = 1;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (sunaddr) {\n\t\t\tunix_copy_addr(msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tif (skb_copy_datagram_iovec(skb, UNIXCB(skb).consumed + skip,\n\t\t\t\t\t    msg->msg_iov, chunk)) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_detach_fds(siocb->scm, skb);\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (siocb->scm->fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tsiocb->scm->fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->readlock);\n\tscm_recv(sock, msg, siocb->scm, flags);\nout:\n\treturn copied ? : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,8 +24,6 @@\n \n \ttarget = sock_rcvlowat(sk, flags&MSG_WAITALL, size);\n \ttimeo = sock_rcvtimeo(sk, flags&MSG_DONTWAIT);\n-\n-\tmsg->msg_namelen = 0;\n \n \t/* Lock the socket to prevent queue disordering\n \t * while sleeps in memcpy_tomsg",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 401
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\tif (size > INT_MAX)\n\t\tsize = INT_MAX;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_iovlen = 1;\n\tmsg.msg_iov = &iov;\n\tiov.iov_len = size;\n\tiov.iov_base = ubuf;\n\tmsg.msg_name = (struct sockaddr *)&address;\n\tmsg.msg_namelen = sizeof(address);\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, size, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "code_after_change": "\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\tif (size > INT_MAX)\n\t\tsize = INT_MAX;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_iovlen = 1;\n\tmsg.msg_iov = &iov;\n\tiov.iov_len = size;\n\tiov.iov_base = ubuf;\n\t/* Save some cycles and don't copy the address if not needed */\n\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg.msg_namelen = 0;\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, size, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,8 +22,10 @@\n \tmsg.msg_iov = &iov;\n \tiov.iov_len = size;\n \tiov.iov_base = ubuf;\n-\tmsg.msg_name = (struct sockaddr *)&address;\n-\tmsg.msg_namelen = sizeof(address);\n+\t/* Save some cycles and don't copy the address if not needed */\n+\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n+\t/* We assume all kernel code knows the size of sockaddr_storage */\n+\tmsg.msg_namelen = 0;\n \tif (sock->file->f_flags & O_NONBLOCK)\n \t\tflags |= MSG_DONTWAIT;\n \terr = sock_recvmsg(sock, &msg, size, flags);",
        "function_modified_lines": {
            "added": [
                "\t/* Save some cycles and don't copy the address if not needed */",
                "\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;",
                "\t/* We assume all kernel code knows the size of sockaddr_storage */",
                "\tmsg.msg_namelen = 0;"
            ],
            "deleted": [
                "\tmsg.msg_name = (struct sockaddr *)&address;",
                "\tmsg.msg_namelen = sizeof(address);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 397
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "int rxrpc_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct rxrpc_skb_priv *sp;\n\tstruct rxrpc_call *call = NULL, *continue_call = NULL;\n\tstruct rxrpc_sock *rx = rxrpc_sk(sock->sk);\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tint copy, ret, ullen, offset, copied = 0;\n\tu32 abort_code;\n\n\tDEFINE_WAIT(wait);\n\n\t_enter(\",,,%zu,%d\", len, flags);\n\n\tif (flags & (MSG_OOB | MSG_TRUNC))\n\t\treturn -EOPNOTSUPP;\n\n\tullen = msg->msg_flags & MSG_CMSG_COMPAT ? 4 : sizeof(unsigned long);\n\n\ttimeo = sock_rcvtimeo(&rx->sk, flags & MSG_DONTWAIT);\n\tmsg->msg_flags |= MSG_MORE;\n\n\tlock_sock(&rx->sk);\n\n\tfor (;;) {\n\t\t/* return immediately if a client socket has no outstanding\n\t\t * calls */\n\t\tif (RB_EMPTY_ROOT(&rx->calls)) {\n\t\t\tif (copied)\n\t\t\t\tgoto out;\n\t\t\tif (rx->sk.sk_state != RXRPC_SERVER_LISTENING) {\n\t\t\t\trelease_sock(&rx->sk);\n\t\t\t\tif (continue_call)\n\t\t\t\t\trxrpc_put_call(continue_call);\n\t\t\t\treturn -ENODATA;\n\t\t\t}\n\t\t}\n\n\t\t/* get the next message on the Rx queue */\n\t\tskb = skb_peek(&rx->sk.sk_receive_queue);\n\t\tif (!skb) {\n\t\t\t/* nothing remains on the queue */\n\t\t\tif (copied &&\n\t\t\t    (msg->msg_flags & MSG_PEEK || timeo == 0))\n\t\t\t\tgoto out;\n\n\t\t\t/* wait for a message to turn up */\n\t\t\trelease_sock(&rx->sk);\n\t\t\tprepare_to_wait_exclusive(sk_sleep(&rx->sk), &wait,\n\t\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\t\tret = sock_error(&rx->sk);\n\t\t\tif (ret)\n\t\t\t\tgoto wait_error;\n\n\t\t\tif (skb_queue_empty(&rx->sk.sk_receive_queue)) {\n\t\t\t\tif (signal_pending(current))\n\t\t\t\t\tgoto wait_interrupted;\n\t\t\t\ttimeo = schedule_timeout(timeo);\n\t\t\t}\n\t\t\tfinish_wait(sk_sleep(&rx->sk), &wait);\n\t\t\tlock_sock(&rx->sk);\n\t\t\tcontinue;\n\t\t}\n\n\tpeek_next_packet:\n\t\tsp = rxrpc_skb(skb);\n\t\tcall = sp->call;\n\t\tASSERT(call != NULL);\n\n\t\t_debug(\"next pkt %s\", rxrpc_pkts[sp->hdr.type]);\n\n\t\t/* make sure we wait for the state to be updated in this call */\n\t\tspin_lock_bh(&call->lock);\n\t\tspin_unlock_bh(&call->lock);\n\n\t\tif (test_bit(RXRPC_CALL_RELEASED, &call->flags)) {\n\t\t\t_debug(\"packet from released call\");\n\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\t\tBUG();\n\t\t\trxrpc_free_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* determine whether to continue last data receive */\n\t\tif (continue_call) {\n\t\t\t_debug(\"maybe cont\");\n\t\t\tif (call != continue_call ||\n\t\t\t    skb->mark != RXRPC_SKB_MARK_DATA) {\n\t\t\t\trelease_sock(&rx->sk);\n\t\t\t\trxrpc_put_call(continue_call);\n\t\t\t\t_leave(\" = %d [noncont]\", copied);\n\t\t\t\treturn copied;\n\t\t\t}\n\t\t}\n\n\t\trxrpc_get_call(call);\n\n\t\t/* copy the peer address and timestamp */\n\t\tif (!continue_call) {\n\t\t\tif (msg->msg_name && msg->msg_namelen > 0)\n\t\t\t\tmemcpy(msg->msg_name,\n\t\t\t\t       &call->conn->trans->peer->srx,\n\t\t\t\t       sizeof(call->conn->trans->peer->srx));\n\t\t\tsock_recv_ts_and_drops(msg, &rx->sk, skb);\n\t\t}\n\n\t\t/* receive the message */\n\t\tif (skb->mark != RXRPC_SKB_MARK_DATA)\n\t\t\tgoto receive_non_data_message;\n\n\t\t_debug(\"recvmsg DATA #%u { %d, %d }\",\n\t\t       ntohl(sp->hdr.seq), skb->len, sp->offset);\n\n\t\tif (!continue_call) {\n\t\t\t/* only set the control data once per recvmsg() */\n\t\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_USER_CALL_ID,\n\t\t\t\t       ullen, &call->user_call_ID);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto copy_error;\n\t\t\tASSERT(test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\n\t\t}\n\n\t\tASSERTCMP(ntohl(sp->hdr.seq), >=, call->rx_data_recv);\n\t\tASSERTCMP(ntohl(sp->hdr.seq), <=, call->rx_data_recv + 1);\n\t\tcall->rx_data_recv = ntohl(sp->hdr.seq);\n\n\t\tASSERTCMP(ntohl(sp->hdr.seq), >, call->rx_data_eaten);\n\n\t\toffset = sp->offset;\n\t\tcopy = skb->len - offset;\n\t\tif (copy > len - copied)\n\t\t\tcopy = len - copied;\n\n\t\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\t\tret = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t      msg->msg_iov, copy);\n\t\t} else {\n\t\t\tret = skb_copy_and_csum_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t       msg->msg_iov);\n\t\t\tif (ret == -EINVAL)\n\t\t\t\tgoto csum_copy_error;\n\t\t}\n\n\t\tif (ret < 0)\n\t\t\tgoto copy_error;\n\n\t\t/* handle piecemeal consumption of data packets */\n\t\t_debug(\"copied %d+%d\", copy, copied);\n\n\t\toffset += copy;\n\t\tcopied += copy;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tsp->offset = offset;\n\n\t\tif (sp->offset < skb->len) {\n\t\t\t_debug(\"buffer full\");\n\t\t\tASSERTCMP(copied, ==, len);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* we transferred the whole data packet */\n\t\tif (sp->hdr.flags & RXRPC_LAST_PACKET) {\n\t\t\t_debug(\"last\");\n\t\t\tif (call->conn->out_clientflag) {\n\t\t\t\t /* last byte of reply received */\n\t\t\t\tret = copied;\n\t\t\t\tgoto terminal_message;\n\t\t\t}\n\n\t\t\t/* last bit of request received */\n\t\t\tif (!(flags & MSG_PEEK)) {\n\t\t\t\t_debug(\"eat packet\");\n\t\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) !=\n\t\t\t\t    skb)\n\t\t\t\t\tBUG();\n\t\t\t\trxrpc_free_skb(skb);\n\t\t\t}\n\t\t\tmsg->msg_flags &= ~MSG_MORE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* move on to the next data message */\n\t\t_debug(\"next\");\n\t\tif (!continue_call)\n\t\t\tcontinue_call = sp->call;\n\t\telse\n\t\t\trxrpc_put_call(call);\n\t\tcall = NULL;\n\n\t\tif (flags & MSG_PEEK) {\n\t\t\t_debug(\"peek next\");\n\t\t\tskb = skb->next;\n\t\t\tif (skb == (struct sk_buff *) &rx->sk.sk_receive_queue)\n\t\t\t\tbreak;\n\t\t\tgoto peek_next_packet;\n\t\t}\n\n\t\t_debug(\"eat packet\");\n\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\tBUG();\n\t\trxrpc_free_skb(skb);\n\t}\n\n\t/* end of non-terminal data packet reception for the moment */\n\t_debug(\"end rcv data\");\nout:\n\trelease_sock(&rx->sk);\n\tif (call)\n\t\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d [data]\", copied);\n\treturn copied;\n\n\t/* handle non-DATA messages such as aborts, incoming connections and\n\t * final ACKs */\nreceive_non_data_message:\n\t_debug(\"non-data\");\n\n\tif (skb->mark == RXRPC_SKB_MARK_NEW_CALL) {\n\t\t_debug(\"RECV NEW CALL\");\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_NEW_CALL, 0, &abort_code);\n\t\tif (ret < 0)\n\t\t\tgoto copy_error;\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\t\tBUG();\n\t\t\trxrpc_free_skb(skb);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_USER_CALL_ID,\n\t\t       ullen, &call->user_call_ID);\n\tif (ret < 0)\n\t\tgoto copy_error;\n\tASSERT(test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\n\n\tswitch (skb->mark) {\n\tcase RXRPC_SKB_MARK_DATA:\n\t\tBUG();\n\tcase RXRPC_SKB_MARK_FINAL_ACK:\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_ACK, 0, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_BUSY:\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_BUSY, 0, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_REMOTE_ABORT:\n\t\tabort_code = call->abort_code;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_ABORT, 4, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_NET_ERROR:\n\t\t_debug(\"RECV NET ERROR %d\", sp->error);\n\t\tabort_code = sp->error;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_NET_ERROR, 4, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_LOCAL_ERROR:\n\t\t_debug(\"RECV LOCAL ERROR %d\", sp->error);\n\t\tabort_code = sp->error;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_LOCAL_ERROR, 4,\n\t\t\t       &abort_code);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tif (ret < 0)\n\t\tgoto copy_error;\n\nterminal_message:\n\t_debug(\"terminal\");\n\tmsg->msg_flags &= ~MSG_MORE;\n\tmsg->msg_flags |= MSG_EOR;\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t_net(\"free terminal skb %p\", skb);\n\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\tBUG();\n\t\trxrpc_free_skb(skb);\n\t\trxrpc_remove_user_ID(rx, call);\n\t}\n\n\trelease_sock(&rx->sk);\n\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\ncopy_error:\n\t_debug(\"copy error\");\n\trelease_sock(&rx->sk);\n\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\ncsum_copy_error:\n\t_debug(\"csum error\");\n\trelease_sock(&rx->sk);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\trxrpc_kill_skb(skb);\n\tskb_kill_datagram(&rx->sk, skb, flags);\n\trxrpc_put_call(call);\n\treturn -EAGAIN;\n\nwait_interrupted:\n\tret = sock_intr_errno(timeo);\nwait_error:\n\tfinish_wait(sk_sleep(&rx->sk), &wait);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\tif (copied)\n\t\tcopied = ret;\n\t_leave(\" = %d [waitfail %d]\", copied, ret);\n\treturn copied;\n\n}",
        "code_after_change": "int rxrpc_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct rxrpc_skb_priv *sp;\n\tstruct rxrpc_call *call = NULL, *continue_call = NULL;\n\tstruct rxrpc_sock *rx = rxrpc_sk(sock->sk);\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tint copy, ret, ullen, offset, copied = 0;\n\tu32 abort_code;\n\n\tDEFINE_WAIT(wait);\n\n\t_enter(\",,,%zu,%d\", len, flags);\n\n\tif (flags & (MSG_OOB | MSG_TRUNC))\n\t\treturn -EOPNOTSUPP;\n\n\tullen = msg->msg_flags & MSG_CMSG_COMPAT ? 4 : sizeof(unsigned long);\n\n\ttimeo = sock_rcvtimeo(&rx->sk, flags & MSG_DONTWAIT);\n\tmsg->msg_flags |= MSG_MORE;\n\n\tlock_sock(&rx->sk);\n\n\tfor (;;) {\n\t\t/* return immediately if a client socket has no outstanding\n\t\t * calls */\n\t\tif (RB_EMPTY_ROOT(&rx->calls)) {\n\t\t\tif (copied)\n\t\t\t\tgoto out;\n\t\t\tif (rx->sk.sk_state != RXRPC_SERVER_LISTENING) {\n\t\t\t\trelease_sock(&rx->sk);\n\t\t\t\tif (continue_call)\n\t\t\t\t\trxrpc_put_call(continue_call);\n\t\t\t\treturn -ENODATA;\n\t\t\t}\n\t\t}\n\n\t\t/* get the next message on the Rx queue */\n\t\tskb = skb_peek(&rx->sk.sk_receive_queue);\n\t\tif (!skb) {\n\t\t\t/* nothing remains on the queue */\n\t\t\tif (copied &&\n\t\t\t    (msg->msg_flags & MSG_PEEK || timeo == 0))\n\t\t\t\tgoto out;\n\n\t\t\t/* wait for a message to turn up */\n\t\t\trelease_sock(&rx->sk);\n\t\t\tprepare_to_wait_exclusive(sk_sleep(&rx->sk), &wait,\n\t\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\t\tret = sock_error(&rx->sk);\n\t\t\tif (ret)\n\t\t\t\tgoto wait_error;\n\n\t\t\tif (skb_queue_empty(&rx->sk.sk_receive_queue)) {\n\t\t\t\tif (signal_pending(current))\n\t\t\t\t\tgoto wait_interrupted;\n\t\t\t\ttimeo = schedule_timeout(timeo);\n\t\t\t}\n\t\t\tfinish_wait(sk_sleep(&rx->sk), &wait);\n\t\t\tlock_sock(&rx->sk);\n\t\t\tcontinue;\n\t\t}\n\n\tpeek_next_packet:\n\t\tsp = rxrpc_skb(skb);\n\t\tcall = sp->call;\n\t\tASSERT(call != NULL);\n\n\t\t_debug(\"next pkt %s\", rxrpc_pkts[sp->hdr.type]);\n\n\t\t/* make sure we wait for the state to be updated in this call */\n\t\tspin_lock_bh(&call->lock);\n\t\tspin_unlock_bh(&call->lock);\n\n\t\tif (test_bit(RXRPC_CALL_RELEASED, &call->flags)) {\n\t\t\t_debug(\"packet from released call\");\n\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\t\tBUG();\n\t\t\trxrpc_free_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* determine whether to continue last data receive */\n\t\tif (continue_call) {\n\t\t\t_debug(\"maybe cont\");\n\t\t\tif (call != continue_call ||\n\t\t\t    skb->mark != RXRPC_SKB_MARK_DATA) {\n\t\t\t\trelease_sock(&rx->sk);\n\t\t\t\trxrpc_put_call(continue_call);\n\t\t\t\t_leave(\" = %d [noncont]\", copied);\n\t\t\t\treturn copied;\n\t\t\t}\n\t\t}\n\n\t\trxrpc_get_call(call);\n\n\t\t/* copy the peer address and timestamp */\n\t\tif (!continue_call) {\n\t\t\tif (msg->msg_name) {\n\t\t\t\tsize_t len =\n\t\t\t\t\tsizeof(call->conn->trans->peer->srx);\n\t\t\t\tmemcpy(msg->msg_name,\n\t\t\t\t       &call->conn->trans->peer->srx, len);\n\t\t\t\tmsg->msg_namelen = len;\n\t\t\t}\n\t\t\tsock_recv_ts_and_drops(msg, &rx->sk, skb);\n\t\t}\n\n\t\t/* receive the message */\n\t\tif (skb->mark != RXRPC_SKB_MARK_DATA)\n\t\t\tgoto receive_non_data_message;\n\n\t\t_debug(\"recvmsg DATA #%u { %d, %d }\",\n\t\t       ntohl(sp->hdr.seq), skb->len, sp->offset);\n\n\t\tif (!continue_call) {\n\t\t\t/* only set the control data once per recvmsg() */\n\t\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_USER_CALL_ID,\n\t\t\t\t       ullen, &call->user_call_ID);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto copy_error;\n\t\t\tASSERT(test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\n\t\t}\n\n\t\tASSERTCMP(ntohl(sp->hdr.seq), >=, call->rx_data_recv);\n\t\tASSERTCMP(ntohl(sp->hdr.seq), <=, call->rx_data_recv + 1);\n\t\tcall->rx_data_recv = ntohl(sp->hdr.seq);\n\n\t\tASSERTCMP(ntohl(sp->hdr.seq), >, call->rx_data_eaten);\n\n\t\toffset = sp->offset;\n\t\tcopy = skb->len - offset;\n\t\tif (copy > len - copied)\n\t\t\tcopy = len - copied;\n\n\t\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\t\tret = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t      msg->msg_iov, copy);\n\t\t} else {\n\t\t\tret = skb_copy_and_csum_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t       msg->msg_iov);\n\t\t\tif (ret == -EINVAL)\n\t\t\t\tgoto csum_copy_error;\n\t\t}\n\n\t\tif (ret < 0)\n\t\t\tgoto copy_error;\n\n\t\t/* handle piecemeal consumption of data packets */\n\t\t_debug(\"copied %d+%d\", copy, copied);\n\n\t\toffset += copy;\n\t\tcopied += copy;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tsp->offset = offset;\n\n\t\tif (sp->offset < skb->len) {\n\t\t\t_debug(\"buffer full\");\n\t\t\tASSERTCMP(copied, ==, len);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* we transferred the whole data packet */\n\t\tif (sp->hdr.flags & RXRPC_LAST_PACKET) {\n\t\t\t_debug(\"last\");\n\t\t\tif (call->conn->out_clientflag) {\n\t\t\t\t /* last byte of reply received */\n\t\t\t\tret = copied;\n\t\t\t\tgoto terminal_message;\n\t\t\t}\n\n\t\t\t/* last bit of request received */\n\t\t\tif (!(flags & MSG_PEEK)) {\n\t\t\t\t_debug(\"eat packet\");\n\t\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) !=\n\t\t\t\t    skb)\n\t\t\t\t\tBUG();\n\t\t\t\trxrpc_free_skb(skb);\n\t\t\t}\n\t\t\tmsg->msg_flags &= ~MSG_MORE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* move on to the next data message */\n\t\t_debug(\"next\");\n\t\tif (!continue_call)\n\t\t\tcontinue_call = sp->call;\n\t\telse\n\t\t\trxrpc_put_call(call);\n\t\tcall = NULL;\n\n\t\tif (flags & MSG_PEEK) {\n\t\t\t_debug(\"peek next\");\n\t\t\tskb = skb->next;\n\t\t\tif (skb == (struct sk_buff *) &rx->sk.sk_receive_queue)\n\t\t\t\tbreak;\n\t\t\tgoto peek_next_packet;\n\t\t}\n\n\t\t_debug(\"eat packet\");\n\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\tBUG();\n\t\trxrpc_free_skb(skb);\n\t}\n\n\t/* end of non-terminal data packet reception for the moment */\n\t_debug(\"end rcv data\");\nout:\n\trelease_sock(&rx->sk);\n\tif (call)\n\t\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d [data]\", copied);\n\treturn copied;\n\n\t/* handle non-DATA messages such as aborts, incoming connections and\n\t * final ACKs */\nreceive_non_data_message:\n\t_debug(\"non-data\");\n\n\tif (skb->mark == RXRPC_SKB_MARK_NEW_CALL) {\n\t\t_debug(\"RECV NEW CALL\");\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_NEW_CALL, 0, &abort_code);\n\t\tif (ret < 0)\n\t\t\tgoto copy_error;\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\t\tBUG();\n\t\t\trxrpc_free_skb(skb);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_USER_CALL_ID,\n\t\t       ullen, &call->user_call_ID);\n\tif (ret < 0)\n\t\tgoto copy_error;\n\tASSERT(test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\n\n\tswitch (skb->mark) {\n\tcase RXRPC_SKB_MARK_DATA:\n\t\tBUG();\n\tcase RXRPC_SKB_MARK_FINAL_ACK:\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_ACK, 0, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_BUSY:\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_BUSY, 0, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_REMOTE_ABORT:\n\t\tabort_code = call->abort_code;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_ABORT, 4, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_NET_ERROR:\n\t\t_debug(\"RECV NET ERROR %d\", sp->error);\n\t\tabort_code = sp->error;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_NET_ERROR, 4, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_LOCAL_ERROR:\n\t\t_debug(\"RECV LOCAL ERROR %d\", sp->error);\n\t\tabort_code = sp->error;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_LOCAL_ERROR, 4,\n\t\t\t       &abort_code);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tif (ret < 0)\n\t\tgoto copy_error;\n\nterminal_message:\n\t_debug(\"terminal\");\n\tmsg->msg_flags &= ~MSG_MORE;\n\tmsg->msg_flags |= MSG_EOR;\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t_net(\"free terminal skb %p\", skb);\n\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\tBUG();\n\t\trxrpc_free_skb(skb);\n\t\trxrpc_remove_user_ID(rx, call);\n\t}\n\n\trelease_sock(&rx->sk);\n\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\ncopy_error:\n\t_debug(\"copy error\");\n\trelease_sock(&rx->sk);\n\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\ncsum_copy_error:\n\t_debug(\"csum error\");\n\trelease_sock(&rx->sk);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\trxrpc_kill_skb(skb);\n\tskb_kill_datagram(&rx->sk, skb, flags);\n\trxrpc_put_call(call);\n\treturn -EAGAIN;\n\nwait_interrupted:\n\tret = sock_intr_errno(timeo);\nwait_error:\n\tfinish_wait(sk_sleep(&rx->sk), &wait);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\tif (copied)\n\t\tcopied = ret;\n\t_leave(\" = %d [waitfail %d]\", copied, ret);\n\treturn copied;\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -98,10 +98,13 @@\n \n \t\t/* copy the peer address and timestamp */\n \t\tif (!continue_call) {\n-\t\t\tif (msg->msg_name && msg->msg_namelen > 0)\n+\t\t\tif (msg->msg_name) {\n+\t\t\t\tsize_t len =\n+\t\t\t\t\tsizeof(call->conn->trans->peer->srx);\n \t\t\t\tmemcpy(msg->msg_name,\n-\t\t\t\t       &call->conn->trans->peer->srx,\n-\t\t\t\t       sizeof(call->conn->trans->peer->srx));\n+\t\t\t\t       &call->conn->trans->peer->srx, len);\n+\t\t\t\tmsg->msg_namelen = len;\n+\t\t\t}\n \t\t\tsock_recv_ts_and_drops(msg, &rx->sk, skb);\n \t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (msg->msg_name) {",
                "\t\t\t\tsize_t len =",
                "\t\t\t\t\tsizeof(call->conn->trans->peer->srx);",
                "\t\t\t\t       &call->conn->trans->peer->srx, len);",
                "\t\t\t\tmsg->msg_namelen = len;",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tif (msg->msg_name && msg->msg_namelen > 0)",
                "\t\t\t\t       &call->conn->trans->peer->srx,",
                "\t\t\t\t       sizeof(call->conn->trans->peer->srx));"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 396
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int vmci_transport_dgram_dequeue(struct kiocb *kiocb,\n\t\t\t\t\tstruct vsock_sock *vsk,\n\t\t\t\t\tstruct msghdr *msg, size_t len,\n\t\t\t\t\tint flags)\n{\n\tint err;\n\tint noblock;\n\tstruct vmci_datagram *dg;\n\tsize_t payload_len;\n\tstruct sk_buff *skb;\n\n\tnoblock = flags & MSG_DONTWAIT;\n\n\tif (flags & MSG_OOB || flags & MSG_ERRQUEUE)\n\t\treturn -EOPNOTSUPP;\n\n\tmsg->msg_namelen = 0;\n\n\t/* Retrieve the head sk_buff from the socket's receive queue. */\n\terr = 0;\n\tskb = skb_recv_datagram(&vsk->sk, flags, noblock, &err);\n\tif (err)\n\t\treturn err;\n\n\tif (!skb)\n\t\treturn -EAGAIN;\n\n\tdg = (struct vmci_datagram *)skb->data;\n\tif (!dg)\n\t\t/* err is 0, meaning we read zero bytes. */\n\t\tgoto out;\n\n\tpayload_len = dg->payload_size;\n\t/* Ensure the sk_buff matches the payload size claimed in the packet. */\n\tif (payload_len != skb->len - sizeof(*dg)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (payload_len > len) {\n\t\tpayload_len = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\t/* Place the datagram payload in the user's iovec. */\n\terr = skb_copy_datagram_iovec(skb, sizeof(*dg), msg->msg_iov,\n\t\tpayload_len);\n\tif (err)\n\t\tgoto out;\n\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_vm *vm_addr;\n\n\t\t/* Provide the address of the sender. */\n\t\tvm_addr = (struct sockaddr_vm *)msg->msg_name;\n\t\tvsock_addr_init(vm_addr, dg->src.context, dg->src.resource);\n\t\tmsg->msg_namelen = sizeof(*vm_addr);\n\t}\n\terr = payload_len;\n\nout:\n\tskb_free_datagram(&vsk->sk, skb);\n\treturn err;\n}",
        "code_after_change": "static int vmci_transport_dgram_dequeue(struct kiocb *kiocb,\n\t\t\t\t\tstruct vsock_sock *vsk,\n\t\t\t\t\tstruct msghdr *msg, size_t len,\n\t\t\t\t\tint flags)\n{\n\tint err;\n\tint noblock;\n\tstruct vmci_datagram *dg;\n\tsize_t payload_len;\n\tstruct sk_buff *skb;\n\n\tnoblock = flags & MSG_DONTWAIT;\n\n\tif (flags & MSG_OOB || flags & MSG_ERRQUEUE)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Retrieve the head sk_buff from the socket's receive queue. */\n\terr = 0;\n\tskb = skb_recv_datagram(&vsk->sk, flags, noblock, &err);\n\tif (err)\n\t\treturn err;\n\n\tif (!skb)\n\t\treturn -EAGAIN;\n\n\tdg = (struct vmci_datagram *)skb->data;\n\tif (!dg)\n\t\t/* err is 0, meaning we read zero bytes. */\n\t\tgoto out;\n\n\tpayload_len = dg->payload_size;\n\t/* Ensure the sk_buff matches the payload size claimed in the packet. */\n\tif (payload_len != skb->len - sizeof(*dg)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (payload_len > len) {\n\t\tpayload_len = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\t/* Place the datagram payload in the user's iovec. */\n\terr = skb_copy_datagram_iovec(skb, sizeof(*dg), msg->msg_iov,\n\t\tpayload_len);\n\tif (err)\n\t\tgoto out;\n\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_vm *vm_addr;\n\n\t\t/* Provide the address of the sender. */\n\t\tvm_addr = (struct sockaddr_vm *)msg->msg_name;\n\t\tvsock_addr_init(vm_addr, dg->src.context, dg->src.resource);\n\t\tmsg->msg_namelen = sizeof(*vm_addr);\n\t}\n\terr = payload_len;\n\nout:\n\tskb_free_datagram(&vsk->sk, skb);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,8 +13,6 @@\n \n \tif (flags & MSG_OOB || flags & MSG_ERRQUEUE)\n \t\treturn -EOPNOTSUPP;\n-\n-\tmsg->msg_namelen = 0;\n \n \t/* Retrieve the head sk_buff from the socket's receive queue. */\n \terr = 0;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 405
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int llc_ui_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sockaddr_llc *uaddr = (struct sockaddr_llc *)msg->msg_name;\n\tconst int nonblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb = NULL;\n\tstruct sock *sk = sock->sk;\n\tstruct llc_sock *llc = llc_sk(sk);\n\tunsigned long cpu_flags;\n\tsize_t copied = 0;\n\tu32 peek_seq = 0;\n\tu32 *seq;\n\tunsigned long used;\n\tint target;\t/* Read at least this many bytes */\n\tlong timeo;\n\n\tmsg->msg_namelen = 0;\n\n\tlock_sock(sk);\n\tcopied = -ENOTCONN;\n\tif (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\tseq = &llc->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = llc->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tcopied = 0;\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/*\n\t\t * We need to check signals first, to get correct SIGURG\n\t\t * handling. FIXME: Need to check this doesn't impact 1003.1g\n\t\t * and move it down to the bottom of the loop\n\t\t */\n\t\tif (signal_pending(current)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb) {\n\t\t\toffset = *seq;\n\t\t\tgoto found_ok_skb;\n\t\t}\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    (flags & MSG_PEEK))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (copied >= target) { /* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else\n\t\t\tsk_wait_data(sk, &timeo);\n\n\t\tif ((flags & MSG_PEEK) && peek_seq != llc->copied_seq) {\n\t\t\tnet_dbg_ratelimited(\"LLC(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = llc->copied_seq;\n\t\t}\n\t\tcontinue;\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\tint rc = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t msg->msg_iov, used);\n\t\t\tif (rc) {\n\t\t\t\t/* Exception. Bailout! */\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\t/* For non stream protcols we get one packet per recvmsg call */\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\tgoto copy_uaddr;\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t\t}\n\n\t\t/* Partial read */\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\t} while (len > 0);\n\nout:\n\trelease_sock(sk);\n\treturn copied;\ncopy_uaddr:\n\tif (uaddr != NULL && skb != NULL) {\n\t\tmemcpy(uaddr, llc_ui_skb_cb(skb), sizeof(*uaddr));\n\t\tmsg->msg_namelen = sizeof(*uaddr);\n\t}\n\tif (llc_sk(sk)->cmsg_flags)\n\t\tllc_cmsg_rcv(msg, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t}\n\n\tgoto out;\n}",
        "code_after_change": "static int llc_ui_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sockaddr_llc *uaddr = (struct sockaddr_llc *)msg->msg_name;\n\tconst int nonblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb = NULL;\n\tstruct sock *sk = sock->sk;\n\tstruct llc_sock *llc = llc_sk(sk);\n\tunsigned long cpu_flags;\n\tsize_t copied = 0;\n\tu32 peek_seq = 0;\n\tu32 *seq;\n\tunsigned long used;\n\tint target;\t/* Read at least this many bytes */\n\tlong timeo;\n\n\tlock_sock(sk);\n\tcopied = -ENOTCONN;\n\tif (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\tseq = &llc->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = llc->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tcopied = 0;\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/*\n\t\t * We need to check signals first, to get correct SIGURG\n\t\t * handling. FIXME: Need to check this doesn't impact 1003.1g\n\t\t * and move it down to the bottom of the loop\n\t\t */\n\t\tif (signal_pending(current)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb) {\n\t\t\toffset = *seq;\n\t\t\tgoto found_ok_skb;\n\t\t}\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    (flags & MSG_PEEK))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (copied >= target) { /* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else\n\t\t\tsk_wait_data(sk, &timeo);\n\n\t\tif ((flags & MSG_PEEK) && peek_seq != llc->copied_seq) {\n\t\t\tnet_dbg_ratelimited(\"LLC(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = llc->copied_seq;\n\t\t}\n\t\tcontinue;\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\tint rc = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t msg->msg_iov, used);\n\t\t\tif (rc) {\n\t\t\t\t/* Exception. Bailout! */\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\t/* For non stream protcols we get one packet per recvmsg call */\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\tgoto copy_uaddr;\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t\t}\n\n\t\t/* Partial read */\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\t} while (len > 0);\n\nout:\n\trelease_sock(sk);\n\treturn copied;\ncopy_uaddr:\n\tif (uaddr != NULL && skb != NULL) {\n\t\tmemcpy(uaddr, llc_ui_skb_cb(skb), sizeof(*uaddr));\n\t\tmsg->msg_namelen = sizeof(*uaddr);\n\t}\n\tif (llc_sk(sk)->cmsg_flags)\n\t\tllc_cmsg_rcv(msg, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t}\n\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,8 +13,6 @@\n \tunsigned long used;\n \tint target;\t/* Read at least this many bytes */\n \tlong timeo;\n-\n-\tmsg->msg_namelen = 0;\n \n \tlock_sock(sk);\n \tcopied = -ENOTCONN;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 388
    },
    {
        "cve_id": "CVE-2015-8844",
        "code_before_change": "static long restore_tm_sigcontexts(struct pt_regs *regs,\n\t\t\t\t   struct sigcontext __user *sc,\n\t\t\t\t   struct sigcontext __user *tm_sc)\n{\n#ifdef CONFIG_ALTIVEC\n\telf_vrreg_t __user *v_regs, *tm_v_regs;\n#endif\n\tunsigned long err = 0;\n\tunsigned long msr;\n#ifdef CONFIG_VSX\n\tint i;\n#endif\n\t/* copy the GPRs */\n\terr |= __copy_from_user(regs->gpr, tm_sc->gp_regs, sizeof(regs->gpr));\n\terr |= __copy_from_user(&current->thread.ckpt_regs, sc->gp_regs,\n\t\t\t\tsizeof(regs->gpr));\n\n\t/*\n\t * TFHAR is restored from the checkpointed 'wound-back' ucontext's NIP.\n\t * TEXASR was set by the signal delivery reclaim, as was TFIAR.\n\t * Users doing anything abhorrent like thread-switching w/ signals for\n\t * TM-Suspended code will have to back TEXASR/TFIAR up themselves.\n\t * For the case of getting a signal and simply returning from it,\n\t * we don't need to re-copy them here.\n\t */\n\terr |= __get_user(regs->nip, &tm_sc->gp_regs[PT_NIP]);\n\terr |= __get_user(current->thread.tm_tfhar, &sc->gp_regs[PT_NIP]);\n\n\t/* get MSR separately, transfer the LE bit if doing signal return */\n\terr |= __get_user(msr, &sc->gp_regs[PT_MSR]);\n\t/* pull in MSR TM from user context */\n\tregs->msr = (regs->msr & ~MSR_TS_MASK) | (msr & MSR_TS_MASK);\n\n\t/* pull in MSR LE from user context */\n\tregs->msr = (regs->msr & ~MSR_LE) | (msr & MSR_LE);\n\n\t/* The following non-GPR non-FPR non-VR state is also checkpointed: */\n\terr |= __get_user(regs->ctr, &tm_sc->gp_regs[PT_CTR]);\n\terr |= __get_user(regs->link, &tm_sc->gp_regs[PT_LNK]);\n\terr |= __get_user(regs->xer, &tm_sc->gp_regs[PT_XER]);\n\terr |= __get_user(regs->ccr, &tm_sc->gp_regs[PT_CCR]);\n\terr |= __get_user(current->thread.ckpt_regs.ctr,\n\t\t\t  &sc->gp_regs[PT_CTR]);\n\terr |= __get_user(current->thread.ckpt_regs.link,\n\t\t\t  &sc->gp_regs[PT_LNK]);\n\terr |= __get_user(current->thread.ckpt_regs.xer,\n\t\t\t  &sc->gp_regs[PT_XER]);\n\terr |= __get_user(current->thread.ckpt_regs.ccr,\n\t\t\t  &sc->gp_regs[PT_CCR]);\n\n\t/* These regs are not checkpointed; they can go in 'regs'. */\n\terr |= __get_user(regs->trap, &sc->gp_regs[PT_TRAP]);\n\terr |= __get_user(regs->dar, &sc->gp_regs[PT_DAR]);\n\terr |= __get_user(regs->dsisr, &sc->gp_regs[PT_DSISR]);\n\terr |= __get_user(regs->result, &sc->gp_regs[PT_RESULT]);\n\n\t/*\n\t * Do this before updating the thread state in\n\t * current->thread.fpr/vr.  That way, if we get preempted\n\t * and another task grabs the FPU/Altivec, it won't be\n\t * tempted to save the current CPU state into the thread_struct\n\t * and corrupt what we are writing there.\n\t */\n\tdiscard_lazy_cpu_state();\n\n\t/*\n\t * Force reload of FP/VEC.\n\t * This has to be done before copying stuff into current->thread.fpr/vr\n\t * for the reasons explained in the previous comment.\n\t */\n\tregs->msr &= ~(MSR_FP | MSR_FE0 | MSR_FE1 | MSR_VEC | MSR_VSX);\n\n#ifdef CONFIG_ALTIVEC\n\terr |= __get_user(v_regs, &sc->v_regs);\n\terr |= __get_user(tm_v_regs, &tm_sc->v_regs);\n\tif (err)\n\t\treturn err;\n\tif (v_regs && !access_ok(VERIFY_READ, v_regs, 34 * sizeof(vector128)))\n\t\treturn -EFAULT;\n\tif (tm_v_regs && !access_ok(VERIFY_READ,\n\t\t\t\t    tm_v_regs, 34 * sizeof(vector128)))\n\t\treturn -EFAULT;\n\t/* Copy 33 vec registers (vr0..31 and vscr) from the stack */\n\tif (v_regs != NULL && tm_v_regs != NULL && (msr & MSR_VEC) != 0) {\n\t\terr |= __copy_from_user(&current->thread.vr_state, v_regs,\n\t\t\t\t\t33 * sizeof(vector128));\n\t\terr |= __copy_from_user(&current->thread.transact_vr, tm_v_regs,\n\t\t\t\t\t33 * sizeof(vector128));\n\t}\n\telse if (current->thread.used_vr) {\n\t\tmemset(&current->thread.vr_state, 0, 33 * sizeof(vector128));\n\t\tmemset(&current->thread.transact_vr, 0, 33 * sizeof(vector128));\n\t}\n\t/* Always get VRSAVE back */\n\tif (v_regs != NULL && tm_v_regs != NULL) {\n\t\terr |= __get_user(current->thread.vrsave,\n\t\t\t\t  (u32 __user *)&v_regs[33]);\n\t\terr |= __get_user(current->thread.transact_vrsave,\n\t\t\t\t  (u32 __user *)&tm_v_regs[33]);\n\t}\n\telse {\n\t\tcurrent->thread.vrsave = 0;\n\t\tcurrent->thread.transact_vrsave = 0;\n\t}\n\tif (cpu_has_feature(CPU_FTR_ALTIVEC))\n\t\tmtspr(SPRN_VRSAVE, current->thread.vrsave);\n#endif /* CONFIG_ALTIVEC */\n\t/* restore floating point */\n\terr |= copy_fpr_from_user(current, &sc->fp_regs);\n\terr |= copy_transact_fpr_from_user(current, &tm_sc->fp_regs);\n#ifdef CONFIG_VSX\n\t/*\n\t * Get additional VSX data. Update v_regs to point after the\n\t * VMX data.  Copy VSX low doubleword from userspace to local\n\t * buffer for formatting, then into the taskstruct.\n\t */\n\tif (v_regs && ((msr & MSR_VSX) != 0)) {\n\t\tv_regs += ELF_NVRREG;\n\t\ttm_v_regs += ELF_NVRREG;\n\t\terr |= copy_vsx_from_user(current, v_regs);\n\t\terr |= copy_transact_vsx_from_user(current, tm_v_regs);\n\t} else {\n\t\tfor (i = 0; i < 32 ; i++) {\n\t\t\tcurrent->thread.fp_state.fpr[i][TS_VSRLOWOFFSET] = 0;\n\t\t\tcurrent->thread.transact_fp.fpr[i][TS_VSRLOWOFFSET] = 0;\n\t\t}\n\t}\n#endif\n\ttm_enable();\n\t/* Make sure the transaction is marked as failed */\n\tcurrent->thread.tm_texasr |= TEXASR_FS;\n\t/* This loads the checkpointed FP/VEC state, if used */\n\ttm_recheckpoint(&current->thread, msr);\n\n\t/* This loads the speculative FP/VEC state, if used */\n\tif (msr & MSR_FP) {\n\t\tdo_load_up_transact_fpu(&current->thread);\n\t\tregs->msr |= (MSR_FP | current->thread.fpexc_mode);\n\t}\n#ifdef CONFIG_ALTIVEC\n\tif (msr & MSR_VEC) {\n\t\tdo_load_up_transact_altivec(&current->thread);\n\t\tregs->msr |= MSR_VEC;\n\t}\n#endif\n\n\treturn err;\n}",
        "code_after_change": "static long restore_tm_sigcontexts(struct pt_regs *regs,\n\t\t\t\t   struct sigcontext __user *sc,\n\t\t\t\t   struct sigcontext __user *tm_sc)\n{\n#ifdef CONFIG_ALTIVEC\n\telf_vrreg_t __user *v_regs, *tm_v_regs;\n#endif\n\tunsigned long err = 0;\n\tunsigned long msr;\n#ifdef CONFIG_VSX\n\tint i;\n#endif\n\t/* copy the GPRs */\n\terr |= __copy_from_user(regs->gpr, tm_sc->gp_regs, sizeof(regs->gpr));\n\terr |= __copy_from_user(&current->thread.ckpt_regs, sc->gp_regs,\n\t\t\t\tsizeof(regs->gpr));\n\n\t/*\n\t * TFHAR is restored from the checkpointed 'wound-back' ucontext's NIP.\n\t * TEXASR was set by the signal delivery reclaim, as was TFIAR.\n\t * Users doing anything abhorrent like thread-switching w/ signals for\n\t * TM-Suspended code will have to back TEXASR/TFIAR up themselves.\n\t * For the case of getting a signal and simply returning from it,\n\t * we don't need to re-copy them here.\n\t */\n\terr |= __get_user(regs->nip, &tm_sc->gp_regs[PT_NIP]);\n\terr |= __get_user(current->thread.tm_tfhar, &sc->gp_regs[PT_NIP]);\n\n\t/* get MSR separately, transfer the LE bit if doing signal return */\n\terr |= __get_user(msr, &sc->gp_regs[PT_MSR]);\n\t/* Don't allow reserved mode. */\n\tif (MSR_TM_RESV(msr))\n\t\treturn -EINVAL;\n\n\t/* pull in MSR TM from user context */\n\tregs->msr = (regs->msr & ~MSR_TS_MASK) | (msr & MSR_TS_MASK);\n\n\t/* pull in MSR LE from user context */\n\tregs->msr = (regs->msr & ~MSR_LE) | (msr & MSR_LE);\n\n\t/* The following non-GPR non-FPR non-VR state is also checkpointed: */\n\terr |= __get_user(regs->ctr, &tm_sc->gp_regs[PT_CTR]);\n\terr |= __get_user(regs->link, &tm_sc->gp_regs[PT_LNK]);\n\terr |= __get_user(regs->xer, &tm_sc->gp_regs[PT_XER]);\n\terr |= __get_user(regs->ccr, &tm_sc->gp_regs[PT_CCR]);\n\terr |= __get_user(current->thread.ckpt_regs.ctr,\n\t\t\t  &sc->gp_regs[PT_CTR]);\n\terr |= __get_user(current->thread.ckpt_regs.link,\n\t\t\t  &sc->gp_regs[PT_LNK]);\n\terr |= __get_user(current->thread.ckpt_regs.xer,\n\t\t\t  &sc->gp_regs[PT_XER]);\n\terr |= __get_user(current->thread.ckpt_regs.ccr,\n\t\t\t  &sc->gp_regs[PT_CCR]);\n\n\t/* These regs are not checkpointed; they can go in 'regs'. */\n\terr |= __get_user(regs->trap, &sc->gp_regs[PT_TRAP]);\n\terr |= __get_user(regs->dar, &sc->gp_regs[PT_DAR]);\n\terr |= __get_user(regs->dsisr, &sc->gp_regs[PT_DSISR]);\n\terr |= __get_user(regs->result, &sc->gp_regs[PT_RESULT]);\n\n\t/*\n\t * Do this before updating the thread state in\n\t * current->thread.fpr/vr.  That way, if we get preempted\n\t * and another task grabs the FPU/Altivec, it won't be\n\t * tempted to save the current CPU state into the thread_struct\n\t * and corrupt what we are writing there.\n\t */\n\tdiscard_lazy_cpu_state();\n\n\t/*\n\t * Force reload of FP/VEC.\n\t * This has to be done before copying stuff into current->thread.fpr/vr\n\t * for the reasons explained in the previous comment.\n\t */\n\tregs->msr &= ~(MSR_FP | MSR_FE0 | MSR_FE1 | MSR_VEC | MSR_VSX);\n\n#ifdef CONFIG_ALTIVEC\n\terr |= __get_user(v_regs, &sc->v_regs);\n\terr |= __get_user(tm_v_regs, &tm_sc->v_regs);\n\tif (err)\n\t\treturn err;\n\tif (v_regs && !access_ok(VERIFY_READ, v_regs, 34 * sizeof(vector128)))\n\t\treturn -EFAULT;\n\tif (tm_v_regs && !access_ok(VERIFY_READ,\n\t\t\t\t    tm_v_regs, 34 * sizeof(vector128)))\n\t\treturn -EFAULT;\n\t/* Copy 33 vec registers (vr0..31 and vscr) from the stack */\n\tif (v_regs != NULL && tm_v_regs != NULL && (msr & MSR_VEC) != 0) {\n\t\terr |= __copy_from_user(&current->thread.vr_state, v_regs,\n\t\t\t\t\t33 * sizeof(vector128));\n\t\terr |= __copy_from_user(&current->thread.transact_vr, tm_v_regs,\n\t\t\t\t\t33 * sizeof(vector128));\n\t}\n\telse if (current->thread.used_vr) {\n\t\tmemset(&current->thread.vr_state, 0, 33 * sizeof(vector128));\n\t\tmemset(&current->thread.transact_vr, 0, 33 * sizeof(vector128));\n\t}\n\t/* Always get VRSAVE back */\n\tif (v_regs != NULL && tm_v_regs != NULL) {\n\t\terr |= __get_user(current->thread.vrsave,\n\t\t\t\t  (u32 __user *)&v_regs[33]);\n\t\terr |= __get_user(current->thread.transact_vrsave,\n\t\t\t\t  (u32 __user *)&tm_v_regs[33]);\n\t}\n\telse {\n\t\tcurrent->thread.vrsave = 0;\n\t\tcurrent->thread.transact_vrsave = 0;\n\t}\n\tif (cpu_has_feature(CPU_FTR_ALTIVEC))\n\t\tmtspr(SPRN_VRSAVE, current->thread.vrsave);\n#endif /* CONFIG_ALTIVEC */\n\t/* restore floating point */\n\terr |= copy_fpr_from_user(current, &sc->fp_regs);\n\terr |= copy_transact_fpr_from_user(current, &tm_sc->fp_regs);\n#ifdef CONFIG_VSX\n\t/*\n\t * Get additional VSX data. Update v_regs to point after the\n\t * VMX data.  Copy VSX low doubleword from userspace to local\n\t * buffer for formatting, then into the taskstruct.\n\t */\n\tif (v_regs && ((msr & MSR_VSX) != 0)) {\n\t\tv_regs += ELF_NVRREG;\n\t\ttm_v_regs += ELF_NVRREG;\n\t\terr |= copy_vsx_from_user(current, v_regs);\n\t\terr |= copy_transact_vsx_from_user(current, tm_v_regs);\n\t} else {\n\t\tfor (i = 0; i < 32 ; i++) {\n\t\t\tcurrent->thread.fp_state.fpr[i][TS_VSRLOWOFFSET] = 0;\n\t\t\tcurrent->thread.transact_fp.fpr[i][TS_VSRLOWOFFSET] = 0;\n\t\t}\n\t}\n#endif\n\ttm_enable();\n\t/* Make sure the transaction is marked as failed */\n\tcurrent->thread.tm_texasr |= TEXASR_FS;\n\t/* This loads the checkpointed FP/VEC state, if used */\n\ttm_recheckpoint(&current->thread, msr);\n\n\t/* This loads the speculative FP/VEC state, if used */\n\tif (msr & MSR_FP) {\n\t\tdo_load_up_transact_fpu(&current->thread);\n\t\tregs->msr |= (MSR_FP | current->thread.fpexc_mode);\n\t}\n#ifdef CONFIG_ALTIVEC\n\tif (msr & MSR_VEC) {\n\t\tdo_load_up_transact_altivec(&current->thread);\n\t\tregs->msr |= MSR_VEC;\n\t}\n#endif\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,6 +28,10 @@\n \n \t/* get MSR separately, transfer the LE bit if doing signal return */\n \terr |= __get_user(msr, &sc->gp_regs[PT_MSR]);\n+\t/* Don't allow reserved mode. */\n+\tif (MSR_TM_RESV(msr))\n+\t\treturn -EINVAL;\n+\n \t/* pull in MSR TM from user context */\n \tregs->msr = (regs->msr & ~MSR_TS_MASK) | (msr & MSR_TS_MASK);\n ",
        "function_modified_lines": {
            "added": [
                "\t/* Don't allow reserved mode. */",
                "\tif (MSR_TM_RESV(msr))",
                "\t\treturn -EINVAL;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The signal implementation in the Linux kernel before 4.3.5 on powerpc platforms does not check for an MSR with both the S and T bits set, which allows local users to cause a denial of service (TM Bad Thing exception and panic) via a crafted application.",
        "id": 862
    },
    {
        "cve_id": "CVE-2013-6368",
        "code_before_change": "void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data;\n\tvoid *vapic;\n\n\tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n\t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);\n\tdata = *(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr));\n\tkunmap_atomic(vapic);\n\n\tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n}",
        "code_after_change": "void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data;\n\n\tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n\t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\tkvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\tsizeof(u32));\n\n\tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n {\n \tu32 data;\n-\tvoid *vapic;\n \n \tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n \t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n@@ -9,9 +8,8 @@\n \tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n \t\treturn;\n \n-\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);\n-\tdata = *(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr));\n-\tkunmap_atomic(vapic);\n+\tkvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n+\t\t\t\tsizeof(u32));\n \n \tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n }",
        "function_modified_lines": {
            "added": [
                "\tkvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,",
                "\t\t\t\tsizeof(u32));"
            ],
            "deleted": [
                "\tvoid *vapic;",
                "\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);",
                "\tdata = *(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr));",
                "\tkunmap_atomic(vapic);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The KVM subsystem in the Linux kernel through 3.12.5 allows local users to gain privileges or cause a denial of service (system crash) via a VAPIC synchronization operation involving a page-end address.",
        "id": 340
    },
    {
        "cve_id": "CVE-2013-6368",
        "code_before_change": "void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data, tpr;\n\tint max_irr, max_isr;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tvoid *vapic;\n\n\tapic_sync_pv_eoi_to_guest(vcpu, apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\ttpr = kvm_apic_get_reg(apic, APIC_TASKPRI) & 0xff;\n\tmax_irr = apic_find_highest_irr(apic);\n\tif (max_irr < 0)\n\t\tmax_irr = 0;\n\tmax_isr = apic_find_highest_isr(apic);\n\tif (max_isr < 0)\n\t\tmax_isr = 0;\n\tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n\n\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);\n\t*(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr)) = data;\n\tkunmap_atomic(vapic);\n}",
        "code_after_change": "void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data, tpr;\n\tint max_irr, max_isr;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tapic_sync_pv_eoi_to_guest(vcpu, apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\ttpr = kvm_apic_get_reg(apic, APIC_TASKPRI) & 0xff;\n\tmax_irr = apic_find_highest_irr(apic);\n\tif (max_irr < 0)\n\t\tmax_irr = 0;\n\tmax_isr = apic_find_highest_isr(apic);\n\tif (max_isr < 0)\n\t\tmax_isr = 0;\n\tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n\n\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\tsizeof(u32));\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n \tu32 data, tpr;\n \tint max_irr, max_isr;\n \tstruct kvm_lapic *apic = vcpu->arch.apic;\n-\tvoid *vapic;\n \n \tapic_sync_pv_eoi_to_guest(vcpu, apic);\n \n@@ -19,7 +18,6 @@\n \t\tmax_isr = 0;\n \tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n \n-\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);\n-\t*(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr)) = data;\n-\tkunmap_atomic(vapic);\n+\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n+\t\t\t\tsizeof(u32));\n }",
        "function_modified_lines": {
            "added": [
                "\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,",
                "\t\t\t\tsizeof(u32));"
            ],
            "deleted": [
                "\tvoid *vapic;",
                "\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);",
                "\t*(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr)) = data;",
                "\tkunmap_atomic(vapic);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The KVM subsystem in the Linux kernel through 3.12.5 allows local users to gain privileges or cause a denial of service (system crash) via a VAPIC synchronization operation involving a page-end address.",
        "id": 341
    },
    {
        "cve_id": "CVE-2018-14619",
        "code_before_change": "static void aead_release(void *private)\n{\n\tstruct aead_tfm *tfm = private;\n\n\tcrypto_free_aead(tfm->aead);\n\tkfree(tfm);\n}",
        "code_after_change": "static void aead_release(void *private)\n{\n\tstruct aead_tfm *tfm = private;\n\n\tcrypto_free_aead(tfm->aead);\n\tcrypto_put_default_null_skcipher2();\n\tkfree(tfm);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,5 +3,6 @@\n \tstruct aead_tfm *tfm = private;\n \n \tcrypto_free_aead(tfm->aead);\n+\tcrypto_put_default_null_skcipher2();\n \tkfree(tfm);\n }",
        "function_modified_lines": {
            "added": [
                "\tcrypto_put_default_null_skcipher2();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A flaw was found in the crypto subsystem of the Linux kernel before version kernel-4.15-rc4. The \"null skcipher\" was being dropped when each af_alg_ctx was freed instead of when the aead_tfm was freed. This can cause the null skcipher to be freed while it is still in use leading to a local user being able to crash the system or possibly escalate privileges.",
        "id": 1691
    },
    {
        "cve_id": "CVE-2018-12207",
        "code_before_change": "static int kvm_debugfs_open(struct inode *inode, struct file *file,\n\t\t\t   int (*get)(void *, u64 *), int (*set)(void *, u64),\n\t\t\t   const char *fmt)\n{\n\tstruct kvm_stat_data *stat_data = (struct kvm_stat_data *)\n\t\t\t\t\t  inode->i_private;\n\n\t/* The debugfs files are a reference to the kvm struct which\n\t * is still valid when kvm_destroy_vm is called.\n\t * To avoid the race between open and the removal of the debugfs\n\t * directory we test against the users count.\n\t */\n\tif (!refcount_inc_not_zero(&stat_data->kvm->users_count))\n\t\treturn -ENOENT;\n\n\tif (simple_attr_open(inode, file, get, set, fmt)) {\n\t\tkvm_put_kvm(stat_data->kvm);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int kvm_debugfs_open(struct inode *inode, struct file *file,\n\t\t\t   int (*get)(void *, u64 *), int (*set)(void *, u64),\n\t\t\t   const char *fmt)\n{\n\tstruct kvm_stat_data *stat_data = (struct kvm_stat_data *)\n\t\t\t\t\t  inode->i_private;\n\n\t/* The debugfs files are a reference to the kvm struct which\n\t * is still valid when kvm_destroy_vm is called.\n\t * To avoid the race between open and the removal of the debugfs\n\t * directory we test against the users count.\n\t */\n\tif (!refcount_inc_not_zero(&stat_data->kvm->users_count))\n\t\treturn -ENOENT;\n\n\tif (simple_attr_open(inode, file, get,\n\t\t\t     stat_data->mode & S_IWUGO ? set : NULL,\n\t\t\t     fmt)) {\n\t\tkvm_put_kvm(stat_data->kvm);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,9 @@\n \tif (!refcount_inc_not_zero(&stat_data->kvm->users_count))\n \t\treturn -ENOENT;\n \n-\tif (simple_attr_open(inode, file, get, set, fmt)) {\n+\tif (simple_attr_open(inode, file, get,\n+\t\t\t     stat_data->mode & S_IWUGO ? set : NULL,\n+\t\t\t     fmt)) {\n \t\tkvm_put_kvm(stat_data->kvm);\n \t\treturn -ENOMEM;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (simple_attr_open(inode, file, get,",
                "\t\t\t     stat_data->mode & S_IWUGO ? set : NULL,",
                "\t\t\t     fmt)) {"
            ],
            "deleted": [
                "\tif (simple_attr_open(inode, file, get, set, fmt)) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper invalidation for page table updates by a virtual guest operating system for multiple Intel(R) Processors may allow an authenticated user to potentially enable denial of service of the host system via local access.",
        "id": 1646
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "static void sctp_asconf_param_success(struct sctp_association *asoc,\n\t\t\t\t      struct sctp_addip_param *asconf_param)\n{\n\tstruct sctp_bind_addr *bp = &asoc->base.bind_addr;\n\tunion sctp_addr_param *addr_param;\n\tstruct sctp_sockaddr_entry *saddr;\n\tstruct sctp_transport *transport;\n\tunion sctp_addr\taddr;\n\tstruct sctp_af *af;\n\n\taddr_param = (void *)asconf_param + sizeof(*asconf_param);\n\n\t/* We have checked the packet before, so we do not check again.\t*/\n\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n\taf->from_addr_param(&addr, addr_param, htons(bp->port), 0);\n\n\tswitch (asconf_param->param_hdr.type) {\n\tcase SCTP_PARAM_ADD_IP:\n\t\t/* This is always done in BH context with a socket lock\n\t\t * held, so the list can not change.\n\t\t */\n\t\tlocal_bh_disable();\n\t\tlist_for_each_entry(saddr, &bp->address_list, list) {\n\t\t\tif (sctp_cmp_addr_exact(&saddr->a, &addr))\n\t\t\t\tsaddr->state = SCTP_ADDR_SRC;\n\t\t}\n\t\tlocal_bh_enable();\n\t\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tsctp_transport_dst_release(transport);\n\t\t}\n\t\tbreak;\n\tcase SCTP_PARAM_DEL_IP:\n\t\tlocal_bh_disable();\n\t\tsctp_del_bind_addr(bp, &addr);\n\t\tif (asoc->asconf_addr_del_pending != NULL &&\n\t\t    sctp_cmp_addr_exact(asoc->asconf_addr_del_pending, &addr)) {\n\t\t\tkfree(asoc->asconf_addr_del_pending);\n\t\t\tasoc->asconf_addr_del_pending = NULL;\n\t\t}\n\t\tlocal_bh_enable();\n\t\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tsctp_transport_dst_release(transport);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}",
        "code_after_change": "static void sctp_asconf_param_success(struct sctp_association *asoc,\n\t\t\t\t      struct sctp_addip_param *asconf_param)\n{\n\tstruct sctp_bind_addr *bp = &asoc->base.bind_addr;\n\tunion sctp_addr_param *addr_param;\n\tstruct sctp_sockaddr_entry *saddr;\n\tstruct sctp_transport *transport;\n\tunion sctp_addr\taddr;\n\tstruct sctp_af *af;\n\n\taddr_param = (void *)asconf_param + sizeof(*asconf_param);\n\n\t/* We have checked the packet before, so we do not check again.\t*/\n\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n\tif (!af->from_addr_param(&addr, addr_param, htons(bp->port), 0))\n\t\treturn;\n\n\tswitch (asconf_param->param_hdr.type) {\n\tcase SCTP_PARAM_ADD_IP:\n\t\t/* This is always done in BH context with a socket lock\n\t\t * held, so the list can not change.\n\t\t */\n\t\tlocal_bh_disable();\n\t\tlist_for_each_entry(saddr, &bp->address_list, list) {\n\t\t\tif (sctp_cmp_addr_exact(&saddr->a, &addr))\n\t\t\t\tsaddr->state = SCTP_ADDR_SRC;\n\t\t}\n\t\tlocal_bh_enable();\n\t\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tsctp_transport_dst_release(transport);\n\t\t}\n\t\tbreak;\n\tcase SCTP_PARAM_DEL_IP:\n\t\tlocal_bh_disable();\n\t\tsctp_del_bind_addr(bp, &addr);\n\t\tif (asoc->asconf_addr_del_pending != NULL &&\n\t\t    sctp_cmp_addr_exact(asoc->asconf_addr_del_pending, &addr)) {\n\t\t\tkfree(asoc->asconf_addr_del_pending);\n\t\t\tasoc->asconf_addr_del_pending = NULL;\n\t\t}\n\t\tlocal_bh_enable();\n\t\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tsctp_transport_dst_release(transport);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,8 @@\n \n \t/* We have checked the packet before, so we do not check again.\t*/\n \taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n-\taf->from_addr_param(&addr, addr_param, htons(bp->port), 0);\n+\tif (!af->from_addr_param(&addr, addr_param, htons(bp->port), 0))\n+\t\treturn;\n \n \tswitch (asconf_param->param_hdr.type) {\n \tcase SCTP_PARAM_ADD_IP:",
        "function_modified_lines": {
            "added": [
                "\tif (!af->from_addr_param(&addr, addr_param, htons(bp->port), 0))",
                "\t\treturn;"
            ],
            "deleted": [
                "\taf->from_addr_param(&addr, addr_param, htons(bp->port), 0);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3035
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "static struct sctp_association *__sctp_rcv_asconf_lookup(\n\t\t\t\t\tstruct net *net,\n\t\t\t\t\tstruct sctp_chunkhdr *ch,\n\t\t\t\t\tconst union sctp_addr *laddr,\n\t\t\t\t\t__be16 peer_port,\n\t\t\t\t\tstruct sctp_transport **transportp)\n{\n\tstruct sctp_addip_chunk *asconf = (struct sctp_addip_chunk *)ch;\n\tstruct sctp_af *af;\n\tunion sctp_addr_param *param;\n\tunion sctp_addr paddr;\n\n\t/* Skip over the ADDIP header and find the Address parameter */\n\tparam = (union sctp_addr_param *)(asconf + 1);\n\n\taf = sctp_get_af_specific(param_type2af(param->p.type));\n\tif (unlikely(!af))\n\t\treturn NULL;\n\n\taf->from_addr_param(&paddr, param, peer_port, 0);\n\n\treturn __sctp_lookup_association(net, laddr, &paddr, transportp);\n}",
        "code_after_change": "static struct sctp_association *__sctp_rcv_asconf_lookup(\n\t\t\t\t\tstruct net *net,\n\t\t\t\t\tstruct sctp_chunkhdr *ch,\n\t\t\t\t\tconst union sctp_addr *laddr,\n\t\t\t\t\t__be16 peer_port,\n\t\t\t\t\tstruct sctp_transport **transportp)\n{\n\tstruct sctp_addip_chunk *asconf = (struct sctp_addip_chunk *)ch;\n\tstruct sctp_af *af;\n\tunion sctp_addr_param *param;\n\tunion sctp_addr paddr;\n\n\t/* Skip over the ADDIP header and find the Address parameter */\n\tparam = (union sctp_addr_param *)(asconf + 1);\n\n\taf = sctp_get_af_specific(param_type2af(param->p.type));\n\tif (unlikely(!af))\n\t\treturn NULL;\n\n\tif (af->from_addr_param(&paddr, param, peer_port, 0))\n\t\treturn NULL;\n\n\treturn __sctp_lookup_association(net, laddr, &paddr, transportp);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,8 @@\n \tif (unlikely(!af))\n \t\treturn NULL;\n \n-\taf->from_addr_param(&paddr, param, peer_port, 0);\n+\tif (af->from_addr_param(&paddr, param, peer_port, 0))\n+\t\treturn NULL;\n \n \treturn __sctp_lookup_association(net, laddr, &paddr, transportp);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (af->from_addr_param(&paddr, param, peer_port, 0))",
                "\t\treturn NULL;"
            ],
            "deleted": [
                "\taf->from_addr_param(&paddr, param, peer_port, 0);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3030
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static void io_req_free_batch_finish(struct io_ring_ctx *ctx,\n\t\t\t\t     struct req_batch *rb)\n{\n\tif (rb->to_free)\n\t\t__io_req_free_batch_flush(ctx, rb);\n\tif (rb->task) {\n\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\trb->task = NULL;\n\t}\n}",
        "code_after_change": "static void io_req_free_batch_finish(struct io_ring_ctx *ctx,\n\t\t\t\t     struct req_batch *rb)\n{\n\tif (rb->to_free)\n\t\t__io_req_free_batch_flush(ctx, rb);\n\tif (rb->task) {\n\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);\n\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\trb->task = NULL;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tif (rb->to_free)\n \t\t__io_req_free_batch_flush(ctx, rb);\n \tif (rb->task) {\n+\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);\n \t\tput_task_struct_many(rb->task, rb->task_refs);\n \t\trb->task = NULL;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2845
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static __latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tint node,\n\t\t\t\t\tstruct kernel_clone_args *args)\n{\n\tint pidfd = -1, retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\tstruct file *pidfile = NULL;\n\tu64 clone_flags = args->flags;\n\tstruct nsproxy *nsp = current->nsproxy;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) != nsp->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * If the new process will be in a different time namespace\n\t * do not allow it to share VM or a thread group with the forking task.\n\t */\n\tif (clone_flags & (CLONE_THREAD | CLONE_VM)) {\n\t\tif (nsp->time_ns != nsp->time_ns_for_children)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (clone_flags & CLONE_PIDFD) {\n\t\t/*\n\t\t * - CLONE_DETACHED is blocked so that we can potentially\n\t\t *   reuse it later for CLONE_PIDFD.\n\t\t * - CLONE_THREAD is blocked until someone really needs it.\n\t\t */\n\t\tif (clone_flags & (CLONE_DETACHED | CLONE_THREAD))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n\tlockdep_assert_irqs_enabled();\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (data_race(nr_threads >= max_threads))\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cputimers_init(&p->posix_cputimers);\n\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_spinlock_init(&p->mems_allowed_seq, &p->alloc_lock);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tmemset(&p->irqtrace, 0, sizeof(p->irqtrace));\n\tp->irqtrace.hardirq_disable_ip\t= _THIS_IP_;\n\tp->irqtrace.softirq_enable_ip\t= _THIS_IP_;\n\tp->softirqs_enabled\t\t= 1;\n\tp->softirq_context\t\t= 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, args->stack, args->stack_size, p, args->tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children, args->set_tid,\n\t\t\t\targs->set_tid_size);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n\t/*\n\t * This has to happen after we've potentially unshared the file\n\t * descriptor table (so that the pidfd doesn't leak into the child\n\t * if the fd table isn't shared).\n\t */\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tretval = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\t\tif (retval < 0)\n\t\t\tgoto bad_fork_free_pid;\n\n\t\tpidfd = retval;\n\n\t\tpidfile = anon_inode_getfile(\"[pidfd]\", &pidfd_fops, pid,\n\t\t\t\t\t      O_RDWR | O_CLOEXEC);\n\t\tif (IS_ERR(pidfile)) {\n\t\t\tput_unused_fd(pidfd);\n\t\t\tretval = PTR_ERR(pidfile);\n\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t\tget_pid(pid);\t/* held by pidfile now */\n\n\t\tretval = put_user(pidfd, args->pidfd);\n\t\tif (retval)\n\t\t\tgoto bad_fork_put_pidfd;\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n\tfutex_init_task(p);\n\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_tsk_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = args->exit_signal;\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p, args);\n\tif (retval)\n\t\tgoto bad_fork_put_pidfd;\n\n\t/*\n\t * From this point on we must avoid any synchronous user-space\n\t * communication until we take the tasklist-lock. In particular, we do\n\t * not want user-space to be able to predict the process start-time by\n\t * stalling fork(2) after we recorded the start_time but before it is\n\t * visible to the system.\n\t */\n\n\tp->start_time = ktime_get_ns();\n\tp->start_boottime = ktime_get_boottime_ns();\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* past the last point of failure */\n\tif (pidfile)\n\t\tfd_install(pidfd, pidfile);\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\trefcount_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tsched_post_fork(p);\n\tcgroup_post_fork(p, args);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p, args);\nbad_fork_put_pidfd:\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tfput(pidfile);\n\t\tput_unused_fd(pidfd);\n\t}\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm) {\n\t\tmm_clear_owner(p->mm, p);\n\t\tmmput(p->mm);\n\t}\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tdelayed_free_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "code_after_change": "static __latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tint node,\n\t\t\t\t\tstruct kernel_clone_args *args)\n{\n\tint pidfd = -1, retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\tstruct file *pidfile = NULL;\n\tu64 clone_flags = args->flags;\n\tstruct nsproxy *nsp = current->nsproxy;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) != nsp->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * If the new process will be in a different time namespace\n\t * do not allow it to share VM or a thread group with the forking task.\n\t */\n\tif (clone_flags & (CLONE_THREAD | CLONE_VM)) {\n\t\tif (nsp->time_ns != nsp->time_ns_for_children)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (clone_flags & CLONE_PIDFD) {\n\t\t/*\n\t\t * - CLONE_DETACHED is blocked so that we can potentially\n\t\t *   reuse it later for CLONE_PIDFD.\n\t\t * - CLONE_THREAD is blocked until someone really needs it.\n\t\t */\n\t\tif (clone_flags & (CLONE_DETACHED | CLONE_THREAD))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n\tlockdep_assert_irqs_enabled();\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (data_race(nr_threads >= max_threads))\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#ifdef CONFIG_IO_URING\n\tp->io_uring = NULL;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cputimers_init(&p->posix_cputimers);\n\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_spinlock_init(&p->mems_allowed_seq, &p->alloc_lock);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tmemset(&p->irqtrace, 0, sizeof(p->irqtrace));\n\tp->irqtrace.hardirq_disable_ip\t= _THIS_IP_;\n\tp->irqtrace.softirq_enable_ip\t= _THIS_IP_;\n\tp->softirqs_enabled\t\t= 1;\n\tp->softirq_context\t\t= 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, args->stack, args->stack_size, p, args->tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children, args->set_tid,\n\t\t\t\targs->set_tid_size);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n\t/*\n\t * This has to happen after we've potentially unshared the file\n\t * descriptor table (so that the pidfd doesn't leak into the child\n\t * if the fd table isn't shared).\n\t */\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tretval = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\t\tif (retval < 0)\n\t\t\tgoto bad_fork_free_pid;\n\n\t\tpidfd = retval;\n\n\t\tpidfile = anon_inode_getfile(\"[pidfd]\", &pidfd_fops, pid,\n\t\t\t\t\t      O_RDWR | O_CLOEXEC);\n\t\tif (IS_ERR(pidfile)) {\n\t\t\tput_unused_fd(pidfd);\n\t\t\tretval = PTR_ERR(pidfile);\n\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t\tget_pid(pid);\t/* held by pidfile now */\n\n\t\tretval = put_user(pidfd, args->pidfd);\n\t\tif (retval)\n\t\t\tgoto bad_fork_put_pidfd;\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n\tfutex_init_task(p);\n\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_tsk_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = args->exit_signal;\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p, args);\n\tif (retval)\n\t\tgoto bad_fork_put_pidfd;\n\n\t/*\n\t * From this point on we must avoid any synchronous user-space\n\t * communication until we take the tasklist-lock. In particular, we do\n\t * not want user-space to be able to predict the process start-time by\n\t * stalling fork(2) after we recorded the start_time but before it is\n\t * visible to the system.\n\t */\n\n\tp->start_time = ktime_get_ns();\n\tp->start_boottime = ktime_get_boottime_ns();\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* past the last point of failure */\n\tif (pidfile)\n\t\tfd_install(pidfd, pidfile);\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\trefcount_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tsched_post_fork(p);\n\tcgroup_post_fork(p, args);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p, args);\nbad_fork_put_pidfd:\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tfput(pidfile);\n\t\tput_unused_fd(pidfd);\n\t}\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm) {\n\t\tmm_clear_owner(p->mm, p);\n\t\tmmput(p->mm);\n\t}\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tdelayed_free_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "patch": "--- code before\n+++ code after\n@@ -161,6 +161,10 @@\n \tseqcount_init(&p->vtime.seqcount);\n \tp->vtime.starttime = 0;\n \tp->vtime.state = VTIME_INACTIVE;\n+#endif\n+\n+#ifdef CONFIG_IO_URING\n+\tp->io_uring = NULL;\n #endif\n \n #if defined(SPLIT_RSS_COUNTING)",
        "function_modified_lines": {
            "added": [
                "#endif",
                "",
                "#ifdef CONFIG_IO_URING",
                "\tp->io_uring = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2859
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "\nSYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const sigset_t __user *, sig,\n\t\tsize_t, sigsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tif (!list_empty_careful(&ctx->cq_overflow_list))\n\t\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sqo_wait);\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit, f.file, fd);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, sigsz);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}",
        "code_after_change": "\nSYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const sigset_t __user *, sig,\n\t\tsize_t, sigsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tif (!list_empty_careful(&ctx->cq_overflow_list))\n\t\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sqo_wait);\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(f.file);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, sigsz);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,8 +39,11 @@\n \t\t\twake_up(&ctx->sqo_wait);\n \t\tsubmitted = to_submit;\n \t} else if (to_submit) {\n+\t\tret = io_uring_add_task_file(f.file);\n+\t\tif (unlikely(ret))\n+\t\t\tgoto out;\n \t\tmutex_lock(&ctx->uring_lock);\n-\t\tsubmitted = io_submit_sqes(ctx, to_submit, f.file, fd);\n+\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n \t\tmutex_unlock(&ctx->uring_lock);\n \n \t\tif (submitted != to_submit)",
        "function_modified_lines": {
            "added": [
                "\t\tret = io_uring_add_task_file(f.file);",
                "\t\tif (unlikely(ret))",
                "\t\t\tgoto out;",
                "\t\tsubmitted = io_submit_sqes(ctx, to_submit);"
            ],
            "deleted": [
                "\t\tsubmitted = io_submit_sqes(ctx, to_submit, f.file, fd);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2856
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static void io_req_drop_files(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\tlist_del(&req->inflight_entry);\n\tif (waitqueue_active(&ctx->inflight_wait))\n\t\twake_up(&ctx->inflight_wait);\n\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\treq->flags &= ~REQ_F_INFLIGHT;\n\treq->work.files = NULL;\n}",
        "code_after_change": "static void io_req_drop_files(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\tlist_del(&req->inflight_entry);\n\tif (waitqueue_active(&ctx->inflight_wait))\n\t\twake_up(&ctx->inflight_wait);\n\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\treq->flags &= ~REQ_F_INFLIGHT;\n\tput_files_struct(req->work.files);\n\treq->work.files = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,5 +9,6 @@\n \t\twake_up(&ctx->inflight_wait);\n \tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n \treq->flags &= ~REQ_F_INFLIGHT;\n+\tput_files_struct(req->work.files);\n \treq->work.files = NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tput_files_struct(req->work.files);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2855
    },
    {
        "cve_id": "CVE-2017-18509",
        "code_before_change": "int ip6_mroute_getsockopt(struct sock *sk, int optname, char __user *optval,\n\t\t\t  int __user *optlen)\n{\n\tint olr;\n\tint val;\n\tstruct net *net = sock_net(sk);\n\tstruct mr6_table *mrt;\n\n\tmrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);\n\tif (!mrt)\n\t\treturn -ENOENT;\n\n\tswitch (optname) {\n\tcase MRT6_VERSION:\n\t\tval = 0x0305;\n\t\tbreak;\n#ifdef CONFIG_IPV6_PIMSM_V2\n\tcase MRT6_PIM:\n\t\tval = mrt->mroute_do_pim;\n\t\tbreak;\n#endif\n\tcase MRT6_ASSERT:\n\t\tval = mrt->mroute_do_assert;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (get_user(olr, optlen))\n\t\treturn -EFAULT;\n\n\tolr = min_t(int, olr, sizeof(int));\n\tif (olr < 0)\n\t\treturn -EINVAL;\n\n\tif (put_user(olr, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, olr))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "int ip6_mroute_getsockopt(struct sock *sk, int optname, char __user *optval,\n\t\t\t  int __user *optlen)\n{\n\tint olr;\n\tint val;\n\tstruct net *net = sock_net(sk);\n\tstruct mr6_table *mrt;\n\n\tif (sk->sk_type != SOCK_RAW ||\n\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\treturn -EOPNOTSUPP;\n\n\tmrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);\n\tif (!mrt)\n\t\treturn -ENOENT;\n\n\tswitch (optname) {\n\tcase MRT6_VERSION:\n\t\tval = 0x0305;\n\t\tbreak;\n#ifdef CONFIG_IPV6_PIMSM_V2\n\tcase MRT6_PIM:\n\t\tval = mrt->mroute_do_pim;\n\t\tbreak;\n#endif\n\tcase MRT6_ASSERT:\n\t\tval = mrt->mroute_do_assert;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (get_user(olr, optlen))\n\t\treturn -EFAULT;\n\n\tolr = min_t(int, olr, sizeof(int));\n\tif (olr < 0)\n\t\treturn -EINVAL;\n\n\tif (put_user(olr, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, olr))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,10 @@\n \tint val;\n \tstruct net *net = sock_net(sk);\n \tstruct mr6_table *mrt;\n+\n+\tif (sk->sk_type != SOCK_RAW ||\n+\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n+\t\treturn -EOPNOTSUPP;\n \n \tmrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);\n \tif (!mrt)",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (sk->sk_type != SOCK_RAW ||",
                "\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)",
                "\t\treturn -EOPNOTSUPP;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue was discovered in net/ipv6/ip6mr.c in the Linux kernel before 4.11. By setting a specific socket option, an attacker can control a pointer in kernel land and cause an inet_csk_listen_stop general protection fault, or potentially execute arbitrary code under certain circumstances. The issue can be triggered as root (e.g., inside a default LXC container or with the CAP_NET_ADMIN capability) or after namespace unsharing. This occurs because sk_type and protocol are not checked in the appropriate part of the ip6_mroute_* functions. NOTE: this affects Linux distributions that use 4.9.x longterm kernels before 4.9.187.",
        "id": 1435
    },
    {
        "cve_id": "CVE-2013-2888",
        "code_before_change": "static void hid_close_report(struct hid_device *device)\n{\n\tunsigned i, j;\n\n\tfor (i = 0; i < HID_REPORT_TYPES; i++) {\n\t\tstruct hid_report_enum *report_enum = device->report_enum + i;\n\n\t\tfor (j = 0; j < 256; j++) {\n\t\t\tstruct hid_report *report = report_enum->report_id_hash[j];\n\t\t\tif (report)\n\t\t\t\thid_free_report(report);\n\t\t}\n\t\tmemset(report_enum, 0, sizeof(*report_enum));\n\t\tINIT_LIST_HEAD(&report_enum->report_list);\n\t}\n\n\tkfree(device->rdesc);\n\tdevice->rdesc = NULL;\n\tdevice->rsize = 0;\n\n\tkfree(device->collection);\n\tdevice->collection = NULL;\n\tdevice->collection_size = 0;\n\tdevice->maxcollection = 0;\n\tdevice->maxapplication = 0;\n\n\tdevice->status &= ~HID_STAT_PARSED;\n}",
        "code_after_change": "static void hid_close_report(struct hid_device *device)\n{\n\tunsigned i, j;\n\n\tfor (i = 0; i < HID_REPORT_TYPES; i++) {\n\t\tstruct hid_report_enum *report_enum = device->report_enum + i;\n\n\t\tfor (j = 0; j < HID_MAX_IDS; j++) {\n\t\t\tstruct hid_report *report = report_enum->report_id_hash[j];\n\t\t\tif (report)\n\t\t\t\thid_free_report(report);\n\t\t}\n\t\tmemset(report_enum, 0, sizeof(*report_enum));\n\t\tINIT_LIST_HEAD(&report_enum->report_list);\n\t}\n\n\tkfree(device->rdesc);\n\tdevice->rdesc = NULL;\n\tdevice->rsize = 0;\n\n\tkfree(device->collection);\n\tdevice->collection = NULL;\n\tdevice->collection_size = 0;\n\tdevice->maxcollection = 0;\n\tdevice->maxapplication = 0;\n\n\tdevice->status &= ~HID_STAT_PARSED;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tfor (i = 0; i < HID_REPORT_TYPES; i++) {\n \t\tstruct hid_report_enum *report_enum = device->report_enum + i;\n \n-\t\tfor (j = 0; j < 256; j++) {\n+\t\tfor (j = 0; j < HID_MAX_IDS; j++) {\n \t\t\tstruct hid_report *report = report_enum->report_id_hash[j];\n \t\t\tif (report)\n \t\t\t\thid_free_report(report);",
        "function_modified_lines": {
            "added": [
                "\t\tfor (j = 0; j < HID_MAX_IDS; j++) {"
            ],
            "deleted": [
                "\t\tfor (j = 0; j < 256; j++) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Multiple array index errors in drivers/hid/hid-core.c in the Human Interface Device (HID) subsystem in the Linux kernel through 3.11 allow physically proximate attackers to execute arbitrary code or cause a denial of service (heap memory corruption) via a crafted device that provides an invalid Report ID.",
        "id": 244
    },
    {
        "cve_id": "CVE-2013-7263",
        "code_before_change": "int udpv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(struct sockaddr_in6);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, sizeof(struct udphdr), msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *) msg->msg_name;\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t}\n\n\t}\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\t}\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t} else {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\tif (noblock)\n\t\treturn -EAGAIN;\n\n\t/* starting over for a new packet */\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}",
        "code_after_change": "int udpv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, sizeof(struct udphdr), msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *) msg->msg_name;\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t}\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\t}\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t} else {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\tif (noblock)\n\t\treturn -EAGAIN;\n\n\t/* starting over for a new packet */\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,9 +11,6 @@\n \tint is_udplite = IS_UDPLITE(sk);\n \tint is_udp4;\n \tbool slow;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(struct sockaddr_in6);\n \n \tif (flags & MSG_ERRQUEUE)\n \t\treturn ipv6_recv_error(sk, msg, len);\n@@ -100,7 +97,7 @@\n \t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n \t\t\t\t\t\t    IP6CB(skb)->iif);\n \t\t}\n-\n+\t\t*addr_len = sizeof(*sin6);\n \t}\n \tif (is_udp4) {\n \t\tif (inet->cmsg_flags)",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin6);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(struct sockaddr_in6);",
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel before 3.12.4 updates certain length values before ensuring that associated data structures have been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call, related to net/ipv4/ping.c, net/ipv4/raw.c, net/ipv4/udp.c, net/ipv6/raw.c, and net/ipv6/udp.c.",
        "id": 363
    },
    {
        "cve_id": "CVE-2013-7263",
        "code_before_change": "static int raw_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = ip_recv_error(sk, msg, len);\n\t\tgoto out;\n\t}\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}",
        "code_after_change": "static int raw_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = ip_recv_error(sk, msg, len);\n\t\tgoto out;\n\t}\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,6 @@\n \n \tif (flags & MSG_OOB)\n \t\tgoto out;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(*sin);\n \n \tif (flags & MSG_ERRQUEUE) {\n \t\terr = ip_recv_error(sk, msg, len);\n@@ -40,6 +37,7 @@\n \t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n \t\tsin->sin_port = 0;\n \t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n+\t\t*addr_len = sizeof(*sin);\n \t}\n \tif (inet->cmsg_flags)\n \t\tip_cmsg_recv(msg, skb);",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(*sin);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel before 3.12.4 updates certain length values before ensuring that associated data structures have been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call, related to net/ipv4/ping.c, net/ipv4/raw.c, net/ipv4/udp.c, net/ipv6/raw.c, and net/ipv6/udp.c.",
        "id": 360
    },
    {
        "cve_id": "CVE-2013-7263",
        "code_before_change": "static int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "code_after_change": "static int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,6 @@\n \n \tif (flags & MSG_OOB)\n \t\tgoto out;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(*sin);\n \n \tskb = skb_recv_datagram(sk, flags, noblock, &err);\n \tif (!skb)\n@@ -35,6 +32,7 @@\n \t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n \t\tsin->sin_port = 0;\n \t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n+\t\t*addr_len = sizeof(*sin);\n \t}\n \tif (inet->cmsg_flags)\n \t\tip_cmsg_recv(msg, skb);",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(*sin);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel before 3.12.4 updates certain length values before ensuring that associated data structures have been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call, related to net/ipv4/ping.c, net/ipv4/raw.c, net/ipv4/udp.c, net/ipv6/raw.c, and net/ipv6/udp.c.",
        "id": 364
    },
    {
        "cve_id": "CVE-2018-18021",
        "code_before_change": "static int get_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/*\n\t * Because the kvm_regs structure is a mix of 32, 64 and\n\t * 128bit fields, we index it as if it was a 32bit\n\t * array. Hence below, nr_regs is the number of entries, and\n\t * off the index in the \"array\".\n\t */\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\tu32 off;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "code_after_change": "static int get_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/*\n\t * Because the kvm_regs structure is a mix of 32, 64 and\n\t * 128bit fields, we index it as if it was a 32bit\n\t * array. Hence below, nr_regs is the number of entries, and\n\t * off the index in the \"array\".\n\t */\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\tu32 off;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (validate_core_offset(reg))\n\t\treturn -EINVAL;\n\n\tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,9 @@\n \t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n \t\treturn -ENOENT;\n \n+\tif (validate_core_offset(reg))\n+\t\treturn -EINVAL;\n+\n \tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n \t\treturn -EFAULT;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (validate_core_offset(reg))",
                "\t\treturn -EINVAL;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "arch/arm64/kvm/guest.c in KVM in the Linux kernel before 4.18.12 on the arm64 platform mishandles the KVM_SET_ON_REG ioctl. This is exploitable by attackers who can create virtual machines. An attacker can arbitrarily redirect the hypervisor flow of control (with full register control). An attacker can also cause a denial of service (hypervisor panic) via an illegal exception return. This occurs because of insufficient restrictions on userspace access to the core register file, and because PSTATE.M validation does not prevent unintended execution modes.",
        "id": 1729
    },
    {
        "cve_id": "CVE-2017-18200",
        "code_before_change": "int f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range)\n{\n\t__u64 start = F2FS_BYTES_TO_BLK(range->start);\n\t__u64 end = start + F2FS_BYTES_TO_BLK(range->len) - 1;\n\tunsigned int start_segno, end_segno;\n\tstruct cp_control cpc;\n\tint err = 0;\n\n\tif (start >= MAX_BLKADDR(sbi) || range->len < sbi->blocksize)\n\t\treturn -EINVAL;\n\n\tcpc.trimmed = 0;\n\tif (end <= MAIN_BLKADDR(sbi))\n\t\tgoto out;\n\n\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\"Found FS corruption, run fsck to fix.\");\n\t\tgoto out;\n\t}\n\n\t/* start/end segment number in main_area */\n\tstart_segno = (start <= MAIN_BLKADDR(sbi)) ? 0 : GET_SEGNO(sbi, start);\n\tend_segno = (end >= MAX_BLKADDR(sbi)) ? MAIN_SEGS(sbi) - 1 :\n\t\t\t\t\t\tGET_SEGNO(sbi, end);\n\tcpc.reason = CP_DISCARD;\n\tcpc.trim_minlen = max_t(__u64, 1, F2FS_BYTES_TO_BLK(range->minlen));\n\n\t/* do checkpoint to issue discard commands safely */\n\tfor (; start_segno <= end_segno; start_segno = cpc.trim_end + 1) {\n\t\tcpc.trim_start = start_segno;\n\n\t\tif (sbi->discard_blks == 0)\n\t\t\tbreak;\n\t\telse if (sbi->discard_blks < BATCHED_TRIM_BLOCKS(sbi))\n\t\t\tcpc.trim_end = end_segno;\n\t\telse\n\t\t\tcpc.trim_end = min_t(unsigned int,\n\t\t\t\trounddown(start_segno +\n\t\t\t\tBATCHED_TRIM_SEGMENTS(sbi),\n\t\t\t\tsbi->segs_per_sec) - 1, end_segno);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\t/* It's time to issue all the filed discards */\n\tmark_discard_range_all(sbi);\n\tf2fs_wait_discard_bios(sbi);\nout:\n\trange->len = F2FS_BLK_TO_BYTES(cpc.trimmed);\n\treturn err;\n}",
        "code_after_change": "int f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range)\n{\n\t__u64 start = F2FS_BYTES_TO_BLK(range->start);\n\t__u64 end = start + F2FS_BYTES_TO_BLK(range->len) - 1;\n\tunsigned int start_segno, end_segno;\n\tstruct cp_control cpc;\n\tint err = 0;\n\n\tif (start >= MAX_BLKADDR(sbi) || range->len < sbi->blocksize)\n\t\treturn -EINVAL;\n\n\tcpc.trimmed = 0;\n\tif (end <= MAIN_BLKADDR(sbi))\n\t\tgoto out;\n\n\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {\n\t\tf2fs_msg(sbi->sb, KERN_WARNING,\n\t\t\t\"Found FS corruption, run fsck to fix.\");\n\t\tgoto out;\n\t}\n\n\t/* start/end segment number in main_area */\n\tstart_segno = (start <= MAIN_BLKADDR(sbi)) ? 0 : GET_SEGNO(sbi, start);\n\tend_segno = (end >= MAX_BLKADDR(sbi)) ? MAIN_SEGS(sbi) - 1 :\n\t\t\t\t\t\tGET_SEGNO(sbi, end);\n\tcpc.reason = CP_DISCARD;\n\tcpc.trim_minlen = max_t(__u64, 1, F2FS_BYTES_TO_BLK(range->minlen));\n\n\t/* do checkpoint to issue discard commands safely */\n\tfor (; start_segno <= end_segno; start_segno = cpc.trim_end + 1) {\n\t\tcpc.trim_start = start_segno;\n\n\t\tif (sbi->discard_blks == 0)\n\t\t\tbreak;\n\t\telse if (sbi->discard_blks < BATCHED_TRIM_BLOCKS(sbi))\n\t\t\tcpc.trim_end = end_segno;\n\t\telse\n\t\t\tcpc.trim_end = min_t(unsigned int,\n\t\t\t\trounddown(start_segno +\n\t\t\t\tBATCHED_TRIM_SEGMENTS(sbi),\n\t\t\t\tsbi->segs_per_sec) - 1, end_segno);\n\n\t\tmutex_lock(&sbi->gc_mutex);\n\t\terr = write_checkpoint(sbi, &cpc);\n\t\tmutex_unlock(&sbi->gc_mutex);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\t/* It's time to issue all the filed discards */\n\tmark_discard_range_all(sbi);\n\tf2fs_wait_discard_bios(sbi, false);\nout:\n\trange->len = F2FS_BLK_TO_BYTES(cpc.trimmed);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -50,7 +50,7 @@\n \t}\n \t/* It's time to issue all the filed discards */\n \tmark_discard_range_all(sbi);\n-\tf2fs_wait_discard_bios(sbi);\n+\tf2fs_wait_discard_bios(sbi, false);\n out:\n \trange->len = F2FS_BLK_TO_BYTES(cpc.trimmed);\n \treturn err;",
        "function_modified_lines": {
            "added": [
                "\tf2fs_wait_discard_bios(sbi, false);"
            ],
            "deleted": [
                "\tf2fs_wait_discard_bios(sbi);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The f2fs implementation in the Linux kernel before 4.14 mishandles reference counts associated with f2fs_wait_discard_bios calls, which allows local users to cause a denial of service (BUG), as demonstrated by fstrim.",
        "id": 1395
    },
    {
        "cve_id": "CVE-2015-2672",
        "code_before_change": "static inline int xsave_state(struct xsave_struct *fx, u64 mask)\n{\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\tint err = 0;\n\n\t/*\n\t * If xsaves is enabled, xsaves replaces xsaveopt because\n\t * it supports compact format and supervisor states in addition to\n\t * modified optimization in xsaveopt.\n\t *\n\t * Otherwise, if xsaveopt is enabled, xsaveopt replaces xsave\n\t * because xsaveopt supports modified optimization which is not\n\t * supported by xsave.\n\t *\n\t * If none of xsaves and xsaveopt is enabled, use xsave.\n\t */\n\talternative_input_2(\n\t\t\"1:\"XSAVE,\n\t\t\"1:\"XSAVEOPT,\n\t\tX86_FEATURE_XSAVEOPT,\n\t\t\"1:\"XSAVES,\n\t\tX86_FEATURE_XSAVES,\n\t\t[fx] \"D\" (fx), \"a\" (lmask), \"d\" (hmask) :\n\t\t\"memory\");\n\tasm volatile(\"2:\\n\\t\"\n\t\t     xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "code_after_change": "static inline int xsave_state(struct xsave_struct *fx, u64 mask)\n{\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\tint err = 0;\n\n\t/*\n\t * If xsaves is enabled, xsaves replaces xsaveopt because\n\t * it supports compact format and supervisor states in addition to\n\t * modified optimization in xsaveopt.\n\t *\n\t * Otherwise, if xsaveopt is enabled, xsaveopt replaces xsave\n\t * because xsaveopt supports modified optimization which is not\n\t * supported by xsave.\n\t *\n\t * If none of xsaves and xsaveopt is enabled, use xsave.\n\t */\n\talternative_input_2(\n\t\t\"1:\"XSAVE,\n\t\tXSAVEOPT,\n\t\tX86_FEATURE_XSAVEOPT,\n\t\tXSAVES,\n\t\tX86_FEATURE_XSAVES,\n\t\t[fx] \"D\" (fx), \"a\" (lmask), \"d\" (hmask) :\n\t\t\"memory\");\n\tasm volatile(\"2:\\n\\t\"\n\t\t     xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,9 +17,9 @@\n \t */\n \talternative_input_2(\n \t\t\"1:\"XSAVE,\n-\t\t\"1:\"XSAVEOPT,\n+\t\tXSAVEOPT,\n \t\tX86_FEATURE_XSAVEOPT,\n-\t\t\"1:\"XSAVES,\n+\t\tXSAVES,\n \t\tX86_FEATURE_XSAVES,\n \t\t[fx] \"D\" (fx), \"a\" (lmask), \"d\" (hmask) :\n \t\t\"memory\");",
        "function_modified_lines": {
            "added": [
                "\t\tXSAVEOPT,",
                "\t\tXSAVES,"
            ],
            "deleted": [
                "\t\t\"1:\"XSAVEOPT,",
                "\t\t\"1:\"XSAVES,"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The xsave/xrstor implementation in arch/x86/include/asm/xsave.h in the Linux kernel before 3.19.2 creates certain .altinstr_replacement pointers and consequently does not provide any protection against instruction faulting, which allows local users to cause a denial of service (panic) by triggering a fault, as demonstrated by an unaligned memory operand or a non-canonical address memory operand.",
        "id": 743
    },
    {
        "cve_id": "CVE-2015-2672",
        "code_before_change": "static inline int xrstor_state(struct xsave_struct *fx, u64 mask)\n{\n\tint err = 0;\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\n\t/*\n\t * Use xrstors to restore context if it is enabled. xrstors supports\n\t * compacted format of xsave area which is not supported by xrstor.\n\t */\n\talternative_input(\n\t\t\"1: \" XRSTOR,\n\t\t\"1: \" XRSTORS,\n\t\tX86_FEATURE_XSAVES,\n\t\t\"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n\t\t: \"memory\");\n\n\tasm volatile(\"2:\\n\"\n\t\t     xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "code_after_change": "static inline int xrstor_state(struct xsave_struct *fx, u64 mask)\n{\n\tint err = 0;\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\n\t/*\n\t * Use xrstors to restore context if it is enabled. xrstors supports\n\t * compacted format of xsave area which is not supported by xrstor.\n\t */\n\talternative_input(\n\t\t\"1: \" XRSTOR,\n\t\tXRSTORS,\n\t\tX86_FEATURE_XSAVES,\n\t\t\"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n\t\t: \"memory\");\n\n\tasm volatile(\"2:\\n\"\n\t\t     xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,7 @@\n \t */\n \talternative_input(\n \t\t\"1: \" XRSTOR,\n-\t\t\"1: \" XRSTORS,\n+\t\tXRSTORS,\n \t\tX86_FEATURE_XSAVES,\n \t\t\"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n \t\t: \"memory\");",
        "function_modified_lines": {
            "added": [
                "\t\tXRSTORS,"
            ],
            "deleted": [
                "\t\t\"1: \" XRSTORS,"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The xsave/xrstor implementation in arch/x86/include/asm/xsave.h in the Linux kernel before 3.19.2 creates certain .altinstr_replacement pointers and consequently does not provide any protection against instruction faulting, which allows local users to cause a denial of service (panic) by triggering a fault, as demonstrated by an unaligned memory operand or a non-canonical address memory operand.",
        "id": 744
    },
    {
        "cve_id": "CVE-2017-16538",
        "code_before_change": "static int lme2510_identify_state(struct dvb_usb_device *d, const char **name)\n{\n\tstruct lme2510_state *st = d->priv;\n\n\tusb_reset_configuration(d->udev);\n\n\tusb_set_interface(d->udev,\n\t\td->props->bInterfaceNumber, 1);\n\n\tst->dvb_usb_lme2510_firmware = dvb_usb_lme2510_firmware;\n\n\tif (lme2510_return_status(d) == 0x44) {\n\t\t*name = lme_firmware_switch(d, 0);\n\t\treturn COLD;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int lme2510_identify_state(struct dvb_usb_device *d, const char **name)\n{\n\tstruct lme2510_state *st = d->priv;\n\tint status;\n\n\tusb_reset_configuration(d->udev);\n\n\tusb_set_interface(d->udev,\n\t\td->props->bInterfaceNumber, 1);\n\n\tst->dvb_usb_lme2510_firmware = dvb_usb_lme2510_firmware;\n\n\tstatus = lme2510_return_status(d);\n\tif (status == 0x44) {\n\t\t*name = lme_firmware_switch(d, 0);\n\t\treturn COLD;\n\t}\n\n\tif (status != 0x47)\n\t\treturn -EINVAL;\n\n\treturn WARM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,7 @@\n static int lme2510_identify_state(struct dvb_usb_device *d, const char **name)\n {\n \tstruct lme2510_state *st = d->priv;\n+\tint status;\n \n \tusb_reset_configuration(d->udev);\n \n@@ -9,10 +10,14 @@\n \n \tst->dvb_usb_lme2510_firmware = dvb_usb_lme2510_firmware;\n \n-\tif (lme2510_return_status(d) == 0x44) {\n+\tstatus = lme2510_return_status(d);\n+\tif (status == 0x44) {\n \t\t*name = lme_firmware_switch(d, 0);\n \t\treturn COLD;\n \t}\n \n-\treturn 0;\n+\tif (status != 0x47)\n+\t\treturn -EINVAL;\n+\n+\treturn WARM;\n }",
        "function_modified_lines": {
            "added": [
                "\tint status;",
                "\tstatus = lme2510_return_status(d);",
                "\tif (status == 0x44) {",
                "\tif (status != 0x47)",
                "\t\treturn -EINVAL;",
                "",
                "\treturn WARM;"
            ],
            "deleted": [
                "\tif (lme2510_return_status(d) == 0x44) {",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "drivers/media/usb/dvb-usb-v2/lmedm04.c in the Linux kernel through 4.13.11 allows local users to cause a denial of service (general protection fault and system crash) or possibly have unspecified other impact via a crafted USB device, related to a missing warm-start check and incorrect attach timing (dm04_lme2510_frontend_attach versus dm04_lme2510_tuner).",
        "id": 1325
    },
    {
        "cve_id": "CVE-2013-1943",
        "code_before_change": "int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,\n\t\t\tint len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = copy_from_user(data, (void __user *)addr + offset, len);\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,\n\t\t\tint len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = __copy_from_user(data, (void __user *)addr + offset, len);\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \taddr = gfn_to_hva(kvm, gfn);\n \tif (kvm_is_error_hva(addr))\n \t\treturn -EFAULT;\n-\tr = copy_from_user(data, (void __user *)addr + offset, len);\n+\tr = __copy_from_user(data, (void __user *)addr + offset, len);\n \tif (r)\n \t\treturn -EFAULT;\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tr = __copy_from_user(data, (void __user *)addr + offset, len);"
            ],
            "deleted": [
                "\tr = copy_from_user(data, (void __user *)addr + offset, len);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The KVM subsystem in the Linux kernel before 3.0 does not check whether kernel addresses are specified during allocation of memory slots for use in a guest's physical address space, which allows local users to gain privileges or obtain sensitive information from kernel memory via a crafted application, related to arch/x86/kvm/paging_tmpl.h and virt/kvm/kvm_main.c.",
        "id": 209
    },
    {
        "cve_id": "CVE-2020-12363",
        "code_before_change": "static void __guc_ads_init(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct __guc_ads_blob *blob = guc->ads_blob;\n\tconst u32 skipped_size = LRC_PPHWSP_SZ * PAGE_SIZE + LR_HW_CONTEXT_SIZE;\n\tu32 base;\n\tu8 engine_class;\n\n\t/* GuC scheduling policies */\n\tguc_policies_init(&blob->policies);\n\n\t/*\n\t * GuC expects a per-engine-class context image and size\n\t * (minus hwsp and ring context). The context image will be\n\t * used to reinitialize engines after a reset. It must exist\n\t * and be pinned in the GGTT, so that the address won't change after\n\t * we have told GuC where to find it. The context size will be used\n\t * to validate that the LRC base + size fall within allowed GGTT.\n\t */\n\tfor (engine_class = 0; engine_class <= MAX_ENGINE_CLASS; ++engine_class) {\n\t\tif (engine_class == OTHER_CLASS)\n\t\t\tcontinue;\n\t\t/*\n\t\t * TODO: Set context pointer to default state to allow\n\t\t * GuC to re-init guilty contexts after internal reset.\n\t\t */\n\t\tblob->ads.golden_context_lrca[engine_class] = 0;\n\t\tblob->ads.eng_state_size[engine_class] =\n\t\t\tintel_engine_context_size(guc_to_gt(guc),\n\t\t\t\t\t\t  engine_class) -\n\t\t\tskipped_size;\n\t}\n\n\t/* System info */\n\tblob->system_info.slice_enabled = hweight8(gt->info.sseu.slice_mask);\n\tblob->system_info.rcs_enabled = 1;\n\tblob->system_info.bcs_enabled = 1;\n\n\tblob->system_info.vdbox_enable_mask = VDBOX_MASK(gt);\n\tblob->system_info.vebox_enable_mask = VEBOX_MASK(gt);\n\tblob->system_info.vdbox_sfc_support_mask = gt->info.vdbox_sfc_access;\n\n\tbase = intel_guc_ggtt_offset(guc, guc->ads_vma);\n\n\t/* Clients info  */\n\tguc_ct_pool_entries_init(blob->ct_pool, ARRAY_SIZE(blob->ct_pool));\n\n\tblob->clients_info.clients_num = 1;\n\tblob->clients_info.ct_pool_addr = base + ptr_offset(blob, ct_pool);\n\tblob->clients_info.ct_pool_count = ARRAY_SIZE(blob->ct_pool);\n\n\t/* ADS */\n\tblob->ads.scheduler_policies = base + ptr_offset(blob, policies);\n\tblob->ads.reg_state_buffer = base + ptr_offset(blob, reg_state_buffer);\n\tblob->ads.reg_state_addr = base + ptr_offset(blob, reg_state);\n\tblob->ads.gt_system_info = base + ptr_offset(blob, system_info);\n\tblob->ads.clients_info = base + ptr_offset(blob, clients_info);\n\n\ti915_gem_object_flush_map(guc->ads_vma->obj);\n}",
        "code_after_change": "static void __guc_ads_init(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct __guc_ads_blob *blob = guc->ads_blob;\n\tconst u32 skipped_size = LRC_PPHWSP_SZ * PAGE_SIZE + LR_HW_CONTEXT_SIZE;\n\tu32 base;\n\tu8 engine_class;\n\n\t/* GuC scheduling policies */\n\tguc_policies_init(&blob->policies);\n\n\t/*\n\t * GuC expects a per-engine-class context image and size\n\t * (minus hwsp and ring context). The context image will be\n\t * used to reinitialize engines after a reset. It must exist\n\t * and be pinned in the GGTT, so that the address won't change after\n\t * we have told GuC where to find it. The context size will be used\n\t * to validate that the LRC base + size fall within allowed GGTT.\n\t */\n\tfor (engine_class = 0; engine_class <= MAX_ENGINE_CLASS; ++engine_class) {\n\t\tif (engine_class == OTHER_CLASS)\n\t\t\tcontinue;\n\t\t/*\n\t\t * TODO: Set context pointer to default state to allow\n\t\t * GuC to re-init guilty contexts after internal reset.\n\t\t */\n\t\tblob->ads.golden_context_lrca[engine_class] = 0;\n\t\tblob->ads.eng_state_size[engine_class] =\n\t\t\tintel_engine_context_size(guc_to_gt(guc),\n\t\t\t\t\t\t  engine_class) -\n\t\t\tskipped_size;\n\t}\n\n\t/* System info */\n\tblob->system_info.engine_enabled_masks[RENDER_CLASS] = 1;\n\tblob->system_info.engine_enabled_masks[COPY_ENGINE_CLASS] = 1;\n\tblob->system_info.engine_enabled_masks[VIDEO_DECODE_CLASS] = VDBOX_MASK(gt);\n\tblob->system_info.engine_enabled_masks[VIDEO_ENHANCEMENT_CLASS] = VEBOX_MASK(gt);\n\n\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_SLICE_ENABLED] =\n\t\thweight8(gt->info.sseu.slice_mask);\n\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_VDBOX_SFC_SUPPORT_MASK] =\n\t\tgt->info.vdbox_sfc_access;\n\n\tif (INTEL_GEN(i915) >= 12 && !IS_DGFX(i915)) {\n\t\tu32 distdbreg = intel_uncore_read(gt->uncore,\n\t\t\t\t\t\t  GEN12_DIST_DBS_POPULATED);\n\t\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_DOORBELL_COUNT_PER_SQIDI] =\n\t\t\t((distdbreg >> GEN12_DOORBELLS_PER_SQIDI_SHIFT) &\n\t\t\t GEN12_DOORBELLS_PER_SQIDI) + 1;\n\t}\n\n\tguc_mapping_table_init(guc_to_gt(guc), &blob->system_info);\n\n\tbase = intel_guc_ggtt_offset(guc, guc->ads_vma);\n\n\t/* Clients info  */\n\tguc_ct_pool_entries_init(blob->ct_pool, ARRAY_SIZE(blob->ct_pool));\n\n\tblob->clients_info.clients_num = 1;\n\tblob->clients_info.ct_pool_addr = base + ptr_offset(blob, ct_pool);\n\tblob->clients_info.ct_pool_count = ARRAY_SIZE(blob->ct_pool);\n\n\t/* ADS */\n\tblob->ads.scheduler_policies = base + ptr_offset(blob, policies);\n\tblob->ads.gt_system_info = base + ptr_offset(blob, system_info);\n\tblob->ads.clients_info = base + ptr_offset(blob, clients_info);\n\n\t/* Private Data */\n\tblob->ads.private_data = base + guc_ads_private_data_offset(guc);\n\n\ti915_gem_object_flush_map(guc->ads_vma->obj);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,7 @@\n static void __guc_ads_init(struct intel_guc *guc)\n {\n \tstruct intel_gt *gt = guc_to_gt(guc);\n+\tstruct drm_i915_private *i915 = gt->i915;\n \tstruct __guc_ads_blob *blob = guc->ads_blob;\n \tconst u32 skipped_size = LRC_PPHWSP_SZ * PAGE_SIZE + LR_HW_CONTEXT_SIZE;\n \tu32 base;\n@@ -32,13 +33,25 @@\n \t}\n \n \t/* System info */\n-\tblob->system_info.slice_enabled = hweight8(gt->info.sseu.slice_mask);\n-\tblob->system_info.rcs_enabled = 1;\n-\tblob->system_info.bcs_enabled = 1;\n+\tblob->system_info.engine_enabled_masks[RENDER_CLASS] = 1;\n+\tblob->system_info.engine_enabled_masks[COPY_ENGINE_CLASS] = 1;\n+\tblob->system_info.engine_enabled_masks[VIDEO_DECODE_CLASS] = VDBOX_MASK(gt);\n+\tblob->system_info.engine_enabled_masks[VIDEO_ENHANCEMENT_CLASS] = VEBOX_MASK(gt);\n \n-\tblob->system_info.vdbox_enable_mask = VDBOX_MASK(gt);\n-\tblob->system_info.vebox_enable_mask = VEBOX_MASK(gt);\n-\tblob->system_info.vdbox_sfc_support_mask = gt->info.vdbox_sfc_access;\n+\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_SLICE_ENABLED] =\n+\t\thweight8(gt->info.sseu.slice_mask);\n+\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_VDBOX_SFC_SUPPORT_MASK] =\n+\t\tgt->info.vdbox_sfc_access;\n+\n+\tif (INTEL_GEN(i915) >= 12 && !IS_DGFX(i915)) {\n+\t\tu32 distdbreg = intel_uncore_read(gt->uncore,\n+\t\t\t\t\t\t  GEN12_DIST_DBS_POPULATED);\n+\t\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_DOORBELL_COUNT_PER_SQIDI] =\n+\t\t\t((distdbreg >> GEN12_DOORBELLS_PER_SQIDI_SHIFT) &\n+\t\t\t GEN12_DOORBELLS_PER_SQIDI) + 1;\n+\t}\n+\n+\tguc_mapping_table_init(guc_to_gt(guc), &blob->system_info);\n \n \tbase = intel_guc_ggtt_offset(guc, guc->ads_vma);\n \n@@ -51,10 +64,11 @@\n \n \t/* ADS */\n \tblob->ads.scheduler_policies = base + ptr_offset(blob, policies);\n-\tblob->ads.reg_state_buffer = base + ptr_offset(blob, reg_state_buffer);\n-\tblob->ads.reg_state_addr = base + ptr_offset(blob, reg_state);\n \tblob->ads.gt_system_info = base + ptr_offset(blob, system_info);\n \tblob->ads.clients_info = base + ptr_offset(blob, clients_info);\n \n+\t/* Private Data */\n+\tblob->ads.private_data = base + guc_ads_private_data_offset(guc);\n+\n \ti915_gem_object_flush_map(guc->ads_vma->obj);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct drm_i915_private *i915 = gt->i915;",
                "\tblob->system_info.engine_enabled_masks[RENDER_CLASS] = 1;",
                "\tblob->system_info.engine_enabled_masks[COPY_ENGINE_CLASS] = 1;",
                "\tblob->system_info.engine_enabled_masks[VIDEO_DECODE_CLASS] = VDBOX_MASK(gt);",
                "\tblob->system_info.engine_enabled_masks[VIDEO_ENHANCEMENT_CLASS] = VEBOX_MASK(gt);",
                "\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_SLICE_ENABLED] =",
                "\t\thweight8(gt->info.sseu.slice_mask);",
                "\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_VDBOX_SFC_SUPPORT_MASK] =",
                "\t\tgt->info.vdbox_sfc_access;",
                "",
                "\tif (INTEL_GEN(i915) >= 12 && !IS_DGFX(i915)) {",
                "\t\tu32 distdbreg = intel_uncore_read(gt->uncore,",
                "\t\t\t\t\t\t  GEN12_DIST_DBS_POPULATED);",
                "\t\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_DOORBELL_COUNT_PER_SQIDI] =",
                "\t\t\t((distdbreg >> GEN12_DOORBELLS_PER_SQIDI_SHIFT) &",
                "\t\t\t GEN12_DOORBELLS_PER_SQIDI) + 1;",
                "\t}",
                "",
                "\tguc_mapping_table_init(guc_to_gt(guc), &blob->system_info);",
                "\t/* Private Data */",
                "\tblob->ads.private_data = base + guc_ads_private_data_offset(guc);",
                ""
            ],
            "deleted": [
                "\tblob->system_info.slice_enabled = hweight8(gt->info.sseu.slice_mask);",
                "\tblob->system_info.rcs_enabled = 1;",
                "\tblob->system_info.bcs_enabled = 1;",
                "\tblob->system_info.vdbox_enable_mask = VDBOX_MASK(gt);",
                "\tblob->system_info.vebox_enable_mask = VEBOX_MASK(gt);",
                "\tblob->system_info.vdbox_sfc_support_mask = gt->info.vdbox_sfc_access;",
                "\tblob->ads.reg_state_buffer = base + ptr_offset(blob, reg_state_buffer);",
                "\tblob->ads.reg_state_addr = base + ptr_offset(blob, reg_state);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper input validation in some Intel(R) Graphics Drivers for Windows* before version 26.20.100.7212 and before Linux kernel version 5.5 may allow a privileged user to potentially enable a denial of service via local access.",
        "id": 2460
    },
    {
        "cve_id": "CVE-2016-2143",
        "code_before_change": "static inline void arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t struct mm_struct *mm)\n{\n\tif (oldmm->context.asce_limit < mm->context.asce_limit)\n\t\tcrst_table_downgrade(mm, oldmm->context.asce_limit);\n}",
        "code_after_change": "static inline void arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t struct mm_struct *mm)\n{\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,4 @@\n static inline void arch_dup_mmap(struct mm_struct *oldmm,\n \t\t\t\t struct mm_struct *mm)\n {\n-\tif (oldmm->context.asce_limit < mm->context.asce_limit)\n-\t\tcrst_table_downgrade(mm, oldmm->context.asce_limit);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tif (oldmm->context.asce_limit < mm->context.asce_limit)",
                "\t\tcrst_table_downgrade(mm, oldmm->context.asce_limit);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The fork implementation in the Linux kernel before 4.5 on s390 platforms mishandles the case of four page-table levels, which allows local users to cause a denial of service (system crash) or possibly have unspecified other impact via a crafted application, related to arch/s390/include/asm/mmu_context.h and arch/s390/include/asm/pgalloc.h.",
        "id": 923
    },
    {
        "cve_id": "CVE-2015-3288",
        "code_before_change": "static int handle_pte_fault(struct mm_struct *mm,\n\t\t     struct vm_area_struct *vma, unsigned long address,\n\t\t     pte_t *pte, pmd_t *pmd, unsigned int flags)\n{\n\tpte_t entry;\n\tspinlock_t *ptl;\n\n\t/*\n\t * some architectures can have larger ptes than wordsize,\n\t * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and CONFIG_32BIT=y,\n\t * so READ_ONCE or ACCESS_ONCE cannot guarantee atomic accesses.\n\t * The code below just needs a consistent view for the ifs and\n\t * we later double check anyway with the ptl lock held. So here\n\t * a barrier will do.\n\t */\n\tentry = *pte;\n\tbarrier();\n\tif (!pte_present(entry)) {\n\t\tif (pte_none(entry)) {\n\t\t\tif (vma->vm_ops) {\n\t\t\t\tif (likely(vma->vm_ops->fault))\n\t\t\t\t\treturn do_fault(mm, vma, address, pte,\n\t\t\t\t\t\t\tpmd, flags, entry);\n\t\t\t}\n\t\t\treturn do_anonymous_page(mm, vma, address,\n\t\t\t\t\t\t pte, pmd, flags);\n\t\t}\n\t\treturn do_swap_page(mm, vma, address,\n\t\t\t\t\tpte, pmd, flags, entry);\n\t}\n\n\tif (pte_protnone(entry))\n\t\treturn do_numa_page(mm, vma, address, entry, pte, pmd);\n\n\tptl = pte_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\tif (unlikely(!pte_same(*pte, entry)))\n\t\tgoto unlock;\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!pte_write(entry))\n\t\t\treturn do_wp_page(mm, vma, address,\n\t\t\t\t\tpte, pmd, ptl, entry);\n\t\tentry = pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (ptep_set_access_flags(vma, address, pte, entry, flags & FAULT_FLAG_WRITE)) {\n\t\tupdate_mmu_cache(vma, address, pte);\n\t} else {\n\t\t/*\n\t\t * This is needed only for protection faults but the arch code\n\t\t * is not yet telling us if this is a protection fault or not.\n\t\t * This still avoids useless tlb flushes for .text page faults\n\t\t * with threads.\n\t\t */\n\t\tif (flags & FAULT_FLAG_WRITE)\n\t\t\tflush_tlb_fix_spurious_fault(vma, address);\n\t}\nunlock:\n\tpte_unmap_unlock(pte, ptl);\n\treturn 0;\n}",
        "code_after_change": "static int handle_pte_fault(struct mm_struct *mm,\n\t\t     struct vm_area_struct *vma, unsigned long address,\n\t\t     pte_t *pte, pmd_t *pmd, unsigned int flags)\n{\n\tpte_t entry;\n\tspinlock_t *ptl;\n\n\t/*\n\t * some architectures can have larger ptes than wordsize,\n\t * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and CONFIG_32BIT=y,\n\t * so READ_ONCE or ACCESS_ONCE cannot guarantee atomic accesses.\n\t * The code below just needs a consistent view for the ifs and\n\t * we later double check anyway with the ptl lock held. So here\n\t * a barrier will do.\n\t */\n\tentry = *pte;\n\tbarrier();\n\tif (!pte_present(entry)) {\n\t\tif (pte_none(entry)) {\n\t\t\tif (vma->vm_ops)\n\t\t\t\treturn do_fault(mm, vma, address, pte, pmd,\n\t\t\t\t\t\tflags, entry);\n\n\t\t\treturn do_anonymous_page(mm, vma, address, pte, pmd,\n\t\t\t\t\tflags);\n\t\t}\n\t\treturn do_swap_page(mm, vma, address,\n\t\t\t\t\tpte, pmd, flags, entry);\n\t}\n\n\tif (pte_protnone(entry))\n\t\treturn do_numa_page(mm, vma, address, entry, pte, pmd);\n\n\tptl = pte_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\tif (unlikely(!pte_same(*pte, entry)))\n\t\tgoto unlock;\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!pte_write(entry))\n\t\t\treturn do_wp_page(mm, vma, address,\n\t\t\t\t\tpte, pmd, ptl, entry);\n\t\tentry = pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (ptep_set_access_flags(vma, address, pte, entry, flags & FAULT_FLAG_WRITE)) {\n\t\tupdate_mmu_cache(vma, address, pte);\n\t} else {\n\t\t/*\n\t\t * This is needed only for protection faults but the arch code\n\t\t * is not yet telling us if this is a protection fault or not.\n\t\t * This still avoids useless tlb flushes for .text page faults\n\t\t * with threads.\n\t\t */\n\t\tif (flags & FAULT_FLAG_WRITE)\n\t\t\tflush_tlb_fix_spurious_fault(vma, address);\n\t}\nunlock:\n\tpte_unmap_unlock(pte, ptl);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,13 +17,12 @@\n \tbarrier();\n \tif (!pte_present(entry)) {\n \t\tif (pte_none(entry)) {\n-\t\t\tif (vma->vm_ops) {\n-\t\t\t\tif (likely(vma->vm_ops->fault))\n-\t\t\t\t\treturn do_fault(mm, vma, address, pte,\n-\t\t\t\t\t\t\tpmd, flags, entry);\n-\t\t\t}\n-\t\t\treturn do_anonymous_page(mm, vma, address,\n-\t\t\t\t\t\t pte, pmd, flags);\n+\t\t\tif (vma->vm_ops)\n+\t\t\t\treturn do_fault(mm, vma, address, pte, pmd,\n+\t\t\t\t\t\tflags, entry);\n+\n+\t\t\treturn do_anonymous_page(mm, vma, address, pte, pmd,\n+\t\t\t\t\tflags);\n \t\t}\n \t\treturn do_swap_page(mm, vma, address,\n \t\t\t\t\tpte, pmd, flags, entry);",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (vma->vm_ops)",
                "\t\t\t\treturn do_fault(mm, vma, address, pte, pmd,",
                "\t\t\t\t\t\tflags, entry);",
                "",
                "\t\t\treturn do_anonymous_page(mm, vma, address, pte, pmd,",
                "\t\t\t\t\tflags);"
            ],
            "deleted": [
                "\t\t\tif (vma->vm_ops) {",
                "\t\t\t\tif (likely(vma->vm_ops->fault))",
                "\t\t\t\t\treturn do_fault(mm, vma, address, pte,",
                "\t\t\t\t\t\t\tpmd, flags, entry);",
                "\t\t\t}",
                "\t\t\treturn do_anonymous_page(mm, vma, address,",
                "\t\t\t\t\t\t pte, pmd, flags);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "mm/memory.c in the Linux kernel before 4.1.4 mishandles anonymous pages, which allows local users to gain privileges or cause a denial of service (page tainting) via a crafted application that triggers writing to page zero.",
        "id": 756
    },
    {
        "cve_id": "CVE-2015-3288",
        "code_before_change": "static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tunsigned long address, pte_t *page_table, pmd_t *pmd,\n\t\tunsigned int flags)\n{\n\tstruct mem_cgroup *memcg;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tpte_unmap(page_table);\n\n\t/* Check if we need to add a guard page to the stack */\n\tif (check_stack_guard_page(vma, address) < 0)\n\t\treturn VM_FAULT_SIGSEGV;\n\n\t/* Use the zero-page for reads */\n\tif (!(flags & FAULT_FLAG_WRITE) && !mm_forbids_zeropage(mm)) {\n\t\tentry = pte_mkspecial(pfn_pte(my_zero_pfn(address),\n\t\t\t\t\t\tvma->vm_page_prot));\n\t\tpage_table = pte_offset_map_lock(mm, pmd, address, &ptl);\n\t\tif (!pte_none(*page_table))\n\t\t\tgoto unlock;\n\t\tgoto setpte;\n\t}\n\n\t/* Allocate our own private page. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\tgoto oom;\n\tpage = alloc_zeroed_user_highpage_movable(vma, address);\n\tif (!page)\n\t\tgoto oom;\n\n\tif (mem_cgroup_try_charge(page, mm, GFP_KERNEL, &memcg))\n\t\tgoto oom_free_page;\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceeding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\n\tentry = mk_pte(page, vma->vm_page_prot);\n\tif (vma->vm_flags & VM_WRITE)\n\t\tentry = pte_mkwrite(pte_mkdirty(entry));\n\n\tpage_table = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tif (!pte_none(*page_table))\n\t\tgoto release;\n\n\tinc_mm_counter_fast(mm, MM_ANONPAGES);\n\tpage_add_new_anon_rmap(page, vma, address);\n\tmem_cgroup_commit_charge(page, memcg, false);\n\tlru_cache_add_active_or_unevictable(page, vma);\nsetpte:\n\tset_pte_at(mm, address, page_table, entry);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, address, page_table);\nunlock:\n\tpte_unmap_unlock(page_table, ptl);\n\treturn 0;\nrelease:\n\tmem_cgroup_cancel_charge(page, memcg);\n\tpage_cache_release(page);\n\tgoto unlock;\noom_free_page:\n\tpage_cache_release(page);\noom:\n\treturn VM_FAULT_OOM;\n}",
        "code_after_change": "static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tunsigned long address, pte_t *page_table, pmd_t *pmd,\n\t\tunsigned int flags)\n{\n\tstruct mem_cgroup *memcg;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tpte_unmap(page_table);\n\n\t/* File mapping without ->vm_ops ? */\n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn VM_FAULT_SIGBUS;\n\n\t/* Check if we need to add a guard page to the stack */\n\tif (check_stack_guard_page(vma, address) < 0)\n\t\treturn VM_FAULT_SIGSEGV;\n\n\t/* Use the zero-page for reads */\n\tif (!(flags & FAULT_FLAG_WRITE) && !mm_forbids_zeropage(mm)) {\n\t\tentry = pte_mkspecial(pfn_pte(my_zero_pfn(address),\n\t\t\t\t\t\tvma->vm_page_prot));\n\t\tpage_table = pte_offset_map_lock(mm, pmd, address, &ptl);\n\t\tif (!pte_none(*page_table))\n\t\t\tgoto unlock;\n\t\tgoto setpte;\n\t}\n\n\t/* Allocate our own private page. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\tgoto oom;\n\tpage = alloc_zeroed_user_highpage_movable(vma, address);\n\tif (!page)\n\t\tgoto oom;\n\n\tif (mem_cgroup_try_charge(page, mm, GFP_KERNEL, &memcg))\n\t\tgoto oom_free_page;\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceeding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\n\tentry = mk_pte(page, vma->vm_page_prot);\n\tif (vma->vm_flags & VM_WRITE)\n\t\tentry = pte_mkwrite(pte_mkdirty(entry));\n\n\tpage_table = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tif (!pte_none(*page_table))\n\t\tgoto release;\n\n\tinc_mm_counter_fast(mm, MM_ANONPAGES);\n\tpage_add_new_anon_rmap(page, vma, address);\n\tmem_cgroup_commit_charge(page, memcg, false);\n\tlru_cache_add_active_or_unevictable(page, vma);\nsetpte:\n\tset_pte_at(mm, address, page_table, entry);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, address, page_table);\nunlock:\n\tpte_unmap_unlock(page_table, ptl);\n\treturn 0;\nrelease:\n\tmem_cgroup_cancel_charge(page, memcg);\n\tpage_cache_release(page);\n\tgoto unlock;\noom_free_page:\n\tpage_cache_release(page);\noom:\n\treturn VM_FAULT_OOM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,10 @@\n \tpte_t entry;\n \n \tpte_unmap(page_table);\n+\n+\t/* File mapping without ->vm_ops ? */\n+\tif (vma->vm_flags & VM_SHARED)\n+\t\treturn VM_FAULT_SIGBUS;\n \n \t/* Check if we need to add a guard page to the stack */\n \tif (check_stack_guard_page(vma, address) < 0)",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* File mapping without ->vm_ops ? */",
                "\tif (vma->vm_flags & VM_SHARED)",
                "\t\treturn VM_FAULT_SIGBUS;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "mm/memory.c in the Linux kernel before 4.1.4 mishandles anonymous pages, which allows local users to gain privileges or cause a denial of service (page tainting) via a crafted application that triggers writing to page zero.",
        "id": 754
    },
    {
        "cve_id": "CVE-2017-17862",
        "code_before_change": "int bpf_check(struct bpf_prog **prog, union bpf_attr *attr)\n{\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifer_log *log;\n\tint ret = -EINVAL;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tenv->insn_aux_data = vzalloc(sizeof(struct bpf_insn_aux_data) *\n\t\t\t\t     (*prog)->len);\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 8 ||\n\t\t    !log->level || !log->ubuf)\n\t\t\tgoto err_unlock;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\n\tif (env->prog->aux->offload) {\n\t\tret = bpf_prog_offload_verifier_prep(env);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->explored_states = kcalloc(env->prog->len,\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->allow_ptr_leaks = capable(CAP_SYS_ADMIN);\n\n\tret = do_check(env);\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\nskip_full_check:\n\twhile (!pop_stack(env, NULL, NULL));\n\tfree_states(env);\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_bpf_prog_info() will release them.\n\t\t */\n\t\trelease_maps(env);\n\t*prog = env->prog;\nerr_unlock:\n\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}",
        "code_after_change": "int bpf_check(struct bpf_prog **prog, union bpf_attr *attr)\n{\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifer_log *log;\n\tint ret = -EINVAL;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tenv->insn_aux_data = vzalloc(sizeof(struct bpf_insn_aux_data) *\n\t\t\t\t     (*prog)->len);\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 8 ||\n\t\t    !log->level || !log->ubuf)\n\t\t\tgoto err_unlock;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\n\tif (env->prog->aux->offload) {\n\t\tret = bpf_prog_offload_verifier_prep(env);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->explored_states = kcalloc(env->prog->len,\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->allow_ptr_leaks = capable(CAP_SYS_ADMIN);\n\n\tret = do_check(env);\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\nskip_full_check:\n\twhile (!pop_stack(env, NULL, NULL));\n\tfree_states(env);\n\n\tif (ret == 0)\n\t\tsanitize_dead_code(env);\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_bpf_prog_info() will release them.\n\t\t */\n\t\trelease_maps(env);\n\t*prog = env->prog;\nerr_unlock:\n\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -80,6 +80,9 @@\n \tfree_states(env);\n \n \tif (ret == 0)\n+\t\tsanitize_dead_code(env);\n+\n+\tif (ret == 0)\n \t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n \t\tret = convert_ctx_accesses(env);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tsanitize_dead_code(env);",
                "",
                "\tif (ret == 0)"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "kernel/bpf/verifier.c in the Linux kernel through 4.14.8 ignores unreachable code, even though it would still be processed by JIT compilers. This behavior, also considered an improper branch-pruning logic issue, could possibly be used by local users for denial of service.",
        "id": 1384
    },
    {
        "cve_id": "CVE-2013-1848",
        "code_before_change": "static struct block_device *ext3_blkdev_get(dev_t dev, struct super_block *sb)\n{\n\tstruct block_device *bdev;\n\tchar b[BDEVNAME_SIZE];\n\n\tbdev = blkdev_get_by_dev(dev, FMODE_READ|FMODE_WRITE|FMODE_EXCL, sb);\n\tif (IS_ERR(bdev))\n\t\tgoto fail;\n\treturn bdev;\n\nfail:\n\text3_msg(sb, \"error: failed to open journal device %s: %ld\",\n\t\t__bdevname(dev, b), PTR_ERR(bdev));\n\n\treturn NULL;\n}",
        "code_after_change": "static struct block_device *ext3_blkdev_get(dev_t dev, struct super_block *sb)\n{\n\tstruct block_device *bdev;\n\tchar b[BDEVNAME_SIZE];\n\n\tbdev = blkdev_get_by_dev(dev, FMODE_READ|FMODE_WRITE|FMODE_EXCL, sb);\n\tif (IS_ERR(bdev))\n\t\tgoto fail;\n\treturn bdev;\n\nfail:\n\text3_msg(sb, KERN_ERR, \"error: failed to open journal device %s: %ld\",\n\t\t__bdevname(dev, b), PTR_ERR(bdev));\n\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \treturn bdev;\n \n fail:\n-\text3_msg(sb, \"error: failed to open journal device %s: %ld\",\n+\text3_msg(sb, KERN_ERR, \"error: failed to open journal device %s: %ld\",\n \t\t__bdevname(dev, b), PTR_ERR(bdev));\n \n \treturn NULL;",
        "function_modified_lines": {
            "added": [
                "\text3_msg(sb, KERN_ERR, \"error: failed to open journal device %s: %ld\","
            ],
            "deleted": [
                "\text3_msg(sb, \"error: failed to open journal device %s: %ld\","
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "fs/ext3/super.c in the Linux kernel before 3.8.4 uses incorrect arguments to functions in certain circumstances related to printk input, which allows local users to conduct format-string attacks and possibly gain privileges via a crafted application.",
        "id": 197
    },
    {
        "cve_id": "CVE-2014-2523",
        "code_before_change": "static bool dccp_new(struct nf_conn *ct, const struct sk_buff *skb,\n\t\t     unsigned int dataoff, unsigned int *timeouts)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tstruct dccp_net *dn;\n\tstruct dccp_hdr _dh, *dh;\n\tconst char *msg;\n\tu_int8_t state;\n\n\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);\n\tBUG_ON(dh == NULL);\n\n\tstate = dccp_state_table[CT_DCCP_ROLE_CLIENT][dh->dccph_type][CT_DCCP_NONE];\n\tswitch (state) {\n\tdefault:\n\t\tdn = dccp_pernet(net);\n\t\tif (dn->dccp_loose == 0) {\n\t\t\tmsg = \"nf_ct_dccp: not picking up existing connection \";\n\t\t\tgoto out_invalid;\n\t\t}\n\tcase CT_DCCP_REQUEST:\n\t\tbreak;\n\tcase CT_DCCP_INVALID:\n\t\tmsg = \"nf_ct_dccp: invalid state transition \";\n\t\tgoto out_invalid;\n\t}\n\n\tct->proto.dccp.role[IP_CT_DIR_ORIGINAL] = CT_DCCP_ROLE_CLIENT;\n\tct->proto.dccp.role[IP_CT_DIR_REPLY] = CT_DCCP_ROLE_SERVER;\n\tct->proto.dccp.state = CT_DCCP_NONE;\n\tct->proto.dccp.last_pkt = DCCP_PKT_REQUEST;\n\tct->proto.dccp.last_dir = IP_CT_DIR_ORIGINAL;\n\tct->proto.dccp.handshake_seq = 0;\n\treturn true;\n\nout_invalid:\n\tif (LOG_INVALID(net, IPPROTO_DCCP))\n\t\tnf_log_packet(net, nf_ct_l3num(ct), 0, skb, NULL, NULL,\n\t\t\t      NULL, \"%s\", msg);\n\treturn false;\n}",
        "code_after_change": "static bool dccp_new(struct nf_conn *ct, const struct sk_buff *skb,\n\t\t     unsigned int dataoff, unsigned int *timeouts)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tstruct dccp_net *dn;\n\tstruct dccp_hdr _dh, *dh;\n\tconst char *msg;\n\tu_int8_t state;\n\n\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);\n\tBUG_ON(dh == NULL);\n\n\tstate = dccp_state_table[CT_DCCP_ROLE_CLIENT][dh->dccph_type][CT_DCCP_NONE];\n\tswitch (state) {\n\tdefault:\n\t\tdn = dccp_pernet(net);\n\t\tif (dn->dccp_loose == 0) {\n\t\t\tmsg = \"nf_ct_dccp: not picking up existing connection \";\n\t\t\tgoto out_invalid;\n\t\t}\n\tcase CT_DCCP_REQUEST:\n\t\tbreak;\n\tcase CT_DCCP_INVALID:\n\t\tmsg = \"nf_ct_dccp: invalid state transition \";\n\t\tgoto out_invalid;\n\t}\n\n\tct->proto.dccp.role[IP_CT_DIR_ORIGINAL] = CT_DCCP_ROLE_CLIENT;\n\tct->proto.dccp.role[IP_CT_DIR_REPLY] = CT_DCCP_ROLE_SERVER;\n\tct->proto.dccp.state = CT_DCCP_NONE;\n\tct->proto.dccp.last_pkt = DCCP_PKT_REQUEST;\n\tct->proto.dccp.last_dir = IP_CT_DIR_ORIGINAL;\n\tct->proto.dccp.handshake_seq = 0;\n\treturn true;\n\nout_invalid:\n\tif (LOG_INVALID(net, IPPROTO_DCCP))\n\t\tnf_log_packet(net, nf_ct_l3num(ct), 0, skb, NULL, NULL,\n\t\t\t      NULL, \"%s\", msg);\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tconst char *msg;\n \tu_int8_t state;\n \n-\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);\n+\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);\n \tBUG_ON(dh == NULL);\n \n \tstate = dccp_state_table[CT_DCCP_ROLE_CLIENT][dh->dccph_type][CT_DCCP_NONE];",
        "function_modified_lines": {
            "added": [
                "\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);"
            ],
            "deleted": [
                "\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "net/netfilter/nf_conntrack_proto_dccp.c in the Linux kernel through 3.13.6 uses a DCCP header pointer incorrectly, which allows remote attackers to cause a denial of service (system crash) or possibly execute arbitrary code via a DCCP packet that triggers a call to the (1) dccp_new, (2) dccp_packet, or (3) dccp_error function.",
        "id": 482
    },
    {
        "cve_id": "CVE-2017-15121",
        "code_before_change": "void truncate_inode_pages_range(struct address_space *mapping,\n\t\t\t\tloff_t lstart, loff_t lend)\n{\n\tconst pgoff_t start = (lstart + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;\n\tconst unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);\n\tstruct pagevec pvec;\n\tpgoff_t index;\n\tpgoff_t end;\n\tint i;\n\n\tcleancache_invalidate_inode(mapping);\n\tif (mapping->nrpages == 0)\n\t\treturn;\n\n\tBUG_ON((lend & (PAGE_CACHE_SIZE - 1)) != (PAGE_CACHE_SIZE - 1));\n\tend = (lend >> PAGE_CACHE_SHIFT);\n\n\tpagevec_init(&pvec, 0);\n\tindex = start;\n\twhile (index <= end && pagevec_lookup(&pvec, mapping, index,\n\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/* We rely upon deletion not changing page->index */\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\n\t\t\tif (!trylock_page(page))\n\t\t\t\tcontinue;\n\t\t\tWARN_ON(page->index != index);\n\t\t\tif (PageWriteback(page)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttruncate_inode_page(mapping, page);\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tcond_resched();\n\t\tindex++;\n\t}\n\n\tif (partial) {\n\t\tstruct page *page = find_lock_page(mapping, start - 1);\n\t\tif (page) {\n\t\t\twait_on_page_writeback(page);\n\t\t\ttruncate_partial_page(page, partial);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\n\tindex = start;\n\tfor ( ; ; ) {\n\t\tcond_resched();\n\t\tif (!pagevec_lookup(&pvec, mapping, index,\n\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {\n\t\t\tif (index == start)\n\t\t\t\tbreak;\n\t\t\tindex = start;\n\t\t\tcontinue;\n\t\t}\n\t\tif (index == start && pvec.pages[0]->index > end) {\n\t\t\tpagevec_release(&pvec);\n\t\t\tbreak;\n\t\t}\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/* We rely upon deletion not changing page->index */\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\n\t\t\tlock_page(page);\n\t\t\tWARN_ON(page->index != index);\n\t\t\twait_on_page_writeback(page);\n\t\t\ttruncate_inode_page(mapping, page);\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tindex++;\n\t}\n\tcleancache_invalidate_inode(mapping);\n}",
        "code_after_change": "void truncate_inode_pages_range(struct address_space *mapping,\n\t\t\t\tloff_t lstart, loff_t lend)\n{\n\tpgoff_t\t\tstart;\t\t/* inclusive */\n\tpgoff_t\t\tend;\t\t/* exclusive */\n\tunsigned int\tpartial_start;\t/* inclusive */\n\tunsigned int\tpartial_end;\t/* exclusive */\n\tstruct pagevec\tpvec;\n\tpgoff_t\t\tindex;\n\tint\t\ti;\n\n\tcleancache_invalidate_inode(mapping);\n\tif (mapping->nrpages == 0)\n\t\treturn;\n\n\t/* Offsets within partial pages */\n\tpartial_start = lstart & (PAGE_CACHE_SIZE - 1);\n\tpartial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);\n\n\t/*\n\t * 'start' and 'end' always covers the range of pages to be fully\n\t * truncated. Partial pages are covered with 'partial_start' at the\n\t * start of the range and 'partial_end' at the end of the range.\n\t * Note that 'end' is exclusive while 'lend' is inclusive.\n\t */\n\tstart = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\tif (lend == -1)\n\t\t/*\n\t\t * lend == -1 indicates end-of-file so we have to set 'end'\n\t\t * to the highest possible pgoff_t and since the type is\n\t\t * unsigned we're using -1.\n\t\t */\n\t\tend = -1;\n\telse\n\t\tend = (lend + 1) >> PAGE_CACHE_SHIFT;\n\n\tpagevec_init(&pvec, 0);\n\tindex = start;\n\twhile (index < end && pagevec_lookup(&pvec, mapping, index,\n\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/* We rely upon deletion not changing page->index */\n\t\t\tindex = page->index;\n\t\t\tif (index >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (!trylock_page(page))\n\t\t\t\tcontinue;\n\t\t\tWARN_ON(page->index != index);\n\t\t\tif (PageWriteback(page)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttruncate_inode_page(mapping, page);\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tcond_resched();\n\t\tindex++;\n\t}\n\n\tif (partial_start) {\n\t\tstruct page *page = find_lock_page(mapping, start - 1);\n\t\tif (page) {\n\t\t\tunsigned int top = PAGE_CACHE_SIZE;\n\t\t\tif (start > end) {\n\t\t\t\t/* Truncation within a single page */\n\t\t\t\ttop = partial_end;\n\t\t\t\tpartial_end = 0;\n\t\t\t}\n\t\t\twait_on_page_writeback(page);\n\t\t\tzero_user_segment(page, partial_start, top);\n\t\t\tcleancache_invalidate_page(mapping, page);\n\t\t\tif (page_has_private(page))\n\t\t\t\tdo_invalidatepage(page, partial_start,\n\t\t\t\t\t\t  top - partial_start);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\tif (partial_end) {\n\t\tstruct page *page = find_lock_page(mapping, end);\n\t\tif (page) {\n\t\t\twait_on_page_writeback(page);\n\t\t\tzero_user_segment(page, 0, partial_end);\n\t\t\tcleancache_invalidate_page(mapping, page);\n\t\t\tif (page_has_private(page))\n\t\t\t\tdo_invalidatepage(page, 0,\n\t\t\t\t\t\t  partial_end);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\t/*\n\t * If the truncation happened within a single page no pages\n\t * will be released, just zeroed, so we can bail out now.\n\t */\n\tif (start >= end)\n\t\treturn;\n\n\tindex = start;\n\tfor ( ; ; ) {\n\t\tcond_resched();\n\t\tif (!pagevec_lookup(&pvec, mapping, index,\n\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {\n\t\t\tif (index == start)\n\t\t\t\tbreak;\n\t\t\tindex = start;\n\t\t\tcontinue;\n\t\t}\n\t\tif (index == start && pvec.pages[0]->index >= end) {\n\t\t\tpagevec_release(&pvec);\n\t\t\tbreak;\n\t\t}\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/* We rely upon deletion not changing page->index */\n\t\t\tindex = page->index;\n\t\t\tif (index >= end)\n\t\t\t\tbreak;\n\n\t\t\tlock_page(page);\n\t\t\tWARN_ON(page->index != index);\n\t\t\twait_on_page_writeback(page);\n\t\t\ttruncate_inode_page(mapping, page);\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tindex++;\n\t}\n\tcleancache_invalidate_inode(mapping);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,31 +1,50 @@\n void truncate_inode_pages_range(struct address_space *mapping,\n \t\t\t\tloff_t lstart, loff_t lend)\n {\n-\tconst pgoff_t start = (lstart + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;\n-\tconst unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);\n-\tstruct pagevec pvec;\n-\tpgoff_t index;\n-\tpgoff_t end;\n-\tint i;\n+\tpgoff_t\t\tstart;\t\t/* inclusive */\n+\tpgoff_t\t\tend;\t\t/* exclusive */\n+\tunsigned int\tpartial_start;\t/* inclusive */\n+\tunsigned int\tpartial_end;\t/* exclusive */\n+\tstruct pagevec\tpvec;\n+\tpgoff_t\t\tindex;\n+\tint\t\ti;\n \n \tcleancache_invalidate_inode(mapping);\n \tif (mapping->nrpages == 0)\n \t\treturn;\n \n-\tBUG_ON((lend & (PAGE_CACHE_SIZE - 1)) != (PAGE_CACHE_SIZE - 1));\n-\tend = (lend >> PAGE_CACHE_SHIFT);\n+\t/* Offsets within partial pages */\n+\tpartial_start = lstart & (PAGE_CACHE_SIZE - 1);\n+\tpartial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);\n+\n+\t/*\n+\t * 'start' and 'end' always covers the range of pages to be fully\n+\t * truncated. Partial pages are covered with 'partial_start' at the\n+\t * start of the range and 'partial_end' at the end of the range.\n+\t * Note that 'end' is exclusive while 'lend' is inclusive.\n+\t */\n+\tstart = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n+\tif (lend == -1)\n+\t\t/*\n+\t\t * lend == -1 indicates end-of-file so we have to set 'end'\n+\t\t * to the highest possible pgoff_t and since the type is\n+\t\t * unsigned we're using -1.\n+\t\t */\n+\t\tend = -1;\n+\telse\n+\t\tend = (lend + 1) >> PAGE_CACHE_SHIFT;\n \n \tpagevec_init(&pvec, 0);\n \tindex = start;\n-\twhile (index <= end && pagevec_lookup(&pvec, mapping, index,\n-\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {\n+\twhile (index < end && pagevec_lookup(&pvec, mapping, index,\n+\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {\n \t\tmem_cgroup_uncharge_start();\n \t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n \t\t\tstruct page *page = pvec.pages[i];\n \n \t\t\t/* We rely upon deletion not changing page->index */\n \t\t\tindex = page->index;\n-\t\t\tif (index > end)\n+\t\t\tif (index >= end)\n \t\t\t\tbreak;\n \n \t\t\tif (!trylock_page(page))\n@@ -44,27 +63,56 @@\n \t\tindex++;\n \t}\n \n-\tif (partial) {\n+\tif (partial_start) {\n \t\tstruct page *page = find_lock_page(mapping, start - 1);\n \t\tif (page) {\n+\t\t\tunsigned int top = PAGE_CACHE_SIZE;\n+\t\t\tif (start > end) {\n+\t\t\t\t/* Truncation within a single page */\n+\t\t\t\ttop = partial_end;\n+\t\t\t\tpartial_end = 0;\n+\t\t\t}\n \t\t\twait_on_page_writeback(page);\n-\t\t\ttruncate_partial_page(page, partial);\n+\t\t\tzero_user_segment(page, partial_start, top);\n+\t\t\tcleancache_invalidate_page(mapping, page);\n+\t\t\tif (page_has_private(page))\n+\t\t\t\tdo_invalidatepage(page, partial_start,\n+\t\t\t\t\t\t  top - partial_start);\n \t\t\tunlock_page(page);\n \t\t\tpage_cache_release(page);\n \t\t}\n \t}\n+\tif (partial_end) {\n+\t\tstruct page *page = find_lock_page(mapping, end);\n+\t\tif (page) {\n+\t\t\twait_on_page_writeback(page);\n+\t\t\tzero_user_segment(page, 0, partial_end);\n+\t\t\tcleancache_invalidate_page(mapping, page);\n+\t\t\tif (page_has_private(page))\n+\t\t\t\tdo_invalidatepage(page, 0,\n+\t\t\t\t\t\t  partial_end);\n+\t\t\tunlock_page(page);\n+\t\t\tpage_cache_release(page);\n+\t\t}\n+\t}\n+\t/*\n+\t * If the truncation happened within a single page no pages\n+\t * will be released, just zeroed, so we can bail out now.\n+\t */\n+\tif (start >= end)\n+\t\treturn;\n \n \tindex = start;\n \tfor ( ; ; ) {\n \t\tcond_resched();\n \t\tif (!pagevec_lookup(&pvec, mapping, index,\n-\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {\n+\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {\n \t\t\tif (index == start)\n \t\t\t\tbreak;\n \t\t\tindex = start;\n \t\t\tcontinue;\n \t\t}\n-\t\tif (index == start && pvec.pages[0]->index > end) {\n+\t\tif (index == start && pvec.pages[0]->index >= end) {\n \t\t\tpagevec_release(&pvec);\n \t\t\tbreak;\n \t\t}\n@@ -74,7 +122,7 @@\n \n \t\t\t/* We rely upon deletion not changing page->index */\n \t\t\tindex = page->index;\n-\t\t\tif (index > end)\n+\t\t\tif (index >= end)\n \t\t\t\tbreak;\n \n \t\t\tlock_page(page);",
        "function_modified_lines": {
            "added": [
                "\tpgoff_t\t\tstart;\t\t/* inclusive */",
                "\tpgoff_t\t\tend;\t\t/* exclusive */",
                "\tunsigned int\tpartial_start;\t/* inclusive */",
                "\tunsigned int\tpartial_end;\t/* exclusive */",
                "\tstruct pagevec\tpvec;",
                "\tpgoff_t\t\tindex;",
                "\tint\t\ti;",
                "\t/* Offsets within partial pages */",
                "\tpartial_start = lstart & (PAGE_CACHE_SIZE - 1);",
                "\tpartial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);",
                "",
                "\t/*",
                "\t * 'start' and 'end' always covers the range of pages to be fully",
                "\t * truncated. Partial pages are covered with 'partial_start' at the",
                "\t * start of the range and 'partial_end' at the end of the range.",
                "\t * Note that 'end' is exclusive while 'lend' is inclusive.",
                "\t */",
                "\tstart = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;",
                "\tif (lend == -1)",
                "\t\t/*",
                "\t\t * lend == -1 indicates end-of-file so we have to set 'end'",
                "\t\t * to the highest possible pgoff_t and since the type is",
                "\t\t * unsigned we're using -1.",
                "\t\t */",
                "\t\tend = -1;",
                "\telse",
                "\t\tend = (lend + 1) >> PAGE_CACHE_SHIFT;",
                "\twhile (index < end && pagevec_lookup(&pvec, mapping, index,",
                "\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {",
                "\t\t\tif (index >= end)",
                "\tif (partial_start) {",
                "\t\t\tunsigned int top = PAGE_CACHE_SIZE;",
                "\t\t\tif (start > end) {",
                "\t\t\t\t/* Truncation within a single page */",
                "\t\t\t\ttop = partial_end;",
                "\t\t\t\tpartial_end = 0;",
                "\t\t\t}",
                "\t\t\tzero_user_segment(page, partial_start, top);",
                "\t\t\tcleancache_invalidate_page(mapping, page);",
                "\t\t\tif (page_has_private(page))",
                "\t\t\t\tdo_invalidatepage(page, partial_start,",
                "\t\t\t\t\t\t  top - partial_start);",
                "\tif (partial_end) {",
                "\t\tstruct page *page = find_lock_page(mapping, end);",
                "\t\tif (page) {",
                "\t\t\twait_on_page_writeback(page);",
                "\t\t\tzero_user_segment(page, 0, partial_end);",
                "\t\t\tcleancache_invalidate_page(mapping, page);",
                "\t\t\tif (page_has_private(page))",
                "\t\t\t\tdo_invalidatepage(page, 0,",
                "\t\t\t\t\t\t  partial_end);",
                "\t\t\tunlock_page(page);",
                "\t\t\tpage_cache_release(page);",
                "\t\t}",
                "\t}",
                "\t/*",
                "\t * If the truncation happened within a single page no pages",
                "\t * will be released, just zeroed, so we can bail out now.",
                "\t */",
                "\tif (start >= end)",
                "\t\treturn;",
                "\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {",
                "\t\tif (index == start && pvec.pages[0]->index >= end) {",
                "\t\t\tif (index >= end)"
            ],
            "deleted": [
                "\tconst pgoff_t start = (lstart + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;",
                "\tconst unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);",
                "\tstruct pagevec pvec;",
                "\tpgoff_t index;",
                "\tpgoff_t end;",
                "\tint i;",
                "\tBUG_ON((lend & (PAGE_CACHE_SIZE - 1)) != (PAGE_CACHE_SIZE - 1));",
                "\tend = (lend >> PAGE_CACHE_SHIFT);",
                "\twhile (index <= end && pagevec_lookup(&pvec, mapping, index,",
                "\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {",
                "\t\t\tif (index > end)",
                "\tif (partial) {",
                "\t\t\ttruncate_partial_page(page, partial);",
                "\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {",
                "\t\tif (index == start && pvec.pages[0]->index > end) {",
                "\t\t\tif (index > end)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A non-privileged user is able to mount a fuse filesystem on RHEL 6 or 7 and crash a system if an application punches a hole in a file that does not end aligned to a page boundary.",
        "id": 1296
    },
    {
        "cve_id": "CVE-2012-4398",
        "code_before_change": "int drbd_khelper(struct drbd_conf *mdev, char *cmd)\n{\n\tchar *envp[] = { \"HOME=/\",\n\t\t\t\"TERM=linux\",\n\t\t\t\"PATH=/sbin:/usr/sbin:/bin:/usr/bin\",\n\t\t\tNULL, /* Will be set to address family */\n\t\t\tNULL, /* Will be set to address */\n\t\t\tNULL };\n\n\tchar mb[12], af[20], ad[60], *afs;\n\tchar *argv[] = {usermode_helper, cmd, mb, NULL };\n\tint ret;\n\n\tsnprintf(mb, 12, \"minor-%d\", mdev_to_minor(mdev));\n\n\tif (get_net_conf(mdev)) {\n\t\tswitch (((struct sockaddr *)mdev->net_conf->peer_addr)->sa_family) {\n\t\tcase AF_INET6:\n\t\t\tafs = \"ipv6\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI6\",\n\t\t\t\t &((struct sockaddr_in6 *)mdev->net_conf->peer_addr)->sin6_addr);\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tafs = \"ipv4\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI4\",\n\t\t\t\t &((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tafs = \"ssocks\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI4\",\n\t\t\t\t &((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\n\t\t}\n\t\tsnprintf(af, 20, \"DRBD_PEER_AF=%s\", afs);\n\t\tenvp[3]=af;\n\t\tenvp[4]=ad;\n\t\tput_net_conf(mdev);\n\t}\n\n\t/* The helper may take some time.\n\t * write out any unsynced meta data changes now */\n\tdrbd_md_sync(mdev);\n\n\tdev_info(DEV, \"helper command: %s %s %s\\n\", usermode_helper, cmd, mb);\n\n\tdrbd_bcast_ev_helper(mdev, cmd);\n\tret = call_usermodehelper(usermode_helper, argv, envp, 1);\n\tif (ret)\n\t\tdev_warn(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n\t\t\t\tusermode_helper, cmd, mb,\n\t\t\t\t(ret >> 8) & 0xff, ret);\n\telse\n\t\tdev_info(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n\t\t\t\tusermode_helper, cmd, mb,\n\t\t\t\t(ret >> 8) & 0xff, ret);\n\n\tif (ret < 0) /* Ignore any ERRNOs we got. */\n\t\tret = 0;\n\n\treturn ret;\n}",
        "code_after_change": "int drbd_khelper(struct drbd_conf *mdev, char *cmd)\n{\n\tchar *envp[] = { \"HOME=/\",\n\t\t\t\"TERM=linux\",\n\t\t\t\"PATH=/sbin:/usr/sbin:/bin:/usr/bin\",\n\t\t\tNULL, /* Will be set to address family */\n\t\t\tNULL, /* Will be set to address */\n\t\t\tNULL };\n\n\tchar mb[12], af[20], ad[60], *afs;\n\tchar *argv[] = {usermode_helper, cmd, mb, NULL };\n\tint ret;\n\n\tsnprintf(mb, 12, \"minor-%d\", mdev_to_minor(mdev));\n\n\tif (get_net_conf(mdev)) {\n\t\tswitch (((struct sockaddr *)mdev->net_conf->peer_addr)->sa_family) {\n\t\tcase AF_INET6:\n\t\t\tafs = \"ipv6\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI6\",\n\t\t\t\t &((struct sockaddr_in6 *)mdev->net_conf->peer_addr)->sin6_addr);\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tafs = \"ipv4\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI4\",\n\t\t\t\t &((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tafs = \"ssocks\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI4\",\n\t\t\t\t &((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\n\t\t}\n\t\tsnprintf(af, 20, \"DRBD_PEER_AF=%s\", afs);\n\t\tenvp[3]=af;\n\t\tenvp[4]=ad;\n\t\tput_net_conf(mdev);\n\t}\n\n\t/* The helper may take some time.\n\t * write out any unsynced meta data changes now */\n\tdrbd_md_sync(mdev);\n\n\tdev_info(DEV, \"helper command: %s %s %s\\n\", usermode_helper, cmd, mb);\n\n\tdrbd_bcast_ev_helper(mdev, cmd);\n\tret = call_usermodehelper(usermode_helper, argv, envp, UMH_WAIT_PROC);\n\tif (ret)\n\t\tdev_warn(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n\t\t\t\tusermode_helper, cmd, mb,\n\t\t\t\t(ret >> 8) & 0xff, ret);\n\telse\n\t\tdev_info(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n\t\t\t\tusermode_helper, cmd, mb,\n\t\t\t\t(ret >> 8) & 0xff, ret);\n\n\tif (ret < 0) /* Ignore any ERRNOs we got. */\n\t\tret = 0;\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,7 +43,7 @@\n \tdev_info(DEV, \"helper command: %s %s %s\\n\", usermode_helper, cmd, mb);\n \n \tdrbd_bcast_ev_helper(mdev, cmd);\n-\tret = call_usermodehelper(usermode_helper, argv, envp, 1);\n+\tret = call_usermodehelper(usermode_helper, argv, envp, UMH_WAIT_PROC);\n \tif (ret)\n \t\tdev_warn(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n \t\t\t\tusermode_helper, cmd, mb,",
        "function_modified_lines": {
            "added": [
                "\tret = call_usermodehelper(usermode_helper, argv, envp, UMH_WAIT_PROC);"
            ],
            "deleted": [
                "\tret = call_usermodehelper(usermode_helper, argv, envp, 1);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __request_module function in kernel/kmod.c in the Linux kernel before 3.4 does not set a certain killable attribute, which allows local users to cause a denial of service (memory consumption) via a crafted application.",
        "id": 95
    },
    {
        "cve_id": "CVE-2018-14641",
        "code_before_change": "static bool\nnf_ct_frag6_reasm(struct frag_queue *fq, struct sk_buff *prev,  struct net_device *dev)\n{\n\tstruct sk_buff *fp, *head = fq->q.fragments;\n\tint    payload_len;\n\tu8 ecn;\n\n\tinet_frag_kill(&fq->q);\n\n\tWARN_ON(head == NULL);\n\tWARN_ON(head->ip_defrag_offset != 0);\n\n\tecn = ip_frag_ecn_table[fq->ecn];\n\tif (unlikely(ecn == 0xff))\n\t\treturn false;\n\n\t/* Unfragmented part is taken from the first segment. */\n\tpayload_len = ((head->data - skb_network_header(head)) -\n\t\t       sizeof(struct ipv6hdr) + fq->q.len -\n\t\t       sizeof(struct frag_hdr));\n\tif (payload_len > IPV6_MAXPLEN) {\n\t\tnet_dbg_ratelimited(\"nf_ct_frag6_reasm: payload len = %d\\n\",\n\t\t\t\t    payload_len);\n\t\treturn false;\n\t}\n\n\t/* Head of list must not be cloned. */\n\tif (skb_unclone(head, GFP_ATOMIC))\n\t\treturn false;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tclone = alloc_skb(0, GFP_ATOMIC);\n\t\tif (clone == NULL)\n\t\t\treturn false;\n\n\t\tclone->next = head->next;\n\t\thead->next = clone;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->data_len -= clone->len;\n\t\thead->len -= clone->len;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\n\t\tadd_frag_mem_limit(fq->q.net, clone->truesize);\n\t}\n\n\t/* morph head into last received skb: prev.\n\t *\n\t * This allows callers of ipv6 conntrack defrag to continue\n\t * to use the last skb(frag) passed into the reasm engine.\n\t * The last skb frag 'silently' turns into the full reassembled skb.\n\t *\n\t * Since prev is also part of q->fragments we have to clone it first.\n\t */\n\tif (head != prev) {\n\t\tstruct sk_buff *iter;\n\n\t\tfp = skb_clone(prev, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\treturn false;\n\n\t\tfp->next = prev->next;\n\n\t\titer = head;\n\t\twhile (iter) {\n\t\t\tif (iter->next == prev) {\n\t\t\t\titer->next = fp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\titer = iter->next;\n\t\t}\n\n\t\tskb_morph(prev, head);\n\t\tprev->next = head->next;\n\t\tconsume_skb(head);\n\t\thead = prev;\n\t}\n\n\t/* We have to remove fragment header from datagram and to relocate\n\t * header in order to calculate ICV correctly. */\n\tskb_network_header(head)[fq->nhoffset] = skb_transport_header(head)[0];\n\tmemmove(head->head + sizeof(struct frag_hdr), head->head,\n\t\t(head->data - head->head) - sizeof(struct frag_hdr));\n\thead->mac_header += sizeof(struct frag_hdr);\n\thead->network_header += sizeof(struct frag_hdr);\n\n\tskb_shinfo(head)->frag_list = head->next;\n\tskb_reset_transport_header(head);\n\tskb_push(head, head->data - skb_network_header(head));\n\n\tfor (fp = head->next; fp; fp = fp->next) {\n\t\thead->data_len += fp->len;\n\t\thead->len += fp->len;\n\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\t\thead->truesize += fp->truesize;\n\t}\n\tsub_frag_mem_limit(fq->q.net, head->truesize);\n\n\thead->ignore_df = 1;\n\thead->next = NULL;\n\thead->dev = dev;\n\thead->tstamp = fq->q.stamp;\n\tipv6_hdr(head)->payload_len = htons(payload_len);\n\tipv6_change_dsfield(ipv6_hdr(head), 0xff, ecn);\n\tIP6CB(head)->frag_max_size = sizeof(struct ipv6hdr) + fq->q.max_size;\n\n\t/* Yes, and fold redundant checksum back. 8) */\n\tif (head->ip_summed == CHECKSUM_COMPLETE)\n\t\thead->csum = csum_partial(skb_network_header(head),\n\t\t\t\t\t  skb_network_header_len(head),\n\t\t\t\t\t  head->csum);\n\n\tfq->q.fragments = NULL;\n\tfq->q.rb_fragments = RB_ROOT;\n\tfq->q.fragments_tail = NULL;\n\n\treturn true;\n}",
        "code_after_change": "static bool\nnf_ct_frag6_reasm(struct frag_queue *fq, struct sk_buff *prev,  struct net_device *dev)\n{\n\tstruct sk_buff *fp, *head = fq->q.fragments;\n\tint    payload_len;\n\tu8 ecn;\n\n\tinet_frag_kill(&fq->q);\n\n\tWARN_ON(head == NULL);\n\tWARN_ON(head->ip_defrag_offset != 0);\n\n\tecn = ip_frag_ecn_table[fq->ecn];\n\tif (unlikely(ecn == 0xff))\n\t\treturn false;\n\n\t/* Unfragmented part is taken from the first segment. */\n\tpayload_len = ((head->data - skb_network_header(head)) -\n\t\t       sizeof(struct ipv6hdr) + fq->q.len -\n\t\t       sizeof(struct frag_hdr));\n\tif (payload_len > IPV6_MAXPLEN) {\n\t\tnet_dbg_ratelimited(\"nf_ct_frag6_reasm: payload len = %d\\n\",\n\t\t\t\t    payload_len);\n\t\treturn false;\n\t}\n\n\t/* Head of list must not be cloned. */\n\tif (skb_unclone(head, GFP_ATOMIC))\n\t\treturn false;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tclone = alloc_skb(0, GFP_ATOMIC);\n\t\tif (clone == NULL)\n\t\t\treturn false;\n\n\t\tclone->next = head->next;\n\t\thead->next = clone;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->data_len -= clone->len;\n\t\thead->len -= clone->len;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\n\t\tadd_frag_mem_limit(fq->q.net, clone->truesize);\n\t}\n\n\t/* morph head into last received skb: prev.\n\t *\n\t * This allows callers of ipv6 conntrack defrag to continue\n\t * to use the last skb(frag) passed into the reasm engine.\n\t * The last skb frag 'silently' turns into the full reassembled skb.\n\t *\n\t * Since prev is also part of q->fragments we have to clone it first.\n\t */\n\tif (head != prev) {\n\t\tstruct sk_buff *iter;\n\n\t\tfp = skb_clone(prev, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\treturn false;\n\n\t\tfp->next = prev->next;\n\n\t\titer = head;\n\t\twhile (iter) {\n\t\t\tif (iter->next == prev) {\n\t\t\t\titer->next = fp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\titer = iter->next;\n\t\t}\n\n\t\tskb_morph(prev, head);\n\t\tprev->next = head->next;\n\t\tconsume_skb(head);\n\t\thead = prev;\n\t}\n\n\t/* We have to remove fragment header from datagram and to relocate\n\t * header in order to calculate ICV correctly. */\n\tskb_network_header(head)[fq->nhoffset] = skb_transport_header(head)[0];\n\tmemmove(head->head + sizeof(struct frag_hdr), head->head,\n\t\t(head->data - head->head) - sizeof(struct frag_hdr));\n\thead->mac_header += sizeof(struct frag_hdr);\n\thead->network_header += sizeof(struct frag_hdr);\n\n\tskb_shinfo(head)->frag_list = head->next;\n\tskb_reset_transport_header(head);\n\tskb_push(head, head->data - skb_network_header(head));\n\n\tfor (fp = head->next; fp; fp = fp->next) {\n\t\thead->data_len += fp->len;\n\t\thead->len += fp->len;\n\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\t\thead->truesize += fp->truesize;\n\t\tfp->sk = NULL;\n\t}\n\tsub_frag_mem_limit(fq->q.net, head->truesize);\n\n\thead->ignore_df = 1;\n\thead->next = NULL;\n\thead->dev = dev;\n\thead->tstamp = fq->q.stamp;\n\tipv6_hdr(head)->payload_len = htons(payload_len);\n\tipv6_change_dsfield(ipv6_hdr(head), 0xff, ecn);\n\tIP6CB(head)->frag_max_size = sizeof(struct ipv6hdr) + fq->q.max_size;\n\n\t/* Yes, and fold redundant checksum back. 8) */\n\tif (head->ip_summed == CHECKSUM_COMPLETE)\n\t\thead->csum = csum_partial(skb_network_header(head),\n\t\t\t\t\t  skb_network_header_len(head),\n\t\t\t\t\t  head->csum);\n\n\tfq->q.fragments = NULL;\n\tfq->q.rb_fragments = RB_ROOT;\n\tfq->q.fragments_tail = NULL;\n\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -106,6 +106,7 @@\n \t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n \t\t\thead->csum = csum_add(head->csum, fp->csum);\n \t\thead->truesize += fp->truesize;\n+\t\tfp->sk = NULL;\n \t}\n \tsub_frag_mem_limit(fq->q.net, head->truesize);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tfp->sk = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A security flaw was found in the ip_frag_reasm() function in net/ipv4/ip_fragment.c in the Linux kernel from 4.19-rc1 to 4.19-rc3 inclusive, which can cause a later system crash in ip_do_fragment(). With certain non-default, but non-rare, configuration of a victim host, an attacker can trigger this crash remotely, thus leading to a remote denial-of-service.",
        "id": 1700
    },
    {
        "cve_id": "CVE-2008-7316",
        "code_before_change": "static void __iov_iter_advance_iov(struct iov_iter *i, size_t bytes)\n{\n\tif (likely(i->nr_segs == 1)) {\n\t\ti->iov_offset += bytes;\n\t} else {\n\t\tconst struct iovec *iov = i->iov;\n\t\tsize_t base = i->iov_offset;\n\n\t\twhile (bytes) {\n\t\t\tint copy = min(bytes, iov->iov_len - base);\n\n\t\t\tbytes -= copy;\n\t\t\tbase += copy;\n\t\t\tif (iov->iov_len == base) {\n\t\t\t\tiov++;\n\t\t\t\tbase = 0;\n\t\t\t}\n\t\t}\n\t\ti->iov = iov;\n\t\ti->iov_offset = base;\n\t}\n}",
        "code_after_change": "static void __iov_iter_advance_iov(struct iov_iter *i, size_t bytes)\n{\n\tif (likely(i->nr_segs == 1)) {\n\t\ti->iov_offset += bytes;\n\t} else {\n\t\tconst struct iovec *iov = i->iov;\n\t\tsize_t base = i->iov_offset;\n\n\t\t/*\n\t\t * The !iov->iov_len check ensures we skip over unlikely\n\t\t * zero-length segments.\n\t\t */\n\t\twhile (bytes || !iov->iov_len) {\n\t\t\tint copy = min(bytes, iov->iov_len - base);\n\n\t\t\tbytes -= copy;\n\t\t\tbase += copy;\n\t\t\tif (iov->iov_len == base) {\n\t\t\t\tiov++;\n\t\t\t\tbase = 0;\n\t\t\t}\n\t\t}\n\t\ti->iov = iov;\n\t\ti->iov_offset = base;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,11 @@\n \t\tconst struct iovec *iov = i->iov;\n \t\tsize_t base = i->iov_offset;\n \n-\t\twhile (bytes) {\n+\t\t/*\n+\t\t * The !iov->iov_len check ensures we skip over unlikely\n+\t\t * zero-length segments.\n+\t\t */\n+\t\twhile (bytes || !iov->iov_len) {\n \t\t\tint copy = min(bytes, iov->iov_len - base);\n \n \t\t\tbytes -= copy;",
        "function_modified_lines": {
            "added": [
                "\t\t/*",
                "\t\t * The !iov->iov_len check ensures we skip over unlikely",
                "\t\t * zero-length segments.",
                "\t\t */",
                "\t\twhile (bytes || !iov->iov_len) {"
            ],
            "deleted": [
                "\t\twhile (bytes) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "mm/filemap.c in the Linux kernel before 2.6.25 allows local users to cause a denial of service (infinite loop) via a writev system call that triggers an iovec of zero length, followed by a page fault for an iovec of nonzero length.",
        "id": 9
    },
    {
        "cve_id": "CVE-2019-9453",
        "code_before_change": "static int __f2fs_setxattr(struct inode *inode, int index,\n\t\t\tconst char *name, const void *value, size_t size,\n\t\t\tstruct page *ipage, int flags)\n{\n\tstruct f2fs_xattr_entry *here, *last;\n\tvoid *base_addr;\n\tint found, newsize;\n\tsize_t len;\n\t__u32 new_hsize;\n\tint error = 0;\n\n\tif (name == NULL)\n\t\treturn -EINVAL;\n\n\tif (value == NULL)\n\t\tsize = 0;\n\n\tlen = strlen(name);\n\n\tif (len > F2FS_NAME_LEN)\n\t\treturn -ERANGE;\n\n\tif (size > MAX_VALUE_LEN(inode))\n\t\treturn -E2BIG;\n\n\terror = read_all_xattrs(inode, ipage, &base_addr);\n\tif (error)\n\t\treturn error;\n\n\t/* find entry with wanted name. */\n\there = __find_xattr(base_addr, index, len, name);\n\n\tfound = IS_XATTR_LAST_ENTRY(here) ? 0 : 1;\n\n\tif (found) {\n\t\tif ((flags & XATTR_CREATE)) {\n\t\t\terror = -EEXIST;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tif (value && f2fs_xattr_value_same(here, value, size))\n\t\t\tgoto exit;\n\t} else if ((flags & XATTR_REPLACE)) {\n\t\terror = -ENODATA;\n\t\tgoto exit;\n\t}\n\n\tlast = here;\n\twhile (!IS_XATTR_LAST_ENTRY(last))\n\t\tlast = XATTR_NEXT_ENTRY(last);\n\n\tnewsize = XATTR_ALIGN(sizeof(struct f2fs_xattr_entry) + len + size);\n\n\t/* 1. Check space */\n\tif (value) {\n\t\tint free;\n\t\t/*\n\t\t * If value is NULL, it is remove operation.\n\t\t * In case of update operation, we calculate free.\n\t\t */\n\t\tfree = MIN_OFFSET(inode) - ((char *)last - (char *)base_addr);\n\t\tif (found)\n\t\t\tfree = free + ENTRY_SIZE(here);\n\n\t\tif (unlikely(free < newsize)) {\n\t\t\terror = -E2BIG;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\n\t/* 2. Remove old entry */\n\tif (found) {\n\t\t/*\n\t\t * If entry is found, remove old entry.\n\t\t * If not found, remove operation is not needed.\n\t\t */\n\t\tstruct f2fs_xattr_entry *next = XATTR_NEXT_ENTRY(here);\n\t\tint oldsize = ENTRY_SIZE(here);\n\n\t\tmemmove(here, next, (char *)last - (char *)next);\n\t\tlast = (struct f2fs_xattr_entry *)((char *)last - oldsize);\n\t\tmemset(last, 0, oldsize);\n\t}\n\n\tnew_hsize = (char *)last - (char *)base_addr;\n\n\t/* 3. Write new entry */\n\tif (value) {\n\t\tchar *pval;\n\t\t/*\n\t\t * Before we come here, old entry is removed.\n\t\t * We just write new entry.\n\t\t */\n\t\tlast->e_name_index = index;\n\t\tlast->e_name_len = len;\n\t\tmemcpy(last->e_name, name, len);\n\t\tpval = last->e_name + len;\n\t\tmemcpy(pval, value, size);\n\t\tlast->e_value_size = cpu_to_le16(size);\n\t\tnew_hsize += newsize;\n\t}\n\n\terror = write_all_xattrs(inode, new_hsize, base_addr, ipage);\n\tif (error)\n\t\tgoto exit;\n\n\tif (is_inode_flag_set(inode, FI_ACL_MODE)) {\n\t\tinode->i_mode = F2FS_I(inode)->i_acl_mode;\n\t\tinode->i_ctime = current_time(inode);\n\t\tclear_inode_flag(inode, FI_ACL_MODE);\n\t}\n\tif (index == F2FS_XATTR_INDEX_ENCRYPTION &&\n\t\t\t!strcmp(name, F2FS_XATTR_NAME_ENCRYPTION_CONTEXT))\n\t\tf2fs_set_encrypted_inode(inode);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (!error && S_ISDIR(inode->i_mode))\n\t\tset_sbi_flag(F2FS_I_SB(inode), SBI_NEED_CP);\nexit:\n\tkvfree(base_addr);\n\treturn error;\n}",
        "code_after_change": "static int __f2fs_setxattr(struct inode *inode, int index,\n\t\t\tconst char *name, const void *value, size_t size,\n\t\t\tstruct page *ipage, int flags)\n{\n\tstruct f2fs_xattr_entry *here, *last;\n\tvoid *base_addr, *last_base_addr;\n\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n\tint found, newsize;\n\tsize_t len;\n\t__u32 new_hsize;\n\tint error = 0;\n\n\tif (name == NULL)\n\t\treturn -EINVAL;\n\n\tif (value == NULL)\n\t\tsize = 0;\n\n\tlen = strlen(name);\n\n\tif (len > F2FS_NAME_LEN)\n\t\treturn -ERANGE;\n\n\tif (size > MAX_VALUE_LEN(inode))\n\t\treturn -E2BIG;\n\n\terror = read_all_xattrs(inode, ipage, &base_addr);\n\tif (error)\n\t\treturn error;\n\n\tlast_base_addr = (void *)base_addr + XATTR_SIZE(xnid, inode);\n\n\t/* find entry with wanted name. */\n\there = __find_xattr(base_addr, last_base_addr, index, len, name);\n\tif (!here) {\n\t\terror = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tfound = IS_XATTR_LAST_ENTRY(here) ? 0 : 1;\n\n\tif (found) {\n\t\tif ((flags & XATTR_CREATE)) {\n\t\t\terror = -EEXIST;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tif (value && f2fs_xattr_value_same(here, value, size))\n\t\t\tgoto exit;\n\t} else if ((flags & XATTR_REPLACE)) {\n\t\terror = -ENODATA;\n\t\tgoto exit;\n\t}\n\n\tlast = here;\n\twhile (!IS_XATTR_LAST_ENTRY(last))\n\t\tlast = XATTR_NEXT_ENTRY(last);\n\n\tnewsize = XATTR_ALIGN(sizeof(struct f2fs_xattr_entry) + len + size);\n\n\t/* 1. Check space */\n\tif (value) {\n\t\tint free;\n\t\t/*\n\t\t * If value is NULL, it is remove operation.\n\t\t * In case of update operation, we calculate free.\n\t\t */\n\t\tfree = MIN_OFFSET(inode) - ((char *)last - (char *)base_addr);\n\t\tif (found)\n\t\t\tfree = free + ENTRY_SIZE(here);\n\n\t\tif (unlikely(free < newsize)) {\n\t\t\terror = -E2BIG;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\n\t/* 2. Remove old entry */\n\tif (found) {\n\t\t/*\n\t\t * If entry is found, remove old entry.\n\t\t * If not found, remove operation is not needed.\n\t\t */\n\t\tstruct f2fs_xattr_entry *next = XATTR_NEXT_ENTRY(here);\n\t\tint oldsize = ENTRY_SIZE(here);\n\n\t\tmemmove(here, next, (char *)last - (char *)next);\n\t\tlast = (struct f2fs_xattr_entry *)((char *)last - oldsize);\n\t\tmemset(last, 0, oldsize);\n\t}\n\n\tnew_hsize = (char *)last - (char *)base_addr;\n\n\t/* 3. Write new entry */\n\tif (value) {\n\t\tchar *pval;\n\t\t/*\n\t\t * Before we come here, old entry is removed.\n\t\t * We just write new entry.\n\t\t */\n\t\tlast->e_name_index = index;\n\t\tlast->e_name_len = len;\n\t\tmemcpy(last->e_name, name, len);\n\t\tpval = last->e_name + len;\n\t\tmemcpy(pval, value, size);\n\t\tlast->e_value_size = cpu_to_le16(size);\n\t\tnew_hsize += newsize;\n\t}\n\n\terror = write_all_xattrs(inode, new_hsize, base_addr, ipage);\n\tif (error)\n\t\tgoto exit;\n\n\tif (is_inode_flag_set(inode, FI_ACL_MODE)) {\n\t\tinode->i_mode = F2FS_I(inode)->i_acl_mode;\n\t\tinode->i_ctime = current_time(inode);\n\t\tclear_inode_flag(inode, FI_ACL_MODE);\n\t}\n\tif (index == F2FS_XATTR_INDEX_ENCRYPTION &&\n\t\t\t!strcmp(name, F2FS_XATTR_NAME_ENCRYPTION_CONTEXT))\n\t\tf2fs_set_encrypted_inode(inode);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (!error && S_ISDIR(inode->i_mode))\n\t\tset_sbi_flag(F2FS_I_SB(inode), SBI_NEED_CP);\nexit:\n\tkvfree(base_addr);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,8 @@\n \t\t\tstruct page *ipage, int flags)\n {\n \tstruct f2fs_xattr_entry *here, *last;\n-\tvoid *base_addr;\n+\tvoid *base_addr, *last_base_addr;\n+\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n \tint found, newsize;\n \tsize_t len;\n \t__u32 new_hsize;\n@@ -27,8 +28,14 @@\n \tif (error)\n \t\treturn error;\n \n+\tlast_base_addr = (void *)base_addr + XATTR_SIZE(xnid, inode);\n+\n \t/* find entry with wanted name. */\n-\there = __find_xattr(base_addr, index, len, name);\n+\there = __find_xattr(base_addr, last_base_addr, index, len, name);\n+\tif (!here) {\n+\t\terror = -EFAULT;\n+\t\tgoto exit;\n+\t}\n \n \tfound = IS_XATTR_LAST_ENTRY(here) ? 0 : 1;\n ",
        "function_modified_lines": {
            "added": [
                "\tvoid *base_addr, *last_base_addr;",
                "\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;",
                "\tlast_base_addr = (void *)base_addr + XATTR_SIZE(xnid, inode);",
                "",
                "\there = __find_xattr(base_addr, last_base_addr, index, len, name);",
                "\tif (!here) {",
                "\t\terror = -EFAULT;",
                "\t\tgoto exit;",
                "\t}"
            ],
            "deleted": [
                "\tvoid *base_addr;",
                "\there = __find_xattr(base_addr, index, len, name);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "In the Android kernel in F2FS touch driver there is a possible out of bounds read due to improper input validation. This could lead to local information disclosure with system execution privileges needed. User interaction is not needed for exploitation.",
        "id": 2357
    },
    {
        "cve_id": "CVE-2013-2897",
        "code_before_change": "static int mt_touch_input_mapping(struct hid_device *hdev, struct hid_input *hi,\n\t\tstruct hid_field *field, struct hid_usage *usage,\n\t\tunsigned long **bit, int *max)\n{\n\tstruct mt_device *td = hid_get_drvdata(hdev);\n\tstruct mt_class *cls = &td->mtclass;\n\tint code;\n\tstruct hid_usage *prev_usage = NULL;\n\n\tif (field->application == HID_DG_TOUCHSCREEN)\n\t\ttd->mt_flags |= INPUT_MT_DIRECT;\n\n\t/*\n\t * Model touchscreens providing buttons as touchpads.\n\t */\n\tif (field->application == HID_DG_TOUCHPAD ||\n\t    (usage->hid & HID_USAGE_PAGE) == HID_UP_BUTTON)\n\t\ttd->mt_flags |= INPUT_MT_POINTER;\n\n\tif (usage->usage_index)\n\t\tprev_usage = &field->usage[usage->usage_index - 1];\n\n\tswitch (usage->hid & HID_USAGE_PAGE) {\n\n\tcase HID_UP_GENDESK:\n\t\tswitch (usage->hid) {\n\t\tcase HID_GD_X:\n\t\t\tif (prev_usage && (prev_usage->hid == usage->hid)) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOOL_X);\n\t\t\t\tset_abs(hi->input, ABS_MT_TOOL_X, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t} else {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_POSITION_X);\n\t\t\t\tset_abs(hi->input, ABS_MT_POSITION_X, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t}\n\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_GD_Y:\n\t\t\tif (prev_usage && (prev_usage->hid == usage->hid)) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOOL_Y);\n\t\t\t\tset_abs(hi->input, ABS_MT_TOOL_Y, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t} else {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_POSITION_Y);\n\t\t\t\tset_abs(hi->input, ABS_MT_POSITION_Y, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t}\n\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\t}\n\t\treturn 0;\n\n\tcase HID_UP_DIGITIZER:\n\t\tswitch (usage->hid) {\n\t\tcase HID_DG_INRANGE:\n\t\t\tif (cls->quirks & MT_QUIRK_HOVERING) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_DISTANCE);\n\t\t\t\tinput_set_abs_params(hi->input,\n\t\t\t\t\tABS_MT_DISTANCE, 0, 1, 0, 0);\n\t\t\t}\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONFIDENCE:\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_TIPSWITCH:\n\t\t\thid_map_usage(hi, usage, bit, max, EV_KEY, BTN_TOUCH);\n\t\t\tinput_set_capability(hi->input, EV_KEY, BTN_TOUCH);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTID:\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\ttd->touches_by_report++;\n\t\t\ttd->mt_report_id = field->report->id;\n\t\t\treturn 1;\n\t\tcase HID_DG_WIDTH:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOUCH_MAJOR);\n\t\t\tif (!(cls->quirks & MT_QUIRK_NO_AREA))\n\t\t\t\tset_abs(hi->input, ABS_MT_TOUCH_MAJOR, field,\n\t\t\t\t\tcls->sn_width);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_HEIGHT:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOUCH_MINOR);\n\t\t\tif (!(cls->quirks & MT_QUIRK_NO_AREA)) {\n\t\t\t\tset_abs(hi->input, ABS_MT_TOUCH_MINOR, field,\n\t\t\t\t\tcls->sn_height);\n\t\t\t\tinput_set_abs_params(hi->input,\n\t\t\t\t\tABS_MT_ORIENTATION, 0, 1, 0, 0);\n\t\t\t}\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_TIPPRESSURE:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_PRESSURE);\n\t\t\tset_abs(hi->input, ABS_MT_PRESSURE, field,\n\t\t\t\tcls->sn_pressure);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTCOUNT:\n\t\t\ttd->cc_index = field->index;\n\t\t\ttd->cc_value_index = usage->usage_index;\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTMAX:\n\t\t\t/* we don't set td->last_slot_field as contactcount and\n\t\t\t * contact max are global to the report */\n\t\t\treturn -1;\n\t\tcase HID_DG_TOUCH:\n\t\t\t/* Legacy devices use TIPSWITCH and not TOUCH.\n\t\t\t * Let's just ignore this field. */\n\t\t\treturn -1;\n\t\t}\n\t\t/* let hid-input decide for the others */\n\t\treturn 0;\n\n\tcase HID_UP_BUTTON:\n\t\tcode = BTN_MOUSE + ((usage->hid - 1) & HID_USAGE);\n\t\thid_map_usage(hi, usage, bit, max, EV_KEY, code);\n\t\tinput_set_capability(hi->input, EV_KEY, code);\n\t\treturn 1;\n\n\tcase 0xff000000:\n\t\t/* we do not want to map these: no input-oriented meaning */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int mt_touch_input_mapping(struct hid_device *hdev, struct hid_input *hi,\n\t\tstruct hid_field *field, struct hid_usage *usage,\n\t\tunsigned long **bit, int *max)\n{\n\tstruct mt_device *td = hid_get_drvdata(hdev);\n\tstruct mt_class *cls = &td->mtclass;\n\tint code;\n\tstruct hid_usage *prev_usage = NULL;\n\n\tif (field->application == HID_DG_TOUCHSCREEN)\n\t\ttd->mt_flags |= INPUT_MT_DIRECT;\n\n\t/*\n\t * Model touchscreens providing buttons as touchpads.\n\t */\n\tif (field->application == HID_DG_TOUCHPAD ||\n\t    (usage->hid & HID_USAGE_PAGE) == HID_UP_BUTTON)\n\t\ttd->mt_flags |= INPUT_MT_POINTER;\n\n\tif (usage->usage_index)\n\t\tprev_usage = &field->usage[usage->usage_index - 1];\n\n\tswitch (usage->hid & HID_USAGE_PAGE) {\n\n\tcase HID_UP_GENDESK:\n\t\tswitch (usage->hid) {\n\t\tcase HID_GD_X:\n\t\t\tif (prev_usage && (prev_usage->hid == usage->hid)) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOOL_X);\n\t\t\t\tset_abs(hi->input, ABS_MT_TOOL_X, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t} else {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_POSITION_X);\n\t\t\t\tset_abs(hi->input, ABS_MT_POSITION_X, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t}\n\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_GD_Y:\n\t\t\tif (prev_usage && (prev_usage->hid == usage->hid)) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOOL_Y);\n\t\t\t\tset_abs(hi->input, ABS_MT_TOOL_Y, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t} else {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_POSITION_Y);\n\t\t\t\tset_abs(hi->input, ABS_MT_POSITION_Y, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t}\n\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\t}\n\t\treturn 0;\n\n\tcase HID_UP_DIGITIZER:\n\t\tswitch (usage->hid) {\n\t\tcase HID_DG_INRANGE:\n\t\t\tif (cls->quirks & MT_QUIRK_HOVERING) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_DISTANCE);\n\t\t\t\tinput_set_abs_params(hi->input,\n\t\t\t\t\tABS_MT_DISTANCE, 0, 1, 0, 0);\n\t\t\t}\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONFIDENCE:\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_TIPSWITCH:\n\t\t\thid_map_usage(hi, usage, bit, max, EV_KEY, BTN_TOUCH);\n\t\t\tinput_set_capability(hi->input, EV_KEY, BTN_TOUCH);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTID:\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\ttd->touches_by_report++;\n\t\t\ttd->mt_report_id = field->report->id;\n\t\t\treturn 1;\n\t\tcase HID_DG_WIDTH:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOUCH_MAJOR);\n\t\t\tif (!(cls->quirks & MT_QUIRK_NO_AREA))\n\t\t\t\tset_abs(hi->input, ABS_MT_TOUCH_MAJOR, field,\n\t\t\t\t\tcls->sn_width);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_HEIGHT:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOUCH_MINOR);\n\t\t\tif (!(cls->quirks & MT_QUIRK_NO_AREA)) {\n\t\t\t\tset_abs(hi->input, ABS_MT_TOUCH_MINOR, field,\n\t\t\t\t\tcls->sn_height);\n\t\t\t\tinput_set_abs_params(hi->input,\n\t\t\t\t\tABS_MT_ORIENTATION, 0, 1, 0, 0);\n\t\t\t}\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_TIPPRESSURE:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_PRESSURE);\n\t\t\tset_abs(hi->input, ABS_MT_PRESSURE, field,\n\t\t\t\tcls->sn_pressure);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTCOUNT:\n\t\t\t/* Ignore if indexes are out of bounds. */\n\t\t\tif (field->index >= field->report->maxfield ||\n\t\t\t    usage->usage_index >= field->report_count)\n\t\t\t\treturn 1;\n\t\t\ttd->cc_index = field->index;\n\t\t\ttd->cc_value_index = usage->usage_index;\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTMAX:\n\t\t\t/* we don't set td->last_slot_field as contactcount and\n\t\t\t * contact max are global to the report */\n\t\t\treturn -1;\n\t\tcase HID_DG_TOUCH:\n\t\t\t/* Legacy devices use TIPSWITCH and not TOUCH.\n\t\t\t * Let's just ignore this field. */\n\t\t\treturn -1;\n\t\t}\n\t\t/* let hid-input decide for the others */\n\t\treturn 0;\n\n\tcase HID_UP_BUTTON:\n\t\tcode = BTN_MOUSE + ((usage->hid - 1) & HID_USAGE);\n\t\thid_map_usage(hi, usage, bit, max, EV_KEY, code);\n\t\tinput_set_capability(hi->input, EV_KEY, code);\n\t\treturn 1;\n\n\tcase 0xff000000:\n\t\t/* we do not want to map these: no input-oriented meaning */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -108,6 +108,10 @@\n \t\t\tmt_store_field(usage, td, hi);\n \t\t\treturn 1;\n \t\tcase HID_DG_CONTACTCOUNT:\n+\t\t\t/* Ignore if indexes are out of bounds. */\n+\t\t\tif (field->index >= field->report->maxfield ||\n+\t\t\t    usage->usage_index >= field->report_count)\n+\t\t\t\treturn 1;\n \t\t\ttd->cc_index = field->index;\n \t\t\ttd->cc_value_index = usage->usage_index;\n \t\t\treturn 1;",
        "function_modified_lines": {
            "added": [
                "\t\t\t/* Ignore if indexes are out of bounds. */",
                "\t\t\tif (field->index >= field->report->maxfield ||",
                "\t\t\t    usage->usage_index >= field->report_count)",
                "\t\t\t\treturn 1;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Multiple array index errors in drivers/hid/hid-multitouch.c in the Human Interface Device (HID) subsystem in the Linux kernel through 3.11, when CONFIG_HID_MULTITOUCH is enabled, allow physically proximate attackers to cause a denial of service (heap memory corruption, or NULL pointer dereference and OOPS) via a crafted device.",
        "id": 257
    },
    {
        "cve_id": "CVE-2016-2549",
        "code_before_change": "static int snd_hrtimer_stop(struct snd_timer *t)\n{\n\tstruct snd_hrtimer *stime = t->private_data;\n\tatomic_set(&stime->running, 0);\n\treturn 0;\n}",
        "code_after_change": "static int snd_hrtimer_stop(struct snd_timer *t)\n{\n\tstruct snd_hrtimer *stime = t->private_data;\n\tatomic_set(&stime->running, 0);\n\thrtimer_try_to_cancel(&stime->hrt);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,5 +2,6 @@\n {\n \tstruct snd_hrtimer *stime = t->private_data;\n \tatomic_set(&stime->running, 0);\n+\thrtimer_try_to_cancel(&stime->hrt);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\thrtimer_try_to_cancel(&stime->hrt);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "sound/core/hrtimer.c in the Linux kernel before 4.4.1 does not prevent recursive callback access, which allows local users to cause a denial of service (deadlock) via a crafted ioctl call.",
        "id": 948
    },
    {
        "cve_id": "CVE-2019-9503",
        "code_before_change": "void brcmf_rx_event(struct device *dev, struct sk_buff *skb)\n{\n\tstruct brcmf_if *ifp;\n\tstruct brcmf_bus *bus_if = dev_get_drvdata(dev);\n\tstruct brcmf_pub *drvr = bus_if->drvr;\n\n\tbrcmf_dbg(EVENT, \"Enter: %s: rxp=%p\\n\", dev_name(dev), skb);\n\n\tif (brcmf_rx_hdrpull(drvr, skb, &ifp))\n\t\treturn;\n\n\tbrcmf_fweh_process_skb(ifp->drvr, skb);\n\tbrcmu_pkt_buf_free_skb(skb);\n}",
        "code_after_change": "void brcmf_rx_event(struct device *dev, struct sk_buff *skb)\n{\n\tstruct brcmf_if *ifp;\n\tstruct brcmf_bus *bus_if = dev_get_drvdata(dev);\n\tstruct brcmf_pub *drvr = bus_if->drvr;\n\n\tbrcmf_dbg(EVENT, \"Enter: %s: rxp=%p\\n\", dev_name(dev), skb);\n\n\tif (brcmf_rx_hdrpull(drvr, skb, &ifp))\n\t\treturn;\n\n\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);\n\tbrcmu_pkt_buf_free_skb(skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,6 @@\n \tif (brcmf_rx_hdrpull(drvr, skb, &ifp))\n \t\treturn;\n \n-\tbrcmf_fweh_process_skb(ifp->drvr, skb);\n+\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);\n \tbrcmu_pkt_buf_free_skb(skb);\n }",
        "function_modified_lines": {
            "added": [
                "\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);"
            ],
            "deleted": [
                "\tbrcmf_fweh_process_skb(ifp->drvr, skb);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Broadcom brcmfmac WiFi driver prior to commit a4176ec356c73a46c07c181c6d04039fafa34a9f is vulnerable to a frame validation bypass. If the brcmfmac driver receives a firmware event frame from a remote source, the is_wlc_event_frame function will cause this frame to be discarded and unprocessed. If the driver receives the firmware event frame from the host, the appropriate handler is called. This frame validation can be bypassed if the bus used is USB (for instance by a wifi dongle). This can allow firmware event frames from a remote source to be processed. In the worst case scenario, by sending specially-crafted WiFi packets, a remote, unauthenticated attacker may be able to execute arbitrary code on a vulnerable system. More typically, this vulnerability will result in denial-of-service conditions.",
        "id": 2366
    },
    {
        "cve_id": "CVE-2016-2548",
        "code_before_change": "int snd_timer_close(struct snd_timer_instance *timeri)\n{\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t}\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tmutex_unlock(&register_mutex);\n\t} else {\n\t\ttimer = timeri->timer;\n\t\tif (snd_BUG_ON(!timer))\n\t\t\tgoto out;\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tif (timer && list_empty(&timer->open_list_head) &&\n\t\t    timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* remove slave links */\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t\tmutex_unlock(&register_mutex);\n\t}\n out:\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\tif (timer)\n\t\tmodule_put(timer->module);\n\treturn 0;\n}",
        "code_after_change": "int snd_timer_close(struct snd_timer_instance *timeri)\n{\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t}\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tmutex_unlock(&register_mutex);\n\t} else {\n\t\ttimer = timeri->timer;\n\t\tif (snd_BUG_ON(!timer))\n\t\t\tgoto out;\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tif (timer && list_empty(&timer->open_list_head) &&\n\t\t    timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* remove slave links */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\tspin_lock(&timer->lock);\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tlist_del_init(&slave->ack_list);\n\t\t\tlist_del_init(&slave->active_list);\n\t\t}\n\t\tspin_unlock(&timer->lock);\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_unlock(&register_mutex);\n\t}\n out:\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\tif (timer)\n\t\tmodule_put(timer->module);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,15 +39,18 @@\n \t\t    timer->hw.close)\n \t\t\ttimer->hw.close(timer);\n \t\t/* remove slave links */\n+\t\tspin_lock_irq(&slave_active_lock);\n+\t\tspin_lock(&timer->lock);\n \t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n \t\t\t\t\t open_list) {\n-\t\t\tspin_lock_irq(&slave_active_lock);\n-\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);\n \t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n \t\t\tslave->master = NULL;\n \t\t\tslave->timer = NULL;\n-\t\t\tspin_unlock_irq(&slave_active_lock);\n+\t\t\tlist_del_init(&slave->ack_list);\n+\t\t\tlist_del_init(&slave->active_list);\n \t\t}\n+\t\tspin_unlock(&timer->lock);\n+\t\tspin_unlock_irq(&slave_active_lock);\n \t\tmutex_unlock(&register_mutex);\n \t}\n  out:",
        "function_modified_lines": {
            "added": [
                "\t\tspin_lock_irq(&slave_active_lock);",
                "\t\tspin_lock(&timer->lock);",
                "\t\t\tlist_del_init(&slave->ack_list);",
                "\t\t\tlist_del_init(&slave->active_list);",
                "\t\tspin_unlock(&timer->lock);",
                "\t\tspin_unlock_irq(&slave_active_lock);"
            ],
            "deleted": [
                "\t\t\tspin_lock_irq(&slave_active_lock);",
                "\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);",
                "\t\t\tspin_unlock_irq(&slave_active_lock);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 retains certain linked lists after a close or stop action, which allows local users to cause a denial of service (system crash) via a crafted ioctl call, related to the (1) snd_timer_close and (2) _snd_timer_stop functions.",
        "id": 947
    },
    {
        "cve_id": "CVE-2017-17805",
        "code_before_change": "static int encrypt(struct blkcipher_desc *desc,\n\t\t   struct scatterlist *dst, struct scatterlist *src,\n\t\t   unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct salsa20_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt_block(desc, &walk, 64);\n\n\tsalsa20_ivsetup(ctx, walk.iv);\n\n\tif (likely(walk.nbytes == nbytes))\n\t{\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr, nbytes);\n\t\treturn blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\twhile (walk.nbytes >= 64) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr,\n\t\t\t\t      walk.nbytes - (walk.nbytes % 64));\n\t\terr = blkcipher_walk_done(desc, &walk, walk.nbytes % 64);\n\t}\n\n\tif (walk.nbytes) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr, walk.nbytes);\n\t\terr = blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static int encrypt(struct blkcipher_desc *desc,\n\t\t   struct scatterlist *dst, struct scatterlist *src,\n\t\t   unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct salsa20_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt_block(desc, &walk, 64);\n\n\tsalsa20_ivsetup(ctx, walk.iv);\n\n\twhile (walk.nbytes >= 64) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr,\n\t\t\t\t      walk.nbytes - (walk.nbytes % 64));\n\t\terr = blkcipher_walk_done(desc, &walk, walk.nbytes % 64);\n\t}\n\n\tif (walk.nbytes) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr, walk.nbytes);\n\t\terr = blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,13 +11,6 @@\n \terr = blkcipher_walk_virt_block(desc, &walk, 64);\n \n \tsalsa20_ivsetup(ctx, walk.iv);\n-\n-\tif (likely(walk.nbytes == nbytes))\n-\t{\n-\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n-\t\t\t\t      walk.src.virt.addr, nbytes);\n-\t\treturn blkcipher_walk_done(desc, &walk, 0);\n-\t}\n \n \twhile (walk.nbytes >= 64) {\n \t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (likely(walk.nbytes == nbytes))",
                "\t{",
                "\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,",
                "\t\t\t\t      walk.src.virt.addr, nbytes);",
                "\t\treturn blkcipher_walk_done(desc, &walk, 0);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Salsa20 encryption algorithm in the Linux kernel before 4.14.8 does not correctly handle zero-length inputs, allowing a local attacker able to use the AF_ALG-based skcipher interface (CONFIG_CRYPTO_USER_API_SKCIPHER) to cause a denial of service (uninitialized-memory free and kernel crash) or have unspecified other impact by executing a crafted sequence of system calls that use the blkcipher_walk API. Both the generic implementation (crypto/salsa20_generic.c) and x86 implementation (arch/x86/crypto/salsa20_glue.c) of Salsa20 were vulnerable.",
        "id": 1374
    },
    {
        "cve_id": "CVE-2018-14656",
        "code_before_change": "void show_ip(struct pt_regs *regs, const char *loglvl)\n{\n#ifdef CONFIG_X86_32\n\tprintk(\"%sEIP: %pS\\n\", loglvl, (void *)regs->ip);\n#else\n\tprintk(\"%sRIP: %04x:%pS\\n\", loglvl, (int)regs->cs, (void *)regs->ip);\n#endif\n\tshow_opcodes((u8 *)regs->ip, loglvl);\n}",
        "code_after_change": "void show_ip(struct pt_regs *regs, const char *loglvl)\n{\n#ifdef CONFIG_X86_32\n\tprintk(\"%sEIP: %pS\\n\", loglvl, (void *)regs->ip);\n#else\n\tprintk(\"%sRIP: %04x:%pS\\n\", loglvl, (int)regs->cs, (void *)regs->ip);\n#endif\n\tshow_opcodes(regs, loglvl);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,5 +5,5 @@\n #else\n \tprintk(\"%sRIP: %04x:%pS\\n\", loglvl, (int)regs->cs, (void *)regs->ip);\n #endif\n-\tshow_opcodes((u8 *)regs->ip, loglvl);\n+\tshow_opcodes(regs, loglvl);\n }",
        "function_modified_lines": {
            "added": [
                "\tshow_opcodes(regs, loglvl);"
            ],
            "deleted": [
                "\tshow_opcodes((u8 *)regs->ip, loglvl);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A missing address check in the callers of the show_opcodes() in the Linux kernel allows an attacker to dump the kernel memory at an arbitrary kernel address into the dmesg log.",
        "id": 1703
    },
    {
        "cve_id": "CVE-2016-6197",
        "code_before_change": "static int ovl_rename2(struct inode *olddir, struct dentry *old,\n\t\t       struct inode *newdir, struct dentry *new,\n\t\t       unsigned int flags)\n{\n\tint err;\n\tenum ovl_path_type old_type;\n\tenum ovl_path_type new_type;\n\tstruct dentry *old_upperdir;\n\tstruct dentry *new_upperdir;\n\tstruct dentry *olddentry;\n\tstruct dentry *newdentry;\n\tstruct dentry *trap;\n\tbool old_opaque;\n\tbool new_opaque;\n\tbool new_create = false;\n\tbool cleanup_whiteout = false;\n\tbool overwrite = !(flags & RENAME_EXCHANGE);\n\tbool is_dir = d_is_dir(old);\n\tbool new_is_dir = false;\n\tstruct dentry *opaquedir = NULL;\n\tconst struct cred *old_cred = NULL;\n\tstruct cred *override_cred = NULL;\n\n\terr = -EINVAL;\n\tif (flags & ~(RENAME_EXCHANGE | RENAME_NOREPLACE))\n\t\tgoto out;\n\n\tflags &= ~RENAME_NOREPLACE;\n\n\terr = ovl_check_sticky(old);\n\tif (err)\n\t\tgoto out;\n\n\t/* Don't copy up directory trees */\n\told_type = ovl_path_type(old);\n\terr = -EXDEV;\n\tif (OVL_TYPE_MERGE_OR_LOWER(old_type) && is_dir)\n\t\tgoto out;\n\n\tif (new->d_inode) {\n\t\terr = ovl_check_sticky(new);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (d_is_dir(new))\n\t\t\tnew_is_dir = true;\n\n\t\tnew_type = ovl_path_type(new);\n\t\terr = -EXDEV;\n\t\tif (!overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir)\n\t\t\tgoto out;\n\n\t\terr = 0;\n\t\tif (!OVL_TYPE_UPPER(new_type) && !OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_lower(old)->d_inode ==\n\t\t\t    ovl_dentry_lower(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (OVL_TYPE_UPPER(new_type) && OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_upper(old)->d_inode ==\n\t\t\t    ovl_dentry_upper(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (ovl_dentry_is_opaque(new))\n\t\t\tnew_type = __OVL_PATH_UPPER;\n\t\telse\n\t\t\tnew_type = __OVL_PATH_UPPER | __OVL_PATH_PURE;\n\t}\n\n\terr = ovl_want_write(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out_drop_write;\n\tif (!overwrite) {\n\t\terr = ovl_copy_up(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\t}\n\n\told_opaque = !OVL_TYPE_PURE_UPPER(old_type);\n\tnew_opaque = !OVL_TYPE_PURE_UPPER(new_type);\n\n\tif (old_opaque || new_opaque) {\n\t\terr = -ENOMEM;\n\t\toverride_cred = prepare_creds();\n\t\tif (!override_cred)\n\t\t\tgoto out_drop_write;\n\n\t\t/*\n\t\t * CAP_SYS_ADMIN for setting xattr on whiteout, opaque dir\n\t\t * CAP_DAC_OVERRIDE for create in workdir\n\t\t * CAP_FOWNER for removing whiteout from sticky dir\n\t\t * CAP_FSETID for chmod of opaque dir\n\t\t * CAP_CHOWN for chown of opaque dir\n\t\t */\n\t\tcap_raise(override_cred->cap_effective, CAP_SYS_ADMIN);\n\t\tcap_raise(override_cred->cap_effective, CAP_DAC_OVERRIDE);\n\t\tcap_raise(override_cred->cap_effective, CAP_FOWNER);\n\t\tcap_raise(override_cred->cap_effective, CAP_FSETID);\n\t\tcap_raise(override_cred->cap_effective, CAP_CHOWN);\n\t\told_cred = override_creds(override_cred);\n\t}\n\n\tif (overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir) {\n\t\topaquedir = ovl_check_empty_and_clear(new);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir)) {\n\t\t\topaquedir = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\tif (overwrite) {\n\t\tif (old_opaque) {\n\t\t\tif (new->d_inode || !new_opaque) {\n\t\t\t\t/* Whiteout source */\n\t\t\t\tflags |= RENAME_WHITEOUT;\n\t\t\t} else {\n\t\t\t\t/* Switch whiteouts */\n\t\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\t}\n\t\t} else if (is_dir && !new->d_inode && new_opaque) {\n\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\tcleanup_whiteout = true;\n\t\t}\n\t}\n\n\told_upperdir = ovl_dentry_upper(old->d_parent);\n\tnew_upperdir = ovl_dentry_upper(new->d_parent);\n\n\ttrap = lock_rename(new_upperdir, old_upperdir);\n\n\tolddentry = ovl_dentry_upper(old);\n\tnewdentry = ovl_dentry_upper(new);\n\tif (newdentry) {\n\t\tif (opaquedir) {\n\t\t\tnewdentry = opaquedir;\n\t\t\topaquedir = NULL;\n\t\t} else {\n\t\t\tdget(newdentry);\n\t\t}\n\t} else {\n\t\tnew_create = true;\n\t\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n\t\t\t\t\t   new->d_name.len);\n\t\terr = PTR_ERR(newdentry);\n\t\tif (IS_ERR(newdentry))\n\t\t\tgoto out_unlock;\n\t}\n\n\terr = -ESTALE;\n\tif (olddentry->d_parent != old_upperdir)\n\t\tgoto out_dput;\n\tif (newdentry->d_parent != new_upperdir)\n\t\tgoto out_dput;\n\tif (olddentry == trap)\n\t\tgoto out_dput;\n\tif (newdentry == trap)\n\t\tgoto out_dput;\n\n\tif (is_dir && !old_opaque && new_opaque) {\n\t\terr = ovl_set_opaque(olddentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\tif (!overwrite && new_is_dir && old_opaque && !new_opaque) {\n\t\terr = ovl_set_opaque(newdentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\tif (old_opaque || new_opaque) {\n\t\terr = ovl_do_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t    new_upperdir->d_inode, newdentry,\n\t\t\t\t    flags);\n\t} else {\n\t\t/* No debug for the plain case */\n\t\tBUG_ON(flags & ~RENAME_EXCHANGE);\n\t\terr = vfs_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t new_upperdir->d_inode, newdentry,\n\t\t\t\t NULL, flags);\n\t}\n\n\tif (err) {\n\t\tif (is_dir && !old_opaque && new_opaque)\n\t\t\tovl_remove_opaque(olddentry);\n\t\tif (!overwrite && new_is_dir && old_opaque && !new_opaque)\n\t\t\tovl_remove_opaque(newdentry);\n\t\tgoto out_dput;\n\t}\n\n\tif (is_dir && old_opaque && !new_opaque)\n\t\tovl_remove_opaque(olddentry);\n\tif (!overwrite && new_is_dir && !old_opaque && new_opaque)\n\t\tovl_remove_opaque(newdentry);\n\n\t/*\n\t * Old dentry now lives in different location. Dentries in\n\t * lowerstack are stale. We cannot drop them here because\n\t * access to them is lockless. This could be only pure upper\n\t * or opaque directory - numlower is zero. Or upper non-dir\n\t * entry - its pureness is tracked by flag opaque.\n\t */\n\tif (old_opaque != new_opaque) {\n\t\tovl_dentry_set_opaque(old, new_opaque);\n\t\tif (!overwrite)\n\t\t\tovl_dentry_set_opaque(new, old_opaque);\n\t}\n\n\tif (cleanup_whiteout)\n\t\tovl_cleanup(old_upperdir->d_inode, newdentry);\n\n\tovl_dentry_version_inc(old->d_parent);\n\tovl_dentry_version_inc(new->d_parent);\n\nout_dput:\n\tdput(newdentry);\nout_unlock:\n\tunlock_rename(new_upperdir, old_upperdir);\nout_revert_creds:\n\tif (old_opaque || new_opaque) {\n\t\trevert_creds(old_cred);\n\t\tput_cred(override_cred);\n\t}\nout_drop_write:\n\tovl_drop_write(old);\nout:\n\tdput(opaquedir);\n\treturn err;\n}",
        "code_after_change": "static int ovl_rename2(struct inode *olddir, struct dentry *old,\n\t\t       struct inode *newdir, struct dentry *new,\n\t\t       unsigned int flags)\n{\n\tint err;\n\tenum ovl_path_type old_type;\n\tenum ovl_path_type new_type;\n\tstruct dentry *old_upperdir;\n\tstruct dentry *new_upperdir;\n\tstruct dentry *olddentry;\n\tstruct dentry *newdentry;\n\tstruct dentry *trap;\n\tbool old_opaque;\n\tbool new_opaque;\n\tbool new_create = false;\n\tbool cleanup_whiteout = false;\n\tbool overwrite = !(flags & RENAME_EXCHANGE);\n\tbool is_dir = d_is_dir(old);\n\tbool new_is_dir = false;\n\tstruct dentry *opaquedir = NULL;\n\tconst struct cred *old_cred = NULL;\n\tstruct cred *override_cred = NULL;\n\n\terr = -EINVAL;\n\tif (flags & ~(RENAME_EXCHANGE | RENAME_NOREPLACE))\n\t\tgoto out;\n\n\tflags &= ~RENAME_NOREPLACE;\n\n\terr = ovl_check_sticky(old);\n\tif (err)\n\t\tgoto out;\n\n\t/* Don't copy up directory trees */\n\told_type = ovl_path_type(old);\n\terr = -EXDEV;\n\tif (OVL_TYPE_MERGE_OR_LOWER(old_type) && is_dir)\n\t\tgoto out;\n\n\tif (new->d_inode) {\n\t\terr = ovl_check_sticky(new);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (d_is_dir(new))\n\t\t\tnew_is_dir = true;\n\n\t\tnew_type = ovl_path_type(new);\n\t\terr = -EXDEV;\n\t\tif (!overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir)\n\t\t\tgoto out;\n\n\t\terr = 0;\n\t\tif (!OVL_TYPE_UPPER(new_type) && !OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_lower(old)->d_inode ==\n\t\t\t    ovl_dentry_lower(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (OVL_TYPE_UPPER(new_type) && OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_upper(old)->d_inode ==\n\t\t\t    ovl_dentry_upper(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (ovl_dentry_is_opaque(new))\n\t\t\tnew_type = __OVL_PATH_UPPER;\n\t\telse\n\t\t\tnew_type = __OVL_PATH_UPPER | __OVL_PATH_PURE;\n\t}\n\n\terr = ovl_want_write(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out_drop_write;\n\tif (!overwrite) {\n\t\terr = ovl_copy_up(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\t}\n\n\told_opaque = !OVL_TYPE_PURE_UPPER(old_type);\n\tnew_opaque = !OVL_TYPE_PURE_UPPER(new_type);\n\n\tif (old_opaque || new_opaque) {\n\t\terr = -ENOMEM;\n\t\toverride_cred = prepare_creds();\n\t\tif (!override_cred)\n\t\t\tgoto out_drop_write;\n\n\t\t/*\n\t\t * CAP_SYS_ADMIN for setting xattr on whiteout, opaque dir\n\t\t * CAP_DAC_OVERRIDE for create in workdir\n\t\t * CAP_FOWNER for removing whiteout from sticky dir\n\t\t * CAP_FSETID for chmod of opaque dir\n\t\t * CAP_CHOWN for chown of opaque dir\n\t\t */\n\t\tcap_raise(override_cred->cap_effective, CAP_SYS_ADMIN);\n\t\tcap_raise(override_cred->cap_effective, CAP_DAC_OVERRIDE);\n\t\tcap_raise(override_cred->cap_effective, CAP_FOWNER);\n\t\tcap_raise(override_cred->cap_effective, CAP_FSETID);\n\t\tcap_raise(override_cred->cap_effective, CAP_CHOWN);\n\t\told_cred = override_creds(override_cred);\n\t}\n\n\tif (overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir) {\n\t\topaquedir = ovl_check_empty_and_clear(new);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir)) {\n\t\t\topaquedir = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\tif (overwrite) {\n\t\tif (old_opaque) {\n\t\t\tif (new->d_inode || !new_opaque) {\n\t\t\t\t/* Whiteout source */\n\t\t\t\tflags |= RENAME_WHITEOUT;\n\t\t\t} else {\n\t\t\t\t/* Switch whiteouts */\n\t\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\t}\n\t\t} else if (is_dir && !new->d_inode && new_opaque) {\n\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\tcleanup_whiteout = true;\n\t\t}\n\t}\n\n\told_upperdir = ovl_dentry_upper(old->d_parent);\n\tnew_upperdir = ovl_dentry_upper(new->d_parent);\n\n\ttrap = lock_rename(new_upperdir, old_upperdir);\n\n\n\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,\n\t\t\t\t   old->d_name.len);\n\terr = PTR_ERR(olddentry);\n\tif (IS_ERR(olddentry))\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif (olddentry != ovl_dentry_upper(old))\n\t\tgoto out_dput_old;\n\n\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n\t\t\t\t   new->d_name.len);\n\terr = PTR_ERR(newdentry);\n\tif (IS_ERR(newdentry))\n\t\tgoto out_dput_old;\n\n\terr = -ESTALE;\n\tif (ovl_dentry_upper(new)) {\n\t\tif (opaquedir) {\n\t\t\tif (newdentry != opaquedir)\n\t\t\t\tgoto out_dput;\n\t\t} else {\n\t\t\tif (newdentry != ovl_dentry_upper(new))\n\t\t\t\tgoto out_dput;\n\t\t}\n\t} else {\n\t\tnew_create = true;\n\t\tif (!d_is_negative(newdentry) &&\n\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))\n\t\t\tgoto out_dput;\n\t}\n\n\tif (olddentry == trap)\n\t\tgoto out_dput;\n\tif (newdentry == trap)\n\t\tgoto out_dput;\n\n\tif (is_dir && !old_opaque && new_opaque) {\n\t\terr = ovl_set_opaque(olddentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\tif (!overwrite && new_is_dir && old_opaque && !new_opaque) {\n\t\terr = ovl_set_opaque(newdentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\tif (old_opaque || new_opaque) {\n\t\terr = ovl_do_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t    new_upperdir->d_inode, newdentry,\n\t\t\t\t    flags);\n\t} else {\n\t\t/* No debug for the plain case */\n\t\tBUG_ON(flags & ~RENAME_EXCHANGE);\n\t\terr = vfs_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t new_upperdir->d_inode, newdentry,\n\t\t\t\t NULL, flags);\n\t}\n\n\tif (err) {\n\t\tif (is_dir && !old_opaque && new_opaque)\n\t\t\tovl_remove_opaque(olddentry);\n\t\tif (!overwrite && new_is_dir && old_opaque && !new_opaque)\n\t\t\tovl_remove_opaque(newdentry);\n\t\tgoto out_dput;\n\t}\n\n\tif (is_dir && old_opaque && !new_opaque)\n\t\tovl_remove_opaque(olddentry);\n\tif (!overwrite && new_is_dir && !old_opaque && new_opaque)\n\t\tovl_remove_opaque(newdentry);\n\n\t/*\n\t * Old dentry now lives in different location. Dentries in\n\t * lowerstack are stale. We cannot drop them here because\n\t * access to them is lockless. This could be only pure upper\n\t * or opaque directory - numlower is zero. Or upper non-dir\n\t * entry - its pureness is tracked by flag opaque.\n\t */\n\tif (old_opaque != new_opaque) {\n\t\tovl_dentry_set_opaque(old, new_opaque);\n\t\tif (!overwrite)\n\t\t\tovl_dentry_set_opaque(new, old_opaque);\n\t}\n\n\tif (cleanup_whiteout)\n\t\tovl_cleanup(old_upperdir->d_inode, newdentry);\n\n\tovl_dentry_version_inc(old->d_parent);\n\tovl_dentry_version_inc(new->d_parent);\n\nout_dput:\n\tdput(newdentry);\nout_dput_old:\n\tdput(olddentry);\nout_unlock:\n\tunlock_rename(new_upperdir, old_upperdir);\nout_revert_creds:\n\tif (old_opaque || new_opaque) {\n\t\trevert_creds(old_cred);\n\t\tput_cred(override_cred);\n\t}\nout_drop_write:\n\tovl_drop_write(old);\nout:\n\tdput(opaquedir);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -138,29 +138,39 @@\n \n \ttrap = lock_rename(new_upperdir, old_upperdir);\n \n-\tolddentry = ovl_dentry_upper(old);\n-\tnewdentry = ovl_dentry_upper(new);\n-\tif (newdentry) {\n+\n+\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,\n+\t\t\t\t   old->d_name.len);\n+\terr = PTR_ERR(olddentry);\n+\tif (IS_ERR(olddentry))\n+\t\tgoto out_unlock;\n+\n+\terr = -ESTALE;\n+\tif (olddentry != ovl_dentry_upper(old))\n+\t\tgoto out_dput_old;\n+\n+\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n+\t\t\t\t   new->d_name.len);\n+\terr = PTR_ERR(newdentry);\n+\tif (IS_ERR(newdentry))\n+\t\tgoto out_dput_old;\n+\n+\terr = -ESTALE;\n+\tif (ovl_dentry_upper(new)) {\n \t\tif (opaquedir) {\n-\t\t\tnewdentry = opaquedir;\n-\t\t\topaquedir = NULL;\n+\t\t\tif (newdentry != opaquedir)\n+\t\t\t\tgoto out_dput;\n \t\t} else {\n-\t\t\tdget(newdentry);\n+\t\t\tif (newdentry != ovl_dentry_upper(new))\n+\t\t\t\tgoto out_dput;\n \t\t}\n \t} else {\n \t\tnew_create = true;\n-\t\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n-\t\t\t\t\t   new->d_name.len);\n-\t\terr = PTR_ERR(newdentry);\n-\t\tif (IS_ERR(newdentry))\n-\t\t\tgoto out_unlock;\n-\t}\n-\n-\terr = -ESTALE;\n-\tif (olddentry->d_parent != old_upperdir)\n-\t\tgoto out_dput;\n-\tif (newdentry->d_parent != new_upperdir)\n-\t\tgoto out_dput;\n+\t\tif (!d_is_negative(newdentry) &&\n+\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))\n+\t\t\tgoto out_dput;\n+\t}\n+\n \tif (olddentry == trap)\n \t\tgoto out_dput;\n \tif (newdentry == trap)\n@@ -223,6 +233,8 @@\n \n out_dput:\n \tdput(newdentry);\n+out_dput_old:\n+\tdput(olddentry);\n out_unlock:\n \tunlock_rename(new_upperdir, old_upperdir);\n out_revert_creds:",
        "function_modified_lines": {
            "added": [
                "",
                "\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,",
                "\t\t\t\t   old->d_name.len);",
                "\terr = PTR_ERR(olddentry);",
                "\tif (IS_ERR(olddentry))",
                "\t\tgoto out_unlock;",
                "",
                "\terr = -ESTALE;",
                "\tif (olddentry != ovl_dentry_upper(old))",
                "\t\tgoto out_dput_old;",
                "",
                "\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,",
                "\t\t\t\t   new->d_name.len);",
                "\terr = PTR_ERR(newdentry);",
                "\tif (IS_ERR(newdentry))",
                "\t\tgoto out_dput_old;",
                "",
                "\terr = -ESTALE;",
                "\tif (ovl_dentry_upper(new)) {",
                "\t\t\tif (newdentry != opaquedir)",
                "\t\t\t\tgoto out_dput;",
                "\t\t\tif (newdentry != ovl_dentry_upper(new))",
                "\t\t\t\tgoto out_dput;",
                "\t\tif (!d_is_negative(newdentry) &&",
                "\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))",
                "\t\t\tgoto out_dput;",
                "\t}",
                "",
                "out_dput_old:",
                "\tdput(olddentry);"
            ],
            "deleted": [
                "\tolddentry = ovl_dentry_upper(old);",
                "\tnewdentry = ovl_dentry_upper(new);",
                "\tif (newdentry) {",
                "\t\t\tnewdentry = opaquedir;",
                "\t\t\topaquedir = NULL;",
                "\t\t\tdget(newdentry);",
                "\t\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,",
                "\t\t\t\t\t   new->d_name.len);",
                "\t\terr = PTR_ERR(newdentry);",
                "\t\tif (IS_ERR(newdentry))",
                "\t\t\tgoto out_unlock;",
                "\t}",
                "",
                "\terr = -ESTALE;",
                "\tif (olddentry->d_parent != old_upperdir)",
                "\t\tgoto out_dput;",
                "\tif (newdentry->d_parent != new_upperdir)",
                "\t\tgoto out_dput;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "fs/overlayfs/dir.c in the OverlayFS filesystem implementation in the Linux kernel before 4.6 does not properly verify the upper dentry before proceeding with unlink and rename system-call processing, which allows local users to cause a denial of service (system crash) via a rename system call that specifies a self-hardlink.",
        "id": 1065
    },
    {
        "cve_id": "CVE-2016-6162",
        "code_before_change": "int udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\t__UDP_INC_STATS(sock_net(sk),\n\t\t\t\t\t\tUDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\tis_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto drop;\n\n\tudp_csum_pull_header(skb);\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\t__UDP6_INC_STATS(sock_net(sk),\n\t\t\t\t UDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop;\n\t}\n\n\tskb_dst_drop(skb);\n\n\tbh_lock_sock(sk);\n\trc = 0;\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "code_after_change": "int udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\t__UDP_INC_STATS(sock_net(sk),\n\t\t\t\t\t\tUDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\tis_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto drop;\n\tif (unlikely(skb->len < sizeof(struct udphdr)))\n\t\tgoto drop;\n\n\tudp_csum_pull_header(skb);\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\t__UDP6_INC_STATS(sock_net(sk),\n\t\t\t\t UDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop;\n\t}\n\n\tskb_dst_drop(skb);\n\n\tbh_lock_sock(sk);\n\trc = 0;\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -65,6 +65,8 @@\n \n \tif (sk_filter(sk, skb))\n \t\tgoto drop;\n+\tif (unlikely(skb->len < sizeof(struct udphdr)))\n+\t\tgoto drop;\n \n \tudp_csum_pull_header(skb);\n \tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(skb->len < sizeof(struct udphdr)))",
                "\t\tgoto drop;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "net/core/skbuff.c in the Linux kernel 4.7-rc6 allows local users to cause a denial of service (panic) or possibly have unspecified other impact via certain IPv6 socket operations.",
        "id": 1063
    },
    {
        "cve_id": "CVE-2017-6345",
        "code_before_change": "static void llc_sap_rcv(struct llc_sap *sap, struct sk_buff *skb,\n\t\t\tstruct sock *sk)\n{\n\tstruct llc_sap_state_ev *ev = llc_sap_ev(skb);\n\n\tev->type   = LLC_SAP_EV_TYPE_PDU;\n\tev->reason = 0;\n\tskb->sk = sk;\n\tllc_sap_state_process(sap, skb);\n}",
        "code_after_change": "static void llc_sap_rcv(struct llc_sap *sap, struct sk_buff *skb,\n\t\t\tstruct sock *sk)\n{\n\tstruct llc_sap_state_ev *ev = llc_sap_ev(skb);\n\n\tev->type   = LLC_SAP_EV_TYPE_PDU;\n\tev->reason = 0;\n\tskb_orphan(skb);\n\tsock_hold(sk);\n\tskb->sk = sk;\n\tskb->destructor = sock_efree;\n\tllc_sap_state_process(sap, skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,9 @@\n \n \tev->type   = LLC_SAP_EV_TYPE_PDU;\n \tev->reason = 0;\n+\tskb_orphan(skb);\n+\tsock_hold(sk);\n \tskb->sk = sk;\n+\tskb->destructor = sock_efree;\n \tllc_sap_state_process(sap, skb);\n }",
        "function_modified_lines": {
            "added": [
                "\tskb_orphan(skb);",
                "\tsock_hold(sk);",
                "\tskb->destructor = sock_efree;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The LLC subsystem in the Linux kernel before 4.9.13 does not ensure that a certain destructor exists in required circumstances, which allows local users to cause a denial of service (BUG_ON) or possibly have unspecified other impact via crafted system calls.",
        "id": 1481
    },
    {
        "cve_id": "CVE-2017-5123",
        "code_before_change": "\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "code_after_change": "\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))\n\t\tgoto Efault;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,9 @@\n \tif (!infop)\n \t\treturn err;\n \n+\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))\n+\t\tgoto Efault;\n+\n \tuser_access_begin();\n \tunsafe_put_user(signo, &infop->si_signo, Efault);\n \tunsafe_put_user(0, &infop->si_errno, Efault);",
        "function_modified_lines": {
            "added": [
                "\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))",
                "\t\tgoto Efault;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Insufficient data validation in waitid allowed an user to escape sandboxes on Linux.",
        "id": 1457
    },
    {
        "cve_id": "CVE-2013-4129",
        "code_before_change": "static int __br_mdb_del(struct net_bridge *br, struct br_mdb_entry *entry)\n{\n\tstruct net_bridge_mdb_htable *mdb;\n\tstruct net_bridge_mdb_entry *mp;\n\tstruct net_bridge_port_group *p;\n\tstruct net_bridge_port_group __rcu **pp;\n\tstruct br_ip ip;\n\tint err = -EINVAL;\n\n\tif (!netif_running(br->dev) || br->multicast_disabled)\n\t\treturn -EINVAL;\n\n\tif (timer_pending(&br->multicast_querier_timer))\n\t\treturn -EBUSY;\n\n\tip.proto = entry->addr.proto;\n\tif (ip.proto == htons(ETH_P_IP))\n\t\tip.u.ip4 = entry->addr.u.ip4;\n#if IS_ENABLED(CONFIG_IPV6)\n\telse\n\t\tip.u.ip6 = entry->addr.u.ip6;\n#endif\n\n\tspin_lock_bh(&br->multicast_lock);\n\tmdb = mlock_dereference(br->mdb, br);\n\n\tmp = br_mdb_ip_get(mdb, &ip);\n\tif (!mp)\n\t\tgoto unlock;\n\n\tfor (pp = &mp->ports;\n\t     (p = mlock_dereference(*pp, br)) != NULL;\n\t     pp = &p->next) {\n\t\tif (!p->port || p->port->dev->ifindex != entry->ifindex)\n\t\t\tcontinue;\n\n\t\tif (p->port->state == BR_STATE_DISABLED)\n\t\t\tgoto unlock;\n\n\t\trcu_assign_pointer(*pp, p->next);\n\t\thlist_del_init(&p->mglist);\n\t\tdel_timer(&p->timer);\n\t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n\t\terr = 0;\n\n\t\tif (!mp->ports && !mp->mglist &&\n\t\t    netif_running(br->dev))\n\t\t\tmod_timer(&mp->timer, jiffies);\n\t\tbreak;\n\t}\n\nunlock:\n\tspin_unlock_bh(&br->multicast_lock);\n\treturn err;\n}",
        "code_after_change": "static int __br_mdb_del(struct net_bridge *br, struct br_mdb_entry *entry)\n{\n\tstruct net_bridge_mdb_htable *mdb;\n\tstruct net_bridge_mdb_entry *mp;\n\tstruct net_bridge_port_group *p;\n\tstruct net_bridge_port_group __rcu **pp;\n\tstruct br_ip ip;\n\tint err = -EINVAL;\n\n\tif (!netif_running(br->dev) || br->multicast_disabled)\n\t\treturn -EINVAL;\n\n\tif (timer_pending(&br->multicast_querier_timer))\n\t\treturn -EBUSY;\n\n\tip.proto = entry->addr.proto;\n\tif (ip.proto == htons(ETH_P_IP))\n\t\tip.u.ip4 = entry->addr.u.ip4;\n#if IS_ENABLED(CONFIG_IPV6)\n\telse\n\t\tip.u.ip6 = entry->addr.u.ip6;\n#endif\n\n\tspin_lock_bh(&br->multicast_lock);\n\tmdb = mlock_dereference(br->mdb, br);\n\n\tmp = br_mdb_ip_get(mdb, &ip);\n\tif (!mp)\n\t\tgoto unlock;\n\n\tfor (pp = &mp->ports;\n\t     (p = mlock_dereference(*pp, br)) != NULL;\n\t     pp = &p->next) {\n\t\tif (!p->port || p->port->dev->ifindex != entry->ifindex)\n\t\t\tcontinue;\n\n\t\tif (p->port->state == BR_STATE_DISABLED)\n\t\t\tgoto unlock;\n\n\t\trcu_assign_pointer(*pp, p->next);\n\t\thlist_del_init(&p->mglist);\n\t\tdel_timer(&p->timer);\n\t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n\t\terr = 0;\n\n\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&\n\t\t    netif_running(br->dev))\n\t\t\tmod_timer(&mp->timer, jiffies);\n\t\tbreak;\n\t}\n\nunlock:\n\tspin_unlock_bh(&br->multicast_lock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,7 +43,7 @@\n \t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n \t\terr = 0;\n \n-\t\tif (!mp->ports && !mp->mglist &&\n+\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&\n \t\t    netif_running(br->dev))\n \t\t\tmod_timer(&mp->timer, jiffies);\n \t\tbreak;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&"
            ],
            "deleted": [
                "\t\tif (!mp->ports && !mp->mglist &&"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The bridge multicast implementation in the Linux kernel through 3.10.3 does not check whether a certain timer is armed before modifying the timeout value of that timer, which allows local users to cause a denial of service (BUG and system crash) via vectors involving the shutdown of a KVM virtual machine, related to net/bridge/br_mdb.c and net/bridge/br_multicast.c.",
        "id": 282
    }
]